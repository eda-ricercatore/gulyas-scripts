#	Sets of skills

+ skill set:
	- Understand data landscape i.e tooling, tech stack, source systems etc. and work closely with the data Engineering team to improve the data collection and quality.
	- Ability to define and spot macro and micro levels trends with statistical significance on a regular basis and understand key drivers driving those trends.
	- 8+ years of data analyst experience with 4+ years of proven industry experience in a large scale environment(PBs scale, globally distributed teams).
	- Strong experience in Python, R, SQL, Tableau, Google Analytics, Hive and BigQuery (or any other Big data/Cloud equivalent) etc.
+ skill set:
	- Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.
	- 3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.
	- Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.
	- Experience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.
	- Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.
+ skill set:
	- Partner and align with business leaders, stakeholders, product managers and internal teams to understand the business and product challenges and goals and address them using predictive analytics in a globally distributed environment.
	- Understand data landscape i.e tooling, tech stack, source systems etc. and work closely with the data Engineering team to improve the data collection and quality.
	- Understand business/product strategy and high-level roadmap and align analysis efforts to enable them with data insights and help achieve their strategic goals.
	- Strong audience focused presentation and storytelling skills focused on key takeaways in a crisp and concise manner.
	- Define hypothesis driven models and best practices to derive and publish model scores/insights/learnings at scale within the company.
	- Ability to define and spot macro and micro levels trends with statistical significance on a regular basis and understand key drivers driving those trends.
	- 8+ years of data scientist experience with 4+ years of proven industry experience in a large scale environment(PBs scale, globally distributed teams).
	- Proven lead in driving multi-million dollar revenue generator models for the company and setting up data science related best practices at a company.
	- 2+ years experience with a fast-growing SaaS business based company is preferred.
	- Strong experience in Python, R, Spark, SQL, Tableau, Google Analytics, Hive and BigQuery (or any other Big data/Cloud equivalent) etc.
+ skill set:
	- Solid foundation in computer science, with strong competencies in algorithms, data structures, software design, web security, and building large, distributed systems
	- Excel at planning, working multi-functionally, leading execution across teams to meet commitments and deliver with predictability
	- Demonstrate a track record of leading teams, including hiring, onboarding, and professional development. You inspire your team to reach higher. You’re as good as explaining “why” as you are “how”
	- Experience implementing tools, process, internal instrumentation, methodologies and resolving blockages
	- Demonstrated ability to recruit and hire top talent
	- Comfortable managing teams/projects with tight deadlines and short release cycles
	- Experience working with and getting the best out of post-doctorate researchers
	- Being passionate about cryptography and/or web technology such as TLS
	- Familiarity working with DNS, database systems, Internet performance, and/or Internet security
+ skill set:
	- You will help build customer facing apps and our core platform to create, improve, and scale agricultural models. Our culture is built on cross-disciplinary collaboration, learning, and rapid prototyping. CiBO is a science-based company, so prepare to learn and invent with us!
	- Graduate degree in a technical field and 5+ years experience as a Data Scientist working on commercial software products
	- Experience engineering production-level software systems evidenced by having built a parallel/distributed system, using software best practices, performance optimizations, or an automated pipeline to interpret other data science experiments
	- Experience applying the scientific method to complex, multivariate unstructured problems to produce rigorously-defendable conclusion in prose and graphics
	- Experience across and significant work in at least one of clustering and classification, multivariate regression, prediction, and forecasting; and dimensionality reduction and feature engineering
	- Experience or interest in Bayesian methods of crafting generative probabilistic models; engineering informative priors and solution via MCMC
	- Experience collecting, curating, structuring and retrieving large data sets in relational databases.
	- Additional experience with NoSQL preferred
	- Experience with either time-series or spatial statistics, especially Gaussian Processes
	- Experience creating complex data visualizations for multivariate data
	- Experience with AWS or other programmatic distributed computing environments
	- Experience with Bayesian non-parametrics (CRP, IBP, etc.)
	- Experience with advanced MCMC techniques such as DREAM, Hamiltonian-MC, variational approximations, etc.
	- Experience or keen interest in FP (Scala, Haskell, Clojure, Erlang, OCaML/SML, Elm, F\#)
+ skill set: Use Python, React/Redux, Spark, Presto, Airflow, Git, CircleCI, and more.
+ Experience with data analytics platforms, such as Semantic Pro, Semantic Cortex, IBM i2
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift) is a plus
+ Experience using vector editors like Figma or Sketch and prototyping tools like Principle or Framer
+ You are an expert in AfterEffects, Cinema 4D, Sketch, Principle
+ You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
+ Conduct testing such as functional testing, user acceptance testing (UAT), automated acceptance testing (AAT), and specification by example.
+ skill set:
	- 3+ years of experience in machine learning, data mining, natural language processing, information retrieval, or statistical analysis
	- Experience working with large data sets using open source technologies such as Spark, Hadoop, and NoSQL
	- Experience developing and productizing real-world AI/ML applications such as prediction, personalization, recommendation, content understanding and NLP
	- Experience working with at least 3 of the following popular machine learning frameworks/libraries: sklearn, tensorflow, pytorch, caffe, keras, theano, cntk, mxnet, spark mllib
	- Experience developing and deploying deep learning NLP models is a plus
	- Experience working with a knowledge graph is a plus
+ tech stack:
	- Experience with Deep Learning frameworks, e.g., PyTorch, DeepLearning4J, TensorFlow
	- Experience in SPARK (using Python or Scala). Knowledge of AWS services will be appreciated.
+ tech stack:
	- 2+ years of analytical work experience (experience working with product organizations a plus)
	- Strong critical thinking and problem solving skills
	- Experience  communicating effectively with non-technical audiences
	- Strong ability to devise data-driven solutions to business problems
	- Strong competency with SQL
	- Experience with or exposure to a scripting language (Python preferred)
+ tech stack:
	- 5+ years of relevant analytical experience working with data or MS in a relevant technical field and 2+ years of analytical work experience (experience working with product organizations a plus)
	- Strong critical thinking and problem solving skills
	- Comfort and expertise communicating effectively with a wide-range of audiences (including product managers, engineers, business development managers, occasionally executives)
	- Strong ability to devise data-driven solutions to business problems
	- Ability to drive impact by thoughtfully tackling open-ended problems
	- Strong data intuition and deep understanding of and experience with statistical and/or ML modeling techniques
	- Strong competency with SQL
	- Fluency in a scripting language (Python preferred)
	- A plus: Experience with large scale data processing tools (Apache Spark) or implementing systems in production at scale
+ skill set:
	- Strong proficiency in Python a necessity, especially the Python data science stack
	- Experience developing data science models, workflows, and software for real world applications and working with imperfect data
	- Exploratory analysis, modeling, and visualization in Jupyter notebooks
	- Integrating data sources, creating subsets (ex. train/test) for modeling, and assessing potential datasets, tools, and approaches
	- Translating the results of analysis into implications for people and problems
	- Developing well-organized code that can be collaboratively reviewed, reproduced, and integrated into applications
	- Quickly assessing and becoming productive in relevant new technologies and methods
	- Experience with core data science tools (ex. pandas, scikit-learn, numpy, jupyter)
	- Experience working with messy data and real-world applications
	- Experience using IaaS like Amazon AWS
	- Working on a small team means doing a little bit of a lot of things. We're looking for somebody who can ask the right questions to figure out what is important, iterate between brainstorming together, working independently, and managing other data scientists, scope new data science projects, and exercise sound judgment to make reasonable decisions under conditions of ambiguity.
	- Communication is a core data science skill at DrivenData. Doing client-facing work involves articulating concepts, interpreting results, and selecting the method that satisfies the constraints of the project.
	- Working familiarity with the tools and practices used in software engineering and deployment (including test-driven deployment, containerization (ex. Docker), platform as a service (ex. Heroku), and infrastructure as a service (ex. AWS, Azure)
+ Interact with MySQL data stores and NSQ messaging queues.
+ skill set:
	- Our data infrastructure team is responsible for all things data — our data warehouse, Hadoop, Redshift, Spark, Kafka, Airflow and so on.
	- Deep experience with MySQL, NoSQL data stores like HBase or similar.
	- Strong understanding of Unix/Linux variants, web network protocols, persistence solutions
+ skill set:
	- Knowledge of backend storage systems like MySQL, HBase, Memcached, Redis, Kafka etc.
	- Deep understanding of at least one popular server side MVC Framework (e.g Django, Rails, AngularJS etc).
+ skill set:
	- Working knowledge of relational databases and query authoring (SQL)
	- Experience working with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
	- Experience running A/B tests to optimize the growth loop of a product
	- Strong experience with MySQL and in-memory caching systems such as Redis, Memcached
	- Experience with Linux operating system internals, filesystems, disk/storage technologies and storage protocols
+ skill set:
	- Interest in using data and user research to inform product decisions. Experience with effective A/B testing is a plus.
	- Ability to think holistically about a complex, social product, and map big picture metrics to a realistic actionable plan.
	- Passion for experimentation and new ideas.
+ skill set:
	- Deep knowledge of web technologies, e.g. HTML, CSS. Experience with LESS or SASS is a plus.
	- Deep knowledge of JavaScript frameworks, e.g. jQuery. Experience with pure Javascript is a plus.
	- Some knowledge of server-side languages and web frameworks. Experience with Python is a plus.
	- Experience debugging across multiple browsers. Experience with UI testing tools like Selenium or phantomJS is a plus.
	- Experience optimizing the speed and performance of websites.
	- Experience maintaining large and growing code bases in a fast-moving environment.
	- Interest in staying current with new and evolving web technologies.
+ skill set:
	- 7+ years of industry/academic experience in Machine Learning or related field
	- You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
	- Previous experience building end to end scalable Machine Learning systems
	- Software engineering skills. Knowledge of Python and C++ is a plus.
	- Knowledge of existing open source frameworks such as scikit-learn, Torch, Caffe, or Theano is a plus
+ skill set:
	- Individuals in this role should be experts in machine learning and NLP and have experience working on problems such as language models, discourse analysis, question-answering, word-sense disambiguation, automatic summarization etc.
	- Improve our existing NLP and Machine Learning systems using your expertise
	- Identify new opportunities to apply NLP and Machine Learning to different parts of the Quora product
	- Work with other engineers to implement algorithms, abstractions and systems in an efficient way, with strong positive impact on our user-facing products
	- Take end to end ownership of Machine Learning systems - from data pipelines and training to realtime prediction engines
	- Good mathematical understanding of popular NLP and Machine Learning algorithms
	- Experience building production-ready NLP or information retrieval systems
	- Hands-on experience with NLP tools, libraries and corpora (e.g. NLTK, Stanford CoreNLP, Wikipedia corpus, etc)
	- Knowledge of Python or C++, or the ability to learn them quickly
+ skill set:
	- At Quora, we use Machine Learning in almost every part of the product - feed ranking, answer ranking, search, topic and user recommendations, spam detection etc.
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- Previous experience building internet applications and large systems
	- General understanding of Machine Learning at the level of a semester-long ML class (college or multiple MOOCs)
	- Passion for learning
+ skill set:
	- We use a variety of algorithms — everything from linear models to decision trees and deep neural networks.
	- To that end, we are looking for engineers to help us build our company-wide ML development platform. In this role, you will be the part of a small team solving very interesting technical problems at the intersection of various exciting domains like Machine Learning, Distributed Systems and High Performance Computing.
	- Build and maintain large scale distributed systems to support the whole pipeline from data collection and training to deployment
	- Write efficient implementations of ML algorithms over CPUs & GPUs
	- Integrate our in-house systems with open source libraries like Spark and Tensorflow
	- Build abstractions to automate various steps in different ML workflows
	- Build tools to debug, visualize and inspect various features and models
	- Work with the engineers who use the platform, and help them be more impactful by improving the platform
	- Experience with designing large-scale distributed systems
	- Experience with building end-to-end machine learning systems
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- Previous experience building end to end Machine Learning systems
+ skill set:
	- Use Python and SQL to draw insights from data at scale
	- Extract actionable insights from broad, open-ended questions
	- Create dashboards and develop metrics to track the success and growth of the product
	- Design and evaluate experiments to measure the impact of product changes
	- Analyze data from across the product to uncover the root causes of metric movements
	- Communicate results to cross-functional stakeholders to inform product decisions
	- Develop tools to scale and automate analyses, improving productivity across the company
	- Improve the work of other data scientists through mentorship and by bringing industry best practices to the team
	- Experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift)
+ skill set:
	- Identify new methods to test product changes where traditional A/B testing is not possible
	- Drive adoption of good experimental and statistical practices across the company
	- Apply statistical techniques to increase the efficiency and rigor of our experimental analyses
	- Proactively identify ways to optimize and scale up the way we run experiments, and to increase data scientists' impact in general, and create processes and tools to meet these needs
	- Mentor other data scientists in experimental design and causal inference techniques
	- Coursework in experimental design, causal inference, and/or econometrics
	- Experience running and analyzing behavioral experiments
	- Statistical intuition and knowledge of various hypothesis testing and regression approaches, e.g. hierarchical modeling, difference-in-differences
	- Demonstrated ability working effectively with cross-functional teams
	- Experience using git and pushing to a codebase
	- Experience with software engineering projects or coursework
	- Develop tools to scale and automate analyses, improving productivity across the company
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
+ Speaking of code our current stack is Backend: Python, Django, Celery, WebRTC; Frontend: React, HTML (JSX), CSS (in JS), GraphQL; Storage: PostgreSQL, S3, Elasticsearch. Most of our infrastructure is on AWS so it is a huge plus if you know and love AWS.
+ Experience with Natural Language Processing (Topic Modeling, Document Classification, Document Summarization, Sentiment Analysis, etc.)
+ Our current stack is Backend: Python, Django, Celery, WebRTC; Frontend: React, HTML, CSS; Storage: Postgres, S3, Elastic Search. Know it or want to learn, then this is the stack for you!
+ skill set:
	- Experience with Kubernetes and Docker.
	- Experience with Elasticsearch, Redis and/or Memcached.
+ skill set:
	- An understanding of several of these methodologies and tools:
		* Software development methods such as Agile, Scrum, Lean, Waterfall
		* Software project tools like JIRA, Pivotal Tracker, Trello, Asana, and MS Project
		* Continuous integration and build automation with Jenkins, TeamCity, TFS, TravisCI, CircleCI
	- Experience configuring the following technologies:
		* LDAP, ActiveDirectory, and other SAML/Single-Sign-on services
		* VMware vSphere, ESXi, AWS, Azure, GCP, and other virtual infrastructure tools
+ Familiarity with configuration/orchestration management software such as Puppet, Chef, Ansible, or Salt.
+ skill set:
	- Experience with the Hashicorp stack, specifically Vault.
	- Experience with infrastructure services such as LDAP, SSH, VPN, HTTP proxies, etc.
+ Experience with search and information retrieval systems, either custom or commercial (Elasticsearch, Solr).
+ You have used GraphQL in production environments
+ skill set:
	- You have worked with technologies like OAuth, SAML, SCIM, and LDAP
	- You have worked with external identity provisioning services like Azure Active Directory, Okta, OneLogin, and Ping
+ skill set:
	- You have experience with CDNs and SSL certificates
	- You have experience with static site generators
	- You have experience with Object storage system
	- You have experience with nginx and Lua scripting
+ Experience with queueing and streaming systems like Kafka, RabbitMQ, etc
+ skill set:
	- Familiarity with configuration management, particularly using Ansible + Napalm.
	- Comfortable working with Arista EOS and Juniper JunOS.
	- Expert-level exposure to IP routing, particularly with BGP, OSPF, and IS-IS.
	- Knowledge of MPLS, DWDM, and other backbone-related technologies
	- Experience in network segmentation strategies, BGP VPNs, VXLAN, and segment routing
+ skill set:
	- Experience building infrastructure automation.
	- Experience with logs-based analysis and RPC tracing technologies.
	- Practical experience with Prometheus.
+ skill set:
	- Knowledge of data center architecture: power, cooling, and networking.
	- Significant experience working with data center hardware and writing software to make that easier, faster, and less manual effort
	- Familiarity with best practices for hardware acceptance testing
+ Familiarity with configuration management software such as Puppet, Chef, Ansible, or Salt.
+ Experience with CNCF technologies and cloud computing at scale.
+ skill set:
	- Experience in the following areas: TLS, SSO, SAML, OAuth2, Kerberos, LDAP
	- Familiar with concepts of big data- Hadoop, Spark, Kafka, NoSql
	- Familiarity with GDPR, HIPAA, PCI or other compliances
	- Experience working with Docker or Kubernetes
	- Experience working with AWS, Azure, GCP
+ skill set:
	- Familiarity with Kafka or Kafka Connect
	- Large public clouds: AWS, GCP, AzureDocker, Kubernetes
	- Bridge data connectivity between various data systems and Kafka by building highly available and scalable Confluent connectors that run on top of Apache Kafka Connect framework
+ skill set:
	- Experience with concepts of distributed systems and big data such as - Hadoop, Spark, Kafka, Big Table, HBase etc
	- Experience in Network Security and Cloud Security
	- 2+ Experience working with AWS, Azure, GCP security related services and other cloud agnostic security tools
	- Strong fundamentals in distributed systems design and development
+ skill set:
	- Working knowledge of development systems such as Git, Maven and Jenkins
	- Experience in automating release, continuous integration/delivery system, and relevant tools (e.g. Jenkins, Packer, Terraform, Puppet, Ansible, Travis CI, etc)
	- Experience with configuration management, distributed testing and benchmarking / load generation systems
	- Experience with cloud computing platforms (e.g. Amazon AWS, Microsoft Azure, Google App Engine, etc.)
	- Working knowledge of database and messaging systems (MySql, PostgreSQL, Redis, Hbase, Voldemort, Vault, Espresso, Cassandra, Kafka)
+ ***Capsule Networks***
+ ***[Apache Airflow, Luigi](https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7), workflow management system (WMS), Azkaban, [Open Source Data Pipeline – Luigi vs Azkaban vs Oozie vs Airflow](https://www.bizety.com/2017/06/05/open-source-data-pipeline-luigi-vs-azkaban-vs-oozie-vs-airflow/), [Pinball](https://robinhood.engineering/why-robinhood-uses-airflow-aed13a9a90c8), Airbnb Airflow vs Apache Nifi***
	- ***Jenkins vs Airflow. Jenkins is an open source continuous integration tool written in Java.***
+ Apache Kafka Connect framework
+ Building out best in class stream processing solutions like Kafka Streams and KSQL to perform rich, real time, transformation and querying of data in Kafka
+ skill set:
	- Modernizing Kafka to make it infinitely scalable, elastic, and globally replicated
	- Building out best in class stream processing solutions like Kafka Streams and KSQL to perform rich, real time, transformation and querying of data in Kafka
	- Building a scalable & highly available observability stack for cloud and on premise
+ skill set:
	- Experience with React/Flux, modern js tooling (Gulp/Grunt)/Webpack/Babel
	- Excellent understanding of JavaScript, HTML5, and CSS3
	- Experience with writing/monitoring/managing large scale system deployments
+ skill set:
	- Modernizing Kafka to make it infinitely scalable, elastic, and globally replicated
	- Building out best in class stream processing solutions like Kafka Streams and KSQL to perform rich, real time, transformation and querying of data in Kafka
	- Revolutionizing how data pipelines are built through Kafka Connect
	- Running all of the above in our very own elastic, scalable cloud offering
+ Exposure to big data systems like Hadoop, Spark, Kafka, etc.
+ skill set:
	- Design, implement, and maintain platform metadata features
	- Designing APIs and Platform Information Architecture
	- Serve as primary point of contact in one or more platform metadata areas
	- Collaborate with various Confluent Engineering groups to provide strong technical guidance and leadership related to managing metadata
	- In depth experience with concepts of distributed systems and big data such as - Kafka, Hadoop, Spark, Big Table, HBase etc
	- Full stack experience
	- Experience with Lineage, Governance and Auditing
	- Experience in ML/Data Engineering
	- Experience working with Docker or Kubernetes is a plus
	- Experience working with AWS, Azure, and/or GCP
+ skill set (Platform DevOps Engineer):
	- As a Platform DevOps, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience with Go, Java, C++ or Python required
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- As a Platform DevOps Engineer, you will be designing and implementing a control plane to manage the life cycle of Confluent Platform using tools such as Ansible, Terraform etc. You will be responsible for building an extensible and easy to use control plane to enable deployment, elastic scaling, monitoring, and self-healing of various Confluent Platform components. We are looking for engineers with a strong desire to build a pluggable and extensible control plane with a strong emphasis on user experience.
	- Experience in Platform/Infra deployment and configuration management frameworks such as Ansible, Terraform, Chef, Puppet etc
	- Experience building and operating extensible, scalable resilient systems
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Knowledge of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Knowledge of Apache Kafka
	- Experience building APIs
+ skill set:
	- Solid understanding of container orchestration systems such as Kubernetes, Mesos, etc.
	- Experience with C, C++, Java or Python required
	- Experience with container orchestration tools such as Docker, CoreOS, etc.
	- Experience with configuration management or provisioning tools such as chef, puppet, Ansible, etc
	- Experience building and operating large-scale systems
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Hands-on experience with Kubernetes operator, Helm, or StatefulSets is a plus
	- Proficiency in Go is a plus
	- Knowledge of Apache Kafka is a plus
+ skill set:
	- 5+ years proficiency with Java, Go, or C/ C++
	- 3+ years experience designing and implementing cross component or platform wide features
	- In-depth experience in one of the following areas: Encryption at Rest, Key Management, Kerberos, LDAP
	- Working knowledge of TLS
	- Familiar with concepts of big data- Hadoop, Spark, Kafka, NoSql
	- A self-starter with the ability to work effectively in team with excellent spoken / written communication
	- Familiarity with GDPR, HIPAA or PCI is a plus
	- Experience working with hardware security modules is a plus
	- Experience working with Kubernetes is a plus
	- Experience working with AWS, Azure, GCP is a plus
	- Experience working with SQL is a plus
+ skill set:
	- 3+ years experience designing and implementing cross component or platform wide features
	- Working knowledge of TLS
	- Familiar with concepts of big data- Hadoop, Spark, Kafka, NoSql
	- Experience working with hardware security modules
	- Familiarity with GDPR, HIPAA, PCI or other compliances
	- Experience working with Docker or Kubernetes
	- Experience working with AWS, Azure, GCP
+ Experience building and scaling automation frameworks
+ skill set:
	- As a Software Engineer, Cloud Control Plane, you will be designing and implementing a distributed control plane to manage the life cycle of Confluent Platform using Container Orchestration Frameworks such as Kubernetes and Mesos. Control plane is responsible for unified cluster management of Confluent Platform across cloud providers and on-prem data centers including provisioning, auto scaling, monitoring and auto-remediation. We are looking for engineers with a strong desire to build planet-scale, extensible control plane with a strong emphasis on user experience.
	- Strong software design and implementation skills in building infrastructure-frameworks
	- Deep expertise in building distributed systems
	- Experience building and operating extensible, scalable resilient systems
	- Solid understanding of Container Orchestration framework (such as Kubernetes, Docker Swarm or Mesos)
	- Familiarity with using Cloud Infrastructure Providers such as AWS, GCP, and Azure
	- Solid understanding of basic systems operations (disk, network, operating systems, etc)
	- Experience in building control planes in one or more of virtualization, software defined networking/storage
	- Experience building APIs, both RESTful and gRPC/Thrift based
	- Familiarity with Infra such as Networking, Storage and Security in data centers
	- Hands-on experience with Kubernetes operators, Helm or StatefulSets
	- Knowledge of Apache Kafka
	- Experience in App deployment and config management frameworks such as Ansible, Terraform, Chef, Puppet etc.
+ skill set:
	- Build and maintain data foundations, metrics and dashboards to monitor the business performance and extract actionable insights
	- Apply quantitative analysis, data mining, and presentation of data to fuel business growth and drive customer success
	- Design and analyze experiments to test new product ideas, go to market strategies and/or funnel optimization; Convert the results into actionable recommendations
	- Build data products to improve operational efficiencies organizationally to scale with a hyper growth start-up
	- Inform, influence, support, and execute business decisions with senior leadership and business partners
	- Build robust, automated data pipelines to enable team effectiveness
	- 2+ years industry experience working with SQL (Teradata, Oracle, MySQL, BigQuery, etc.) and R (or Python)
	- Proficiency in applying statistical modeling and/or machine learning
	- Proficiency in data visualization (eg. Tableau, Looker, Matlab, etc.)
	- Bachelor or advanced degrees in a quantitative discipline: statistics, operations research, computer science, informatics, engineering, applied mathematics, economics, etc
	- The ability to communicate cross-functionally, derive requirements and deliver insightful analysis and/or models; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience building data warehousing and ETL pipelines
	- Experience with Unix/Linux environment
	- Experience in developing data apps with Python/Java, high charts etc
	- Excellent communications skills, with the ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience working in the B2B growth/marketing or sales domains: CRM, sales effectiveness, branding, segmentation, web analytics, attribution, funnel optimization, etc.
	- Experience working with Marketo, Google Analytics and SFDC
+ skill set:
	- The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.
	- We are looking for a talented and driven individual to build and scale our data analytics infrastructure and tooling. This person will build state of art data warehousing, ETL, and BI platforms, to make data accessible to the entire company. He/she will also partner closely with data scientists and cross functional leaders to develop internal data products. Data engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
	- Collaboration with data scientists, engineers, and business partners to understand data needs to drive key decision making throughout the company
	- Implementing a solid, robust, extensible data warehousing design that supports key business flows
	- Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; Deploy inclusive data quality checks to ensure high quality of data
	- Developing strong subject matter expertise and manage the SLAs for those data pipelines
	- Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
	- Partnering with data scientists and business partners to develop internal data products to improve operational efficiencies organizationally
	- Building and growing  partnership with cross functional teams, and evangelize data-driven culture
	- Contributing to innovations that fuel Confluent’s vision and mission
	- 4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
	- Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
	- Highly proficient in Python and SQL coding
	- Highly proficient with tuning and optimizing data models and pipelines
	- Experience in developing data apps with Python, Javascript, high charts etc
	- The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
	- Experience with Apache Kafka
	- Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc
	- Experience in developing data apps with Python, Javascript, high charts, etc
+ Experience working with Real-time Collaboration, SAML, SCIM, or OpenID preferred
+ Exposure to MariaDB or other RDMS
+ skill set:
	- Experienced in JAXRS; JAXB; AMQP JMS; LDAP and SNMP.
	- Experienced in data streaming;  Apache Kafka a plus
	- Experienced in design and development of Security policies, Authentication/Authorization such as OAuth, JWT.
+ skill set:
	- Highly experienced in Mongo DB.
	- Experienced in Data Structures (know what to use when, and time complexities involved).
	- Experienced in Spring and designing Restful APIs.
	- Experienced in developing Microservices.
	- Experienced in HTTP cycle and middleware architecture.
	- Experienced in design and development of distributed systems and product scaling.
	- Consistently demonstrate ability to design and deliver a project/task/enhancement/epic, considering every use case.
	- Experienced in JAXRS; JAXB; AMQP JMS; LDAP and SNMP.
	- Experienced in data streaming;  Apache Kafka a plus
	- Experienced in design and development of Security policies, Authentication/Authorization such as OAuth, JWT.
+ skill set:
	- Experience with the following, especially when applied to improve software security, resiliency and maintainability:
	- Runtime monitoring
	- Runtime verification
	- Dynamic program transformation and instrumentation
	- Host-based intrusion detection
	- Virtual machine introspection
	- Security policy languages and specifications
	- Software isolation or sandboxing
	- Profiling
	- Fault analysis and isolation
	- Embedded systems
	- Low-level programming at the kernel, hypervisor, firmware, or BIOS level
	- Penetration testing
+ skill set:
	- Experience with fuzzers, at least using and configuring them; experience with AFL especially useful
	- Experience with symbolic execution
	- Experience with binary analysis, an ability to read assembly would be a plus
	- Experience with Windows binaries
	- Experience with penetration testing (e.g., using MetaSploit) or vulnerability demonstration
+ skill set:
	- Experience with hypervisor / container development
		* Especially, Xen or OpenXT
	- Experience with Trusted Platform Module (TPM)
	- Experience with firmware-level code
	- FPGA physical design
	- Experience with device characterization or PUF techniques
	- Experience with ASIC analog and/or digital design
+ skill set:
	- Familiarity with analytics notebooks (Jupyter, RStudio, DataBricks)
	- Strong programming skills and ability to utilize a variety of data/analytic software/languages/tools; e.g., Spark (ML, Mllib, Spark SQL), R (caret, ggplot2), Python (pandas, numpy, scipy, scikit-learn), Scala, Hive, SQL, SAS, Tableau, etc.
	- Familiarity with cloud computing (in AWS or Azure)
	- Deep knowledge of a variety of statistical and data mining concepts and procedures including: generalized regression, machine learning algorithms, deep learning, media mix algorithms, and statistical graphics
	- Predictive Analytics experience desired
	- Experience with big data- Spark, Hive, Hadoop desired
	- Designing and overseeing implementation of solutions for non-routine problems utilizing a large array of know-how areas within analytics e.g. generalized regression, decision tree, non-parametric; and machine learning, e.g., gradient boosting, random forest, neural networks, clustering, pattern recognition
	- Developing best practices and repeatable processes for routine problems arising in business problems business cases including driving targeted marketing campaigns for tune-in and product adoption, creating an enhanced consumer experiences, and developing digital/social advertising audience segments
	- Assisting with strategic decisions about processes, frameworks and standards
+ Strong programming skills and ability to utilize a variety of data/analytic software/languages/tools; e.g., Spark (ML, Mllib, Spark SQL), R (caret, ggplot2), Python (pandas, numpy, scipy, scikit-learn), Scala, Hive, SQL, SAS, Tableau, etc.
+ skill set:
	- Proficient in SQL/Hive
	- Proven ability to apply scientific methods to solve real-world problems
	- Knowledgeable about the machine learning trade-offs and model evaluation
	- Over 4 years of industry experience with proven ability to apply scientific methods to solve real-world problems on web scale data
	- Ability to lead initiatives across multiple product areas and communicate findings with leadership and product teams
+ skill set:
	- Experience in building and owning critical user-facing API systems, and solving scaling, latency, and performance problems in high-volume low-latency distributed systems
	- 8+ years of industry experience with distributed systems, transactional data stores, and systems programming
+ Experience with MapReduce/Hadoop and/or distributed systems.
+ Expertise in design of scalable backend systems with experience in AWS, Kafka, Hive, MySQL
+ Extensive experience with one or more of the follow frameworks. (Spark, Druid, Hadoop, HBase, Kafka)
+ Hands-on experience with open source big data platforms (Hadoop, Hive, Presto) and familiarity with data visualization (Tableau, D3) technologies
+ Familiarity with implementing metric logging and interpreting metrics to make decisions
+ skill set:
	- Deep knowledge of a configuration management tool (i.e. Puppet, Chef, Ansible, Salt, CFEngine). Experience with containers is a plus
	- Familiarity with distributed systems including service discovery, pub/sub, search indexing, storage, and caching. We use Zookeeper, Kafka, Elasticsearch, MySQL, Hbase, and Memcache respectively.
+ The successful candidate must be an expert in field solver-based parasitic extraction and be able to quickly become an expert in new simulation approaches and to develop robust, maintainable, and efficient code.
+ skill set:
	- Able to solve a wide-range of difficult problems in imaginative and creative ways, exercising judgment within broadly defined practices and policies.
	- MSc in Computer Science, Applied Mathematics or related field with 3+ years of experience, or BSc with 5+ years of experience
	- Proficiency in developing and maintaining modern C++ based applications in a Unix/Linux and Windows environment. Proficiency in Qt, Python, and Tcl a plus. Experience with OpenAccess also a plus.
	- Experience in developing enterprise level software, proficiency with debug and configuration management tools as well as quality and performance metric tools.
	- Strong communication skills and ability to write specifications and reference documentation.
	- Proficiency in English is a must.
	- Interest in high performance data structures and algorithms.
	- Prior experience with or developing CAD/EDA tools and/or hardware design also a plus as is experience with geometric algorithms.
	- Excellent organizational, prioritization, time management skills and an unwavering commitment to integrity and professionalism.
	- Self-starter and strong closer with multitasking ability
	- Any other duties as assigned by the Department head
	- Computational Geometry/Topology
	- Graph theory
	- Pattern recognition/machine learning
	- Compilers/parsers (experience with Flex/Bison a plus)
	- Computer architecture (caching, memory, networking, etc.)
	- Boost
	- Test Driven Development
	- Displays strong analytical abilities both quantitative and qualitative.
	- Excellent communication skills and the ability to interface with all levels of management.
	- Relies on experience and judgment to plan and accomplish goals.
	- Performs a variety of complicated tasks - a certain degree of creativity and latitude is required.
	- A key requirement of this role is being the master of all details.
	- Ability to multi-task and handle matters with little supervision and with excellent follow up.
	- A strong entrepreneurial and can-do mindset, undaunted by shifting priorities, uncertainty, and a “figuring it out as we go” environment.
	- Enough courage to say “I don’t know”.
+ skill set:
	- Experience with parallel programming, especially pthreads, OpenMP, and MPI
	- Strong mathematical fundamentals, including linear algebra and numerical methods
	- Experience in implementing direct and iterative solvers for the solution of large sparse linear systems
	- Experience with CUDA or OpenACC
+ skill set:
	- Background in 3D computer graphics, including APIs such as OpenGL
	- Proficient in Java, Maven, Python, Jenkins/Groovy, Vagrant/Docker
	- Good knowledge in DFT : OCC insertion, ATPG generation
+ skill set:
	- Imagimob is a fast-growing, high-tech startup with an exciting future ahead. We are currently developing our next generation hybrid AI platform that allows for advanced motion detection for smaller Internet-of-things-articles, of virtually every kind. For example, the technology is today being used in projects ranging from the automotive and manufacturing industry to the health sector.
	- We are looking for a Machine Learning / AI Application Engineer to join our development team in Stockholm. Do you have excellent programming skills and are interested in working in the frontline of artificial intelligence? Then this position might be something for you.
	- Working with us you will get the opportunity to become part of our cross functional team with creative and innovative software engineers and AI researchers building the next generation AI beyond Deep Learning. Since we are still in a startup phase, you will also be able to develop in the areas you find the most interesting.
	- Has excellent programming skills in one or several languages, preferably in C, C# or Python
	- Experience with a deep learning framework (e.g. tensorflow, keras, Torch, caffe)
	- University degree or equivalent experience in computer science, electrical engineering, engineering physics or similar
	- A passion for Artificial Intelligence and Machine Learning technologies
	- Extreme ownership and go-get attitude
	- Has experience from programming on embedded platforms
	- Good knowledge in signal processing, statistics and its practical applications
	- Experience from Artificial Intelligence and Machine Learning technologies
	- And if this is not enough, you will get the chance to change history and shape the future of humanity...
	- The opportunity to be part of building the next generation AI beyond deep learning 
	- Being part of an excellent international engineering team with highly motivated individuals striving for a common goal
	- A chance to get to solve real world problems using AI 
	- A prestigeless and an open minded company culture
	- Short decision paths, we love getting things done
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- DevOps and systems experience is highly valued; If you’ve gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we’d love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- JavaScript, NodeJS, Java, GraphQL, Ruby, Python, and whatever’s needed
	- AWS Serverless Cloud Architecture with IaC using Terraform
	- Agile, Scrum, CI/CD, TDD and other best practices
	- Unit Tests, Integration Tests and End-to-End Tests
	- Other tools: Serverless.com, Express, Jest, Jenkins, Polly.js, Yarn, NPM, ESLint, JSDoc, etc.
	- Design, develop, test, debug, and document new and existing software features to ensure that software meets business, quality and operational needs
	- Lead software development of business requirements closely working with product owners and other stakeholders
	- Ensure that high quality code is delivered by following best practices like peer code reviews, code standards, unit testing and test-driven development
	- Drive and participate in code and document reviews, mentoring team in best practices
	- Work with business and technical product owners to interpret and translate business needs to technical requirements
	- Evaluate and recommend tools, technologies and processes to ensure the highest quality and performance is achieved
	- Monitor and troubleshoot code level problems quickly and efficiently
	- Apply deep technical expertise to resolve challenging programming and design problems
	- Focus on scalability, security and availability of all applications and processes
	- Design and architect solutions to enable secure, scalable and maintainable software
	- Contribute to technical roadmap and technical debt elimination, balancing time, resource, and quality constraints to achieve business and strategic goals and requirements
	- BS/MS in Computer Science or equivalent work experience
	- 7+ years hands-on experience developing scalable, distributed applications
	- Experience building robust web-based APIs using REST and/or SOAP
	- Strong development proficiency in one or more backend languages Node.js, Java, Ruby, python, Go, C#, etc.
	- Strong experience with building on top of cloud-based service providers like AWS
	- Experience in designing and deploying distributed microservices architecture
	- Possess strong verbal and written communication skills
	- Possess strong analytical skills with excellent problem-solving abilities
	- Must be extremely detail-oriented with respect to documentation and communication
	- Experience with Agile development, preferably Scrum
	- Experience with code management using Git & build using Maven
	- Experience with Jenkins for Continuous Integration/Continuous Deployment
	- You enjoy making highly scalable and highly available distributed systems
	- You write clean, testable and effective code
	- You hold yourself and others to high technical standards (design, architecture and implementation)
	- You have a deep understanding of object-oriented design and at least one modern backend framework
	- You enjoy shipping features following agile methods
	- You are a talented Software Engineer who is passionate about code quality, usability, and technology
	- You are a power user of infrastructure, keeping yourself up-to-date with the latest trends and breakthroughs in platform development technology
	- You have a strong record of project execution and completion and have experience with Scrum and agile development practices
	- You love working with smart people and want to be part of a team
	- You are excited by the challenge of pushing the limits of the infrastructure to deliver disruptive, innovative solutions to the world that will delight your customers
+ skill set:
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Improve observability with distributed tracing for all requests from client to CDN to load balancer to cluster and back again
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
	- Build observability into every aspect of our production infrastructure
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- Help developers smoke-test better by bringing canary analysis and automated scale testing into their world
	- DevOps and systems experience is highly valued; If you’ve gotten your hands dirty with package and configuration management, infrastructure-as-code principles, Kubernetes, AWS, Linux and security, PostgreSQL replication, and know your way around Docker, bash and Python, we’d love to talk with you
	- You should be passionate about getting in front of problems instead of waiting until things are on fire. If you dream of stability, love metrics, communicate well, document your code, and love building reliable systems that hum along and take care of themselves, we want you on our team
+ skill set:
	- 2+ years of work experience developing and deploying production-quality code
	- Foundational knowledge of commonly used machine learning techniques, such as cluster analysis, classification methods, and linear and nonlinear regression modeling
	- Experience developing applications using Natural Language Processing techniques.
	- Experience working with cross-functional teams in a dynamic environment
	- Hands-on experience building deep learning models on text corpora, preferably using PyTorch and Tensor Flow
	- Experience building machine learning models in the healthcare domain
	- Experience using AWS infrastructure and tools for machine learning
	- Experience with other back-end software engineering frameworks
	- [Talkspace](https://www.talkspace.com/)
+ knowledge of CUDA / OpenCL / OpenCV
+ skill set:
	- Ansible
	- Kafka/Cassandra
	- Linux
	- Git (github)
	- Vi / Vim
	- Elastic Search Stack
	- Graphite/Grafana
	- Data visualization
	- Python, Bash, Golang
	- Familiarity with JSON
+ skill set:
	- Experience with the full site of Go frameworks and tools, including dependency management tools (such as Godep, Sltr, etc.), Go's templating language, Go's code generation tools (such as Stringer), Popular Go web frameworks (such as Revel), Router packages (such as Gorilla Mux)
	- Ability to write clean and effective Godoc comments
	- Strong knowledge of Go programming language, paradigms, constructs, and idioms
	- Knowledge of common Goroutine and channel patterns
+ skill set:
	- Understanding and experience with NoSQL such as MongoDB or Neo4j
	- Experience with the Hadoop ecosystem (HBase, MapReduce, Hive/Pig) or Spark
+ skill set:
	- NoSQL databases experience - Dynamo
	- Experience with Python or other scripting language
	- Amazon RedShift experience - Nice to have
	- Experience with Grafana & Graphite
	- Experience with DynamoDB, Git, RedShift, Athena, Spark, CloudFormation, Terraform, Perl, Python or Go
	- Amazon AWS experience, particularly RDS, Aurora
	- 5+ years experience with managing high transaction volume MySQL systems
+ skill set:
	- Expertise with 12 Factor application principles
	- Containers (Docker, Kubernetes...)
	- Streaming/logging technologies (ElasticSearch, fluentd, LogStash, Kafka)
	- Message Queueing (Kafka, SQS...)
	- Coding and scripting languages (Perl, Bash, Python, Go...)
	- AWS Ecosystem (EC2, VPC, S3, DynamoDB, RDS...)
	- You have deployed and configured a wide range of AWS services including databases, networking, and security. In this role you will work with such paradigms and technologies as: 12 factor app design principles, Docker, Kubernetes, and ElasticSearch ecosystem
	- Support build/deployment processes with eye towards improving our CI/CD pipeline
	- Help troubleshoot production issues and perform root cause analyses that create effective mitigation strategies
	- Design, implement, monitor, and scale self-service oriented infrastructure
+ skill set for Autodesk AI Lab, Pier.9 at San Francisco:
	- [BrickBot](https://www.fastcompany.com/90204615/autodesks-lego-model-building-robot-is-the-future-of-manufacturing)
	- [Auto Sketching and Vectorization](https://canvasdrawer.autodeskresearch.com/)
	- [Topology Optimization for Specific Manufacturing Processes](https://www.autodesk.com/customer-stories/general-motors-generative-design)
	- Exploring and developing new Machine Learning models and techniques
	- Constantly reviewing relevant Machine Learning literature to identify emerging methods or technologies and current best practices
	- Introduces creative approaches to research topics and generates new approaches, perspectives and solutions to research topics
	- Planning and designing research projects: specifying the problem and defining the project scope
	- Connecting with academics and institutions to build relationships and collaborate
	- Realizing solutions through prototypes
	- Exploring new data sources and discovering techniques for best leveraging data
	- Collecting and performing data analysis to validate and further new theories and discoveries
	- Publishing and talking at conferences
	- Working closely with product engineers to design, develop and incorporate AI solutions into new products
	- Meeting with customers to understand how ML could be applied to their problems
	- Thinking strategically about research directions
	- Mentoring more junior researchers and engineers
	- An MS or PhD in a field related to Machine Learning such as: Computer Science, Mathematics, Statistics or Physics
	- Significant doctoral or post-doctoral research experience or 5 or greater years of work experience
	- Truly excited by the pace of advancement in AI research and technology
	- Understanding of fundamental CS algorithms and their scaling behaviors
	- Solid background in statistical methods for Machine Learning: Bayesian methods, dimensional reduction, SVD, clustering, classification, forms of regression, etc
	- Strong familiarity with Deep Learning techniques: various network architectures (CNNs, GANs, RNNs, Auto-Encoders, etc.); regularization; embeddings; loss-functions; optimization strategies; etc
	- Familiarity with one or more typical deep learning frameworks: TensorFlow, Caffe, MxNet, TORCH, PaddlePaddle, etc
	- Experience training and debugging networks
	- Strong coding abilities in:  Python and C/C++
	- Good communication skills and an awareness of how to communicate data and results effectively
	- Comfortable working in newly forming ambiguous areas where learning and adaptability are key skills
	- At times, the ability to lead and rally stakeholders and team members
	- Reinforcement Learning and other areas of Control Theory
	- Distributed Systems and High Performance Computing methods
	- Geometric Shape Analysis
	- Advanced simulation methods such as: FEA, CFD, Shape and Design Optimization, Photo-Realistic Rendering, etc
	- Knowledge Representation (semantic models, graph databases, etc.)
+ skill set:
	- Experience with Amazon Web Services (i.e. EC2, S3, RDS, CloudFront, CloudWatch, Lambda, CloudFormation, etc.)
	- Production React experience
	- Familiarity with responsive web design
+ skill set:
	- Our AI Labs focus on research in: deep learning, control systems, simulation and knowledge representation applied to diverse areas such as: geometry, robotics, advanced sensing, design exploration and sustainable engineering or construction practices. The labs also host product engineers resulting in early productization of our research, so you can see your work in action.
	- You will be a senior researcher focusing on problems related to geometry understanding, manipulation and synthesis.
	- The Lab brings together AI Researchers, Software Engineers and specialists in various problem areas to create novel AI solutions in all the areas mentioned above and more. They work closely with experts in: geometric modeling, simulation systems, robotics, knowledge representation, sensing and computer vision, industrial manufacturing and construction techniques.
	- Explore and develop new Machine Learning models and techniques
	- Constantly review relevant Machine Learning literature to identify emerging methods or technologies and current best practices
	- Introduce creative approaches to research topics and generates new approaches, perspectives and solutions to research topics
	- Plan and design research projects: specifying the problem and defining the project scope
	- Connect with academics and institutions to build relationships and collaborate
	- Realize solutions through prototypes
	- Explore new data sources and discover techniques for best leveraging data
	- Collect and perform data analysis to validate and further new theories and discoveries
	- Publish and talk at conferences
	- Work closely with product engineers to design, develop and incorporate AI solutions into new products
	- Meet with customers to understand how ML could be applied to their problems
	- Think strategically about research directions
	- Mentor more junior researchers and engineers
	- An MS or PhD in a field related to Machine Learning such as: Computer Science, Mathematics, Statistics or Physics
	- Significant doctoral or post-doctoral research experience or 5 or greater years of work experience
	- Solid theoretical background in geometry and geometric methods (e.g. shape analysis, topology, differential geometry, discrete geometry, functional mapping, etc.)
	- Good background in statistical methods for Machine Learning (e.g. Bayesian methods, HMMs, Graphical Models, dimensional reduction, clustering, classification, regression techniques, etc)
	- Familiarity with Deep Learning techniques (e.g. Network architectures; regularization techniques; learning techniques; loss-functions; optimization strategies etc)
	- Familiarity with one or more typical deep learning frameworks: TensorFlow, Caffe, MxNet, TORCH, Chainer, etc.
	- Strong coding abilities in: Python and C/C++
	- Good communication skills and an awareness of how to communicate data and results effectively
	- Comfortable working in newly forming ambiguous areas where learning and adaptability are key skills
	- At times, the ability to lead and rally stakeholders and team members
	- Reinforcement Learning and other areas of Control Theory
	- Distributed Systems and High Performance Computing methods
	- Advanced simulation methods such as: FEA, CFD, Shape and Design Optimization, Photo-Realistic Rendering, etc.
	- Knowledge Representation (semantic models, graph databases, etc.)
+ skill set:
	- We are implementing a new master data management system within a new backend service API, using modern best practices such as REST, microservices, CI/CD, and polyglot persistence.
	- Our team is versatile, polyglot, and cross-functional. We work in Python, AWS, NodeJS, Scala, Elasticsearch, Informatica, Terraform, and much more. The team is geographically dispersed, and values transparent, collaborative communication and cooperation.
	- Ownership of platform and domain architecture for Enterprise Data Management systems and related components in the Customer Domain
	- Consulting and collaborating with all stakeholders to adequately understand requirements for software architecture and implementation.
	- Defining and drafting architecture artifacts, roadmaps, and program vision for consumption across technical and non-technical teams.
	- Evangelizing, defending, and adapting architecture decisions and direction according to business drivers
	- Define and consider the full lifecycle of software systems and components
	- Advise, mentor, educate, and influence teams across Autodesk
	- Continuous professional development, to ensure awareness and expertise in leading-edge best practices and evolving technology drivers and landscapes
	- Consciously monitoring implementation to ensure non-functional requirements such as availability, scalability, reliability, security, compliance, governance are all in place
	- 10+ years’ experience delivering software with a strong focus on data, working throughout a cross-section of the software engineering spectrum (APIs, front-end, backend, security, authentication/authorization, data, service, persistence layers, automated testing, etc.)
	- Demonstrable experience as an architect for a large organization in major software domains (CRM, ERP, HR, etc.)
	- Extensive experience integrating software domains within an enterprise
	- Ready to apply and adhere to best practices and standard methodologies in software and architecture
	- Strong experience in API (REST/SOAP) and Pub/Sub architecture with hands on implementation experience at scale
	- Experience architecting for applications on cloud infrastructure (AWS, Azure)
	- Strong experience using and designing for relational/non-relational databases
	- Familiarity with a large cross-section of current software landscape
	- Not dogmatic about a particular approach and are ready to take the best idea in the room
	- A strong sense of ownership with a bias for action
	- Superior communication and cooperation skills
	- Ability to work independently and collaboratively across an organization
	- Confidence to lead and stand by recommendations in the face of challenges
	- Ready to try and take-on new technologies, ideas, and/or engineering challenges
	- Eagerness to learn and share knowledge with a good attitude
	- Experience with Master Data Management (MDM) theory and platforms
	- Experience with SAP and/or ERP systems
	- Experience with Salesforce and/or CRM systems
	- Experience with Compliance and Data governance (GDPR, SOX, etc.)
	- Experience implementing Domain Driven Design
	- Continued hands-on code-level expertise
+ skill set:
	- Experience using tools such as Apache Maven/ANT/Jenkins/Hudson
	- Knowledge of Load and performance testing using open source tools such as JMeter or LoadUI
+ skill set:
	- Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Python, Elastic Search, Storm, Kafka, Oozie WFs etc. in both an on premise and cloud deployment model to solve large scale processing problems
	- Design, build and maintain Big Data workflows/pipelines to process billions of records into and out of our data lake
	- Provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics, user access, and security
	- Proficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue
	- Keen understanding of big data and parallelization accompanied with a stellar record of delivery
	- Experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred), AWS Glue
	- Experience with on-premises to cloud migrations including re-hosting, re-platforming and re-factoring
	- Experience with orchestration template technologies such as AWS CloudFormation
+ skill set:
	- Content delivery network (CDN) experience
	- Experience with Amazon Web Services (i.e. EC2, S3, RDS, CloudFront, CloudWatch, Lambda, CloudFormation, etc.)
+ skill set:
	- Strong knowledge and experience with server-side Java applications
	- Familiarity and experience with Adobe AEM (CQ) and related technologies
+ Experience with Node.js, npm, Gulp, and Webpack
+ Load and performance testing using open source tools such as JMeter or LoadUI
+ Experience with using other product web end-points/APIs
+ skill set:
	- 10+ Years of professional software engineering experience in building large-scale distributed systems
	- Strong hands-on experience in developing applications in one or more language stacks: Java, Python, Go
	- Strong experience in building platform-level shared libraries, frameworks, components, tools and services
	- Strong understanding of object-oriented programming, service-oriented architectures, microservices and design patterns
	- Strong hands-on experience in one or more of Containers and Container Orchestration frameworks: Docker, Kubernetes, Docker Swarm, Amazon ECS, Amazon EKS, AWS Fargate, etc.
	- Strong hands-on knowledge of one or more of Infrastructure-as-Code tools and technologies: Terraform, AWS CloudFormation, Packer, etc.
	- 3+ Years of experience in public cloud infrastructures: AWS preferred
	- Experience with Service Mesh, Service Discovery, Routing tools and technologies: Istio, Consul, ZooKeeper, zuul, linkerd, envoy, etc.
	- Experience with Metrics, Monitoring & Alerting tools: Catchpoint, Sensu, Prometheus, Nagios, Zabbix, InfluxDB, Graphite, Grafana, AWS CloudWatch, Datadog, etc.
	- Experience with APM tools: New Relic, Dynatrace, etc.
	- Experience with Log Management tools: ELK stack, Splunk, etc.
	- Experience with secrets management, certificates, encryption and keys: Vault, AWS KMS, etc.
	- Experience with CI/CD, DevOps and Pipeline-As-Code: Jenkins
	- Exposure to Configuration Management Tools: Chef, Puppet, etc.
	- Exposure to Function-as-a-Service, AWS Lambda, Serverless, etc.
	- Experience with Agile software development and Scrum methodology
	- Practice strong software development principles and best practices: Test-driven development (TDD), CI/CD, code refactoring, coding standards, etc.
+ BIM (Building Information Model)
+ skill set:
	- Deep understanding of machine learning, statistical modeling and data mining techniques such as gradient boosting, neural networks, natural language processing and clustering
	- Understanding of experimental design and adaptive sampling
	- Experience working with big data platforms (Hadoop, Spark, Hive)
	- Experience working with relational SQL and NoSQL databases
	- Familiar with ML and statistical modeling tools such as R, SparkML, TensorFlow, SciKit
	- Proven track record overseeing multiple data science projects at all stages, from initial conception to implementation and optimization
	- Good programming skills using analytics-oriented languages such as Python, R and Scala
+ skill set:
	- Experience working with relational SQL and NoSQL databases
	- Experience working with big data platforms (Hadoop, Spark, Hive)
	- Fluency with one or more programing language: Python, Java, Scala, etc
	- Good understanding of CS fundamentals, e.g. algorithms and data structures
	- Experience with data science tools and libraries, e.g. R, pandas, Jupyter, scikit-learn, TensorFlow, etc
	- Familiarity with statistical concepts and analyses, e.g. hypothesis testing, regression, etc
	- Familiarity with machine learning techniques, e.g. classification, clustering, regularization, optimization, dimension reduction, etc
	- Guide the utilization or development of a robust CI/CD capability.
	- Identify key performance & effectiveness metrics, monitor & adjust to goals
	- Work with test automation team to lay the groundwork for automated API testing framework and test cases
	- Prioritize backlog & drive product releases
	- Distill strategic intent into structured product release roadmaps that are compelling and achievable
	- Ability to execute and manage performance and expectations within a cross-functional, matrix management environment
+ skill set:
	- Analyze existing programs or formulate logic for new systems, devise logic procedures, prepare flowcharts, may perform Marketo and Salesforce configuration
	- Test solutions and ensure they meet business requirements and are "fit-for-purpose." Present and validate solution with user
	- Experience setting up big data platform service providers like Qubole
	- Experience with AWS Services, which includes setting up S3 policies, IAM Roles, Enabling Cloudwatch Alarms, understanding AWS cost etc
	- Experience with Service Performance Monitoring tools for big data like DataDog, Unravel etc
	- Experience setting up Big Data Technologies like, HDFS, MapReduce, Hbase, Pig, Hive, Sqoop, Oozie, Spark, Cloudera manager, Kafka & Splunk
	- Experience setting up schedulers like Oozie and Airflow
	- Experience with building CI/CT/CD pipelines for AWS Services like Lambda, Glue, ECS and Firehoses
	- Experience with AWS including S3, EC2, Lambda, Kinesis, Firehose, Step functions, Cloudwatch, Cloud formation templates
	- Experience with Programming Languages - Java, Python, Linux Shell Scripts, PL/SQL
	- Experience with Databases: Databases: Oracle, MySQL, SQL Server, PostgreSQL & DynamoDb
	- Experience with Web Services like REST & SOAP
+ skill set:
	- Strong foundation in Python & MongoDB/DynamoDB
	- Experienced with software support tools for version control, issue tracking, collaboration, automation, containerization, document generation (JIRA, GIT, Artifactory, Confluence, Jenkins, Docker)
	- Experience with testing frameworks and/or AB testing tools
+ skill set:
	- Hands-on experience with AWS (Lambda, SAM, S3, DynamoDB, CloudFormation, EC2, IAM)
	- Experience building and interpreting data models and analytics dashboards
+ skill set:
	- Reduce RPO/RTO for our S3, RDS, Redis, MongoDB, etcd and PostgreSQL instances
	- Maintain/upgrade our Spinnaker + Kubernetes CI/CD pipeline, and the tooling that makes it all work, in a sane and reproducible way
	- Automate infrastructure deployments with CloudFormation and SaltStack to help us go multi-AWS region
+ Demonstrated track record working with data warehouse concepts.
+ skill set:
	- Experience with data quality processes, data quality checks, validations, data quality metrics, definition, and measurement.
	- Ability to operate with cross-functional teams (for example, customer support, data science, engineering, and sales), including a willingness to balance the changing needs of a client-facing team with a demand for data engineering best practices and the ability to communicate the tradeoffs.
+ skill set:
	- Experience with presentation or data visualization software, such as Microsoft PPT, Tableau, Shiny, etc.
	- Practical understanding of and experience with predictive analytics, machine learning, and/or causal inference
+ skill set:
	- Proficiency with statistical programming languages (R, Python, etc.) and proven ability to work pragmatically with statistical concepts
	- Practical understanding of and experience with predictive analytics, machine learning, and/or causal inference
+ Proficiency with machine learning and statistical modeling (e.g., scikit-learn, TensorFlow, Stan)
+ Experience identifying data quality and developing automated QC checks and/or reports
+ skill set:
	- Experience with automation tools and configuration-as-code (CloudFormation, Ansible, Puppet, Chef, Vagrant, etc.)
	- Experience working with either AWS or GCP services such as compute, databases, VPCs, networking, permissioning and storage
+ skill set:
	- Significant experience with several of the following:
		* Leading technical teams
		* Experience with project management and/or UI/UX design
		* Python, Ruby, and/or Go (golang)
		* Designing and building APIs
		* Query optimization, database administration, analytics databases, and/or NoSQL
		* Scaling and ensuring reliability of large SaaS applications
		* Automated software testing and continuous integration
		* Cloud application deployment and monitoring
		* Proficiency working with Amazon Web Services (AWS)
		* Data visualization for the web (using D3 or similar)
		* React or AngularJS
		* Statistics and predictive modeling (using tools like pandas, scikit-learn, NumPy, SciPy, R, STATA)
+ skill set:
	- Experience working with either AWS or GCP services such as compute, databases, VPCs, networking, permissioning and storage
	- Experience with automation tools and configuration-as-code (CloudFormation, Ansible, Puppet, Chef, Vagrant, etc.)
	- Experience with continuous integration/delivery services
	- Experience with containerized code deployment
	- Experience with networking concepts and protocols
	- Experience managing large cloud infrastructures
	- Experience scaling and ensuring reliability of large SaaS or PaaS applications
	- Experience with orchestration frameworks such as Kubernetes or Mesos
	- Experience with security, systems, or application monitoring and metrics
	- Cloud application deployment and monitoring
	- Data visualization for the web (using D3 or similar)
	- Statistics and predictive modeling (using tools like pandas, scikit-learn, NumPy, SciPy, R, STATA)
	- Query optimization, database administration, analytics databases, and/or NoSQL
+ Familiarity with GraphQL and Relay
+ skill set:
	- Expert knowledge of debugging and crash dump analysis in Windbg;
	- Experience with system level tools like Process Monitor;
	- Experience with profiling tools like PerfView (CPU, Memory, Garbage collection);
	- Experience with in process and out of process COM and how it works in .NET
+ skill set:
	- Retargeting of a C/C++ compiler towards specific microcontroller architecturesman
	- Activities comprise participation in the development, maintenance, build, test, and release of compiler and run-time libraries for existing and forthcoming processor architectures, including competitive performance analysis, root cause analysis, and bug resolution
	- 6+ years of experience with SQL databases (we use Postgres) and data manipulation
+ skill set:
	- Experience with CI systems (Jenkins, TeamCity);
	- AWS - EC2, RDS, ECS etc;
	- Docker and orchestration: Swarm, Kubernetes;
	- Elastic Search, RabbitMQ;
	- Bash / Powershell scripting experience;
	- Windows / Linux - admin level;
	- Experience with SVN / GIT - as a user and as an infrastructure owner;
	- Experience with high loaded distributed multi-tenanted cloud systems. Including: Disaster recovering mechanisms; Monitoring and logging; Redundancy (data, network, apps);
	- Comfortable working with distributed teams
+ skill set:
	- Digital Design Technologies is a software development group inside the Tesla design studio. The team creates state-of-the-art tools which improve the way Tesla conceives its products.
	- Although the main focus point will be surface-centered math, successful applicants will also be involved in key software development.
	- Implement techniques and formulas into usable algorithms
	- Create new mathematical formulas to solve complex surface-related problems
	- Thorough understanding of computational surface mathematics
	- Knowledge of the mathematics inherent to NURBS and Subdivision surfaces
	- Experience with real time technologies preferred but not required
+ skill set:
	- Familiarity with board /chip bringup  
	- Experience with real-time operating systems (RTOS) like FreeRTOS, Threadx etc
	- Experience with writing device drivers for low speed interfaces like I2C, SPI, UART, CAN etc
	- Familiarity with containerization (e.g. Docker).
	- Experience with one of the following programming languages: Python, Go, Java/Scala/Kotlin.
	- Experience in creating complex, highly distributed real-time embedded systems.
+ skill set:
	- You will also work closely with our data scientists to make sure our customers have the necessary tools to perform high quality data integrations by building out the Machine Learning and AI infrastructure for entity resolution, automated data mapping, predictive analytics, and risk analysis.
	- As a Software Engineer Intern you will work with a mentor to improve storage, compute, privacy, security, and compliance features necessary to support the operational workflows that help people get the assistance they need.
+ skill set:
	- You will help to ensure great quality of Tesla’s Autopilot software for current and next generation vehicle programs and working towards Tesla’s vision of fully autonomous vehicles. You will be contributing to the implementation of the software system that processes inputs from a variety of vehicle sensors, evaluates possible vehicle strategies/trajectories, and automate safe control of the vehicle.
	- Demonstrate good understanding of software fundamentals including software design, algorithm development, data structures, code modularity, and maintainability.
	- Experience developing embedded firmware in C for safety-critical applications in production environments.  
	- Assess the system for failure modes and design resilient and redundant mechanisms to protect against those failures.
	- Experience in creating complex, highly distributed real-time embedded systems.  
	- Understanding of advanced driver assistance sensors such as radar, camera, ultrasonic, and lidar, including the measurement and data-reduction, target identification and environmental synthesis, and sensor fusion.
	- Collaborate with the control systems, simulation and modeling teams to design control strategies that can be implemented in software efficiently
+ skill set:
	- Strong C/C++ programming skills, preferably in an embedded environment
	- Experience with 32-bit and 64-bit ARM architectures ARMv8-A, ARMv8-M, ARMv8-R)
	- Familiarity with board /chip bringup  
	- Experience with real-time operating systems (RTOS) like FreeRTOS, Threadx etc
	- Experience with writing device drivers for low speed interfaces like I2C, SPI, UART, CAN etc
	- Familiarity with containerization (e.g. Docker).
	- Experience with one of the following programming languages: Python, Go, Java/Scala/Kotlin.
	- Problem solving, critical thinking, and communication skills
	- Strong build, debug and test skills
	- Git experience a plus
+ skill set:
	- As an intern within the Vehicle Software Team, you will have the opportunity to work on a variety of high voltage/high power systems which our customers rely on every day.  You will be responsible for designing and setting up test infrastructure, validating firmware functionality, investigating problems, implementing solutions, and automating test systems as needed to support a rapid pace of development and code delivery.   
	- Your effort to create and equip automated validation infrastructure will have a direct impact on the reliability and robustness of the Tesla products as well as the customer experience.  You will contribute to cross-functional system architecture, software system design, and rapid prototyping.
	- Your application to the Software Engineering Internship will be considered for all opportunities across Autopilot Embedded Systems, Body Controls, Gateway, Charging Systems, Battery Management Systems, Drive Inverter, Tools Development, Applications and/or Platforms Infrastructure.
	- Architect methods of integrating the automated test suites into the development processes
	- Explore and innovate methods of testing the robustness and quality of the charging firmware
	- Understand and deconstruct complicated software systems and devise strategies to test these systems
	- Create and execute test plans designed to expose weakness or faults in components
	- Work with developers to optimize the component validation process with the use of metric driven data
	- Capable of hands-on bring up, debug and code optimization.
	BS/MS Degree EE, CS, CE, Mechatronics/Robotics or equivalent required
	- Experience working with modern software architectures (STM32 microcontrollers, etc)
	- Expertise testing devices and debugging hardware (scopes, logic analyzers, DMMs, CAN loggers)
	- Proficiency in C and python a must
	- Ability to think creatively and produce “outside of the box” solutions
	- Familiar with the embedded microprocessor design process: compilers, debuggers, IDE and source code control
	- Familiarity with automotive ECUs, especially those in hybrid and electric powertrains.
	- Familiarity in schematic design & capture (Altium Designer)
	- Experience with bringing up embedded firmware projects on custom PCBs
	- Familiarity with Linux framework and designing code that runs on Linux platforms
	- Experience designing or interfacing with HIL / SIL test setups
	- Experience with source control (Git) and continuous integration (Jenkins)
	- Experience with DSPs, microcontrollers and real-time operating systems.
	- Experience in battery management systems
	- Experience with automotive or green energy background a plus
	- Interest in solving complex and time critical problems
+ skill set:
	- At the heart of Netflix Product Innovations is an experimentation driven culture led by Science & Analytics (S&A).  In this role, you will lead teams of data scientists and analysts responsible for shaping UI and Content Innovations decisions through experimentation (A/B, quasi) and empirical studies to guide product strategy.
	- Set an impact-focused, strategic science roadmap to guide product innovations.
	- Recruit and inspire exceptional data scientists focused on the span of causal inference, behavioral research, and analytical activities.
	- Uphold the culture of rigor in product decision-making through active participation in product debates.
	- Lead and contribute to cross functional initiatives between product development (product management, design, engineering), content, and marketing.
	- Define a team culture that balances supporting high impact business needs with forward looking research.
	- Serve as thought partner to product development executives across product management, engineering, and design.
	- 5+ years experience in building and inspiring a high-performing data science and analytics team.
	- Capacity and passion to translate business objectives into actionable analyses, and analytic results into business and product recommendations
+ A senior analytics professional with a proven track record of data analysis, reporting and visualization (e.g. Tableau, D3)
+ A background in machine learning and related sub-areas including ranking, personalization, search, recommendation, explore/exploit, causal learning, reinforcement learning, deep learning and probabilistic modeling.
+ skill set:
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
+ skill set:
	- Excellent understanding of video compression. Extensive experience with compression standards such as H.264/AVC, HEVC and VP9.
	- Strong background in image and signal processing, both algorithm design and implementation.
	- Implemented a video codec from scratch
	- Background in video quality metrics, video understanding, computer vision or machine learning
	- Experience with large-scale distributed systems and cloud-computing
	- Involvement in open-source multimedia projects (such as ffmpeg, x264, webm, VLC)
	- Design and prototype algorithms for improving the quality and performance in our cloud-based video ingest and encoding pipeline
	- Study current codec implementations and find areas for improvement in quality and speed
	- Conduct research on next-generation image and video coding and propose technology for industry standards
	- Participate in research conferences and standardization meetings
+ skill set:
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Cloud computing platforms and large web-scale distributed systems
+ skill set:
	- Recommender Systems and Personalization. Almost every aspect of the Netflix experience is personalized, and much of that personalization is driven by our various flavors of recommendation algorithms. You’ll apply a number of techniques, from the latest in deep learning, reinforcement learning, to causal inference.
	- Search Ranking and Query Understanding. You’ll work on the algorithms that allow our members to interactively query and explore our catalog. Using the latest in NLP techniques, you’ll solve problems including: query understanding, knowledge graph discovery, and learning to rank across our global catalog of titles.
	- Large Scale Machine Learning. Netflix is available in over 190 countries, with over 148+ million members. This gives us a unique dataset to work with, but also unique challenges in how we scale our models. You’ll work on cutting edge techniques to scale your models for use in our production systems.
	- Strong background in machine learning with a broad understanding of unsupervised and supervised learning methods
	- Strong software development experience
	- Successful track record of delivering results in complex cross-functional projects
	- Strong mathematical skills with knowledge of statistical methods
+ skill set:
	- Experience building or maintaining databases (MySQL, Hive, etc.)
	- Experience building or maintaining Big data & streaming systems (Hadoop, HDFS, Kafka, etc.)
	- Cross-platform coding
	- Large-scale, large-user base website development experience
	- Data mining, machine learning, AI, statistics, information retrieval, linguistic analysis
+ skill set:
	- Application code development in C++
	- Embedded peripherals and drivers (UART/CAN/SPI/I2C)
	- Wireless communication systems (Cellular, Wifi, BLE)
	- Networking protocols (TCP/UDP)
	- Embedded Linux
	- FreeRTOS or similar low level RTOS
	- Masters degree or similar job experience
	- Experience with algorithm packages (Eigen, OpenCV, etc.)
	- Worked with TI M4 class processors
	- Worked with Linux on ARM processors
	- Worked with Embedded GCC
	- Experience with automotive systems or UAV systems
+ skill set:
	- Design and implementation of state of the art monocular computer vision algorithms
	- Solve problems involving odometry, landmark detection, structure from motion and segmentation in large scale outdoor environments
	- Integrate vision based algorithms into our probabilistic fusion framework
	- Help in identifying core requirements for camera sensors
	- Code development in C++/Python
	- Work with real data on our self driving car
	- Masters or PhD Computer Science, Electrical Engineering or both.
	- Deep Experience in SfM, VO, and classical computer vision algorithms
	- Expert knowledge in computational geometry
	- Experience in machine learning, feature detection and classification
	- Experience with open source computer vision and linear algebra frameworks
	- A solid background in statistics, probability and linear algebra
	- Experience with real world datasets
	- Experience with real time algorithm implementation
	- Ability to work independently without direct supervision
	- Experience with CV algorithm packages (Eigen, OpenCV, etc.)
	- Knowledge of Deep learning techniques applied to CV
	- Experience in Linux based environments
	- Experience in SLAM and/or motion planning
	- Experience with CUDA, OpenCL or other GPU frameworks
	- Experience with automotive systems or UAV systems
	- Ability to lead a small technical team that balances research and application
+ skill set:
	- This role is focused on delivering the best in localization performance from our sensors.
	- Candidates should have extensive experience working with raw navigation data. You should be very comfortable with estimation theory and have implemented complex filters in practice. Real world experience with RTK, Integer Ambiguity Estimation and other high precision techniques are a huge plus.
	- Inertial Measurement Unit Algorithms, Extended Kalman Filter, Visual Odometry, Gnss, RTK-GPS
+ [Sphinx](https://en.wikipedia.org/wiki/Sphinx_(search_engine))
+ skill set:
	- Expertise in image and video processing, computational photography, single and multiview geometry, keypoint extraction, description, association, etc.
	- Experience in efficient large-scale numerical optimization
	- Experience in the area of camera calibration, SLAM, point cloud processing are highly desired
	- Publication records in leading conferences such as CVPR, ICCV, ECCV, NIPS, ICML or PAMI is a plus
+ skill set:
	- Strong knowledge of the state-of-the-art in computer vision and machine learning algorithms with a solid understanding of OpenCV
	- Experience working with point cloud processing and Point Cloud Library (PCL)
+ skill set:
	- As a Deep Learning Engineer at Simbe Robotics, you will be part of a talented team designing and training state of the art deep learning algorithms to identify placement, presentation, pricing, and availability of products in retail stores across the globe.  
	- In this role you will lead various initiatives designing, developing, and training in-house character recognition and image caption algorithms powered by deep learning.
	- Participate in planning and prioritizing, write functional specifications and lead design reviews for our character recognition and image caption algorithms.
	- Generate, clean, and curate real world training datasets
	- Create photorealistic synthetic training data for augmentation
	- Develop, test, tune, and deploy character recognition and image caption systems across a wide variety of customers
	- Evaluate existing character recognition and image caption methods for speed and accuracy performance improvements
	- Collaborate with other developers, quality engineers, product managers, and documentation writers
	- Ph.D. or M.S. preferred
	- Strong machine learning background, with 2+ years of hands-on experience in building real systems
	- Deep understanding of state of the art machine learning and deep learning algorithms, techniques and best practices
	- Solid understanding of linear, non-linear, and dynamic programming
	- Experience using or building synthetic image generation systems, data augmentation pipelines, and OCR/image caption systems
	- Proficient in at least one of the following: Tensorflow, Keras, PyTorch. Tensorboard knowledge is a plus
	- Must be fluent in Python, other languages are a plus
	- Should be familiar with training and running deep learning models on GPUs (both commodity and otherwise)
	- A good understanding of recurrent neural networks (including LSTMs and GRUs)
	- Experience in debugging and diagnosing performance problems with ML algorithms
	- Must have excellent written and verbal communication skills
	- Experience with attention models, text localization, Google Cloud Platform, AWS, and serverless is a plus
	- Strong Linux & Command Line background
	- Ability to work hands-on in cross-functional teams with a strong sense of self-direction
+ [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
	- [TensorBoard, TensorFlow's visualization toolkit](https://www.tensorflow.org/tensorboard)
	- https://databricks.com/tensorflow/visualisation
+ skill set:
	- We are seeking a strategic technical leader who will be responsible for delivering the core infrastructure for machine learning on Databricks. This includes the ML runtime (a packaged environment containing Spark, Tensorflow, and other frameworks), our own machine learning algorithms, storage and IO optimizations, as well as higher level abstractions such as hyper parameter tuning and feature registries.
	- Grow a team of application developers responsible for the Databricks ML Runtime.
	- Grow Databricks’ machine learning capabilities - increase YoY product revenue and adoption at > 100%
	- Manage technical debt, including long term technical architecture decisions and balance product roadmap
+ skill set:
	- Use Databricks to build internal data warehouse and integrate it with BI and CRM services used internally
	- Use Databricks to analyze usage data, and create dashboards and reports
	- Build self-serving internal data products to make data simple within the company
	- Work closely with Product Management and other stakeholders to understand product usage patterns and trends and to make data-driven decisions and forecasts
	- Provide product feedback to PM and Engineering teams
	- Strong desire to work at a rapidly growing startup
	- Knowledge of data processing and applied statistics
	- Proficient in data analysis and visualization using R or PyData
	- Familiar with SQL and databases like MySQL or PostgreSQL
	- Experience with distributed data processing systems like Spark and Hadoop
	- General-purpose languages such as Python and Scala
	- Desire to explore lots of data to find unexpected insights
	- Strong communication and presentation skills
	- [Plus] Advanced degrees in statistics, computer science, math, or similar fields
	- [Plus] Familiarity with interactive data visualization using tools like D3.js
+ skill set:
	- Architect and operate high quality, large scale, multi-geo data pipelines that drive business decisions.
	- Redesigned data pipelines using the applicable DBR features, and incorporating external tools where necessary to have better reliability and tighter SLAs.
	- Established conventions or new APIs for logging feature usage for PM use-cases.
	- Understandable SLAs for each of the production data pipelines.
	- Improved test coverage (90+%) for data pipelines. Best practices and frameworks for unit, functional and integration tests.
	- CI and deployment processes and best practices for the production data pipelines.
	- Reduction in overall alert noise and increase responsiveness by rethinking the current alert categories and priorities.
	- Design schemas for financial, sales and support data in the data warehouse.
	- Experience building, shipping and operating multi-geo data pipelines at scale.
	- Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
	- Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
	- Excellent communication (writing, conversation, presentation) skills, consensus builder
	- Strong analytical and problem solving skills
	- Passion for data engineering and for enabling others by making their data easier to access.
	- Experience with pipelines that are used by many downstream teams, including non-engineering functions.
	- Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
	- Experience working with Apache Spark and data warehousing products.
	- Direct experience with a log collection and aggregation system at scale.
	- Demonstrated execution at a growth stage technology company.
+ skill set:
	- If you are looking for an unparalleled opportunity to build the next generation big data processing platform, and learn how to launch hundreds of thousands of VMs a day at scale while running thousands of Kubernetes clusters, you have come to the right place. The platform team builds and manages the core systems powering Databricks, allowing it to seamlessly scale and run across various geographic regions/clouds, and making Databricks the go-to product for big data processing in the cloud.
	- You will be a senior software engineer responsible for architecting scalable systems to power Databricks, making it the de-facto platform for running Big Data and AI workloads. You will build and extend the Databricks cloud platform, which is based on a micro service architecture and includes systems for managing thousands of Kubernetes clusters at scale, systems for streaming and consuming gigabytes of log data per minute, onboarding and managing thousands of data scientists on Databricks, scalable API gateway, rate limiting framework, network security and encryption, build infrastructure (we use Bazel), and scalable CI/CD framework among many others.
	- Develop and extend the Databricks platform. This implies, among others, writing clean, efficient code in Scala or Python and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with different Databricks services
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python or Javascript and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with internal APIs.
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python, and Javascript, building data pipelines (Apache Spark, Apache Kafka), integrating with third-party applications, and interacting with cloud APIs (AWS, Azure, CloudFormation, Terraform).
	- To achieve this, we build data reporting pipelines that support the underlying pricing infrastructure supporting tens to hundreds of millions of DBUs (Databricks Units) across multiple clouds and regions, UIs that allow Databricks administrators to view and manage their bill, and APIs and integrations to downstream processors to handle payments for all customers.
	- Experience in architecting, developing, deploying, and operating large scale distributed systems.
	- Experience with distributed data processing systems (Apache Spark, Apache Kafka).
	- Experience with cloud APIs (e.g. a public cloud such as AWS, Azure, GCP, or an advanced private cloud such as Google, Facebook).
	- Experience working on a SaaS platform or with Service-Oriented Architectures.
	- Experience with API development.
	- Good knowledge of SQL.
	- Experience with software security and systems that handle sensitive data.
	- Exposure to container technologies, such as Kubernetes, Docker.
	- Unified Analytics Platform
+ skill set:
	- Our team drives state-of-the-art, open source Delta Lake project bringing reliable, scalable, ACID transactions to Apache Spark and other Big Data engines. Our mission is to deliver a robust and performant engine that enables users to build reliable data pipelines that ingest massive data volumes, optimize data layout, generate metadata and evolve data schemas all while guaranteeing transactional correctness and high query performance.
	- Build the core features that make Delta Lake the world’s best Big Data storage abstraction in terms of performance, stability, security and scalability.
+ [Delta Lake Community; Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads.](https://delta.io/)
+ Experience with our web stack (React, Redux, TypeScript, protobuf, Apollo, GraphQL) and Spark
+ skill set:
	- You will build tools and features to make Databricks the best place for large-scale enterprise R workloads.
	- Improve state of distributed R computing through Apache Spark and R integration on Databricks
	- Implement new features on Databricks platform for R users (e.g., ACL)
	- Improve and extend Databricks R notebooks to satisfy R users’ use cases and requirements
	- Implement new R-based APIs on Databricks platform (e.g., secret management API)
	- Expand Databricks workspace through integration with third-party tools such as RStudio and Shiny.
	- Integrate critical packages from the R ecosystem into Databricks Runtime
	- Provide engineering support and thought leadership to Databricks field engineering teams on R
	- Give talks and write blog posts about R on Databricks
+ [MLflow, An open source platform for the machine learning lifecycle](https://mlflow.org/)
+ Production quality coding standards and patterns.
+ skill set:
	- The Machine Learning Platform team is hiring strong engineers to help us design MLflow, an open source tool for managing the Machine Learning lifecycle. In this role you will help define the APIs creating the standard that organizations use to manage their Machine Learning, from tracking offline experimentation through deployment to production systems. You will also build the services supporting the APIs in the open source and their integration into the Databricks product, a unified analytics platform that helps manage data processing and machine learning workloads in a collaborative, enterprise grade product.
	- Design new and extend existing components of MLflow, such as experiment tracking, project management, and model deployment
	- Implement proprietary integrations of MLflow into the core Databricks product
	- Be responsible for full software development lifecycle - design, development, testing, operating in production
	- Architect solutions to achieve a high level of reliability, scalability and security
	- Communicate effectively with other engineers in the same team, with other teams and with various other stakeholders such as product managers
	- Mentor junior engineers or other engineers on the team to help level up their skillset
	- 7+ years of production experience developing services in: Java, Scala, C++, Go, or Python
	- Has designed and developed APIs used in production systems.
	- Deployed production web services using container and orchestration technologies, such as Docker and Kubernetes to public or private clouds.
	- Developed services leveraging SQL backend stores.
	- Demonstrates customer obsession: has altered designs for frontend or APIs with the user experience in mind
	- Developed and debugged software running on Linux OS
	- Experience with Continuous Integration/Continuous Deployment frameworks.
	- Preferred Experience working on a SaaS platform or with Service Oriented Architectures
	- Preferred Experience with software security and systems that handle sensitive data
+ skill set:
	- Develop and extend the Databricks product. This implies, among others, writing software in Scala, Python, and Javascript, building data pipelines (Apache Spark, Apache Kafka), integrating with third-party applications, and interacting with cloud APIs (AWS, Azure, CloudFormation, Terraform).
	- Experience in architecting, developing, deploying, and operating large scale distributed systems.
	- Experience with distributed data processing systems (Apache Spark, Apache Kafka).
	- Experience with cloud APIs (e.g. a public cloud such as AWS, Azure, GCP, or an advanced private cloud such as Google, Facebook).
	- Experience working on a SaaS platform or with Service-Oriented Architectures.
	- Experience with API development.
	- Good knowledge of SQL.
	- Experience with software security and systems that handle sensitive data.
	- Exposure to container technologies, such as Kubernetes, Docker.
+ Build system experience like Maven, Bazel, or Gradle
+ skill set:
	- We've built features such as autoscaling compute and storage, credential passthrough and notebook-scoped libraries, that simplify resource administration in the cloud, secure data in the enterprise and empower data scientists an data engineers in their organizations. You have the opportunity to join us and solve the infrastructure and data management problems of enterprises as they transition from on-prem data centers to the future of the cloud.
	- JVM or lower-level programming languages for systems programming.
	- Experience with services infrastructure.
	- Experience with distributed systems, databases, and big data systems.
	- Experience with cloud APIs preferred (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
	- Exposure to container technologies, such as Docker preferred
+ skill set:
	- As a Software Engineer on the Spark Benchmarking team at Databricks, you are responsible for ensuring that the Databricks Runtime is the world’s best Spark execution environment in terms of performance and scalability.
	- You will be part of the team that is continuously improving the methodology and benchmarking infrastructure, helping to increase the frequency of the releases while maintaining high quality and performance standards. Continuously improving performance is an increasingly challenging job given the high volume of commits that go into a release. In order to meet this challenge, your team will continuously increase the level of automation and provide powerful benchmarking tools to evaluate the performance impact of each change. Engineers on the Spark Benchmarking team also drive the Databricks runtime performance sign-off process, they are the gatekeepers making sure that all performance regressions are addressed before a new version is released.
	- Experience with: Large scale distributed computing, Big Data engines e.g. Spark, Hadoop.
	- Passion for software automation and Continuous Integration experience.
	- Excellent communication and teamwork.
	- Strong foundation in algorithms and data structures and their real-world use cases.
	- Solid understanding of computer systems and networks.
	- Production quality coding standards and patterns.
	- 4+ years of general software programming experience.
	- 4+ years of modern, production level experience in one of: Java, Scala, JavaScript, or C++.
	- BS in Computer Science, Math, related technical field or equivalent practical experience.
	- Experience with benchmarking big data systems
	- Experience with developing infrastructure for testing distributed systems
+ skill set:
	- Experience with: Large scale distributed computing, Big Data engines e.g. Spark, Hadoop.
	- Passion for software automation and Continuous Integration experience.
	- Excellent communication and teamwork.
	- Strong foundation in algorithms and data structures and their real-world use cases.
	- Solid understanding of computer systems and networks.
	- Production quality coding standards and patterns.
	- 4+ years of modern, production level experience in one of: Java, Scala, JavaScript, or C++.
	- Experience with benchmarking big data systems
	- Experience with developing infrastructure for testing distributed systems
+ skill set:
	- Full ownership including: Designing, Implementing, Testing and Metric Analysis.
	- Production quality coding standards and patterns.
+ skill set:
	- The workflow team operates at the core of the Databricks infrastructure: it orchestrates all the workloads scheduled by the customers of Databricks, from the one-off experiment to the massive multi-day query running on hundreds of machines. As part of this team, you will be responsible for maintaining mission-critical operations, and at the same time pushing the boundary in terms of integrating with innovative AI solutions built on top of the Databricks platform. The responsibility covers mainly the backend service itself and all its adjacent functions, from low-level systems in Scala to dashboards and health monitoring, and public APIs for remote management.
	- We are looking for talented engineers who are passionate about large-scale, high availability systems, and who want to make a strong impact on the growth of the company.
	- Maintain the existing backend of Databricks' core scheduling service
	- Own (as a team) the alerting and deployment systems around the backend
	- Scale the scheduling service by 10x
	- Own the testing infrastructure of the backend.
	- Architect the workflow management component of Databricks
	- 3+ years of experience with backend systems written in java, scala, go, or c++
	- Deep understanding of high-concurrency, reliable services
	- Production quality coding standards and patterns
	- Strong foundation in algorithms and data structures and their real world use cases
	- Experience with SAAS/PAAS services (experience with developing cloud-based services strongly desirable)
	- Experience working on complex, data-heavy applications
	- Experience instrumenting services
+ skill set:
	- As a DevOps Engineer at Simbe Robotics you will be part of a talented team ensuring quality in our software as well deploying & managing our cloud services and world-wide fleet of autonomous robots.
	- Has experience with automated build and continuous integration systems (e.g. Jenkins, TravisCI)
	- Has knowledge of application/system level monitoring (Nagios, CloudWatch, Munin, Splunk)
	- Experience with configuration management (Chef, Puppet, Ansible) tools
	- Has experience with various application packaging and deployment technologies (Debian packages, Docker/Linux containers)
	- Experience configuring web servers (e.g. Apache/Tomcat, nginix)
+ Working on the robot’s navigational systems for mapping, localization, path planning, obstacle detection and avoidance. Our robots are designed to work safely and reliably alongside shoppers and employees during normal store hours.
+ programming languages, frameworks and tools
	- .Net
	- .Net Framework
	- ABAP
	- Ada
	- Akka
	- Alice
	- AngularJS
	- Ansible
	- Apex
	- ASP.net
	- assembly language
	- Awk
	- Backbone.js
	- Bash
	- C
	- C\#
	- C++
	- CakePHP
	- CFEngine
	- Chef
	- Clojure
	- COBOL
	- Codeigniter
	- CSS
	- D
	- Dart
	- Delphi/Object Pascal
	- Django
	- Docker
	- Ember.js
	- Erlang
	- Express.js
	- F\#
	- Flask
	- Fortran
	- Go
	- Groovy
	- Haskell
	- HTML
	- Java
	- JavaScript
	- jQuery
	- Kubernetes
	- Ladder Logic
	- Linux
	- Lisp
	- Logo
	- Lua
	- Matlab
	- Meteor
	- MQL4
	- Nagios
	- NodeJS
	- Objective-C
	- Perl
	- Phalcon
	- PHP
	- Play!
	- Prolog
	- Puppet
	- Python
	- Q
	- R
	- React
	- Redux
	- Revel
	- Rkt
	- RPG (OS/400)
	- Ruby
	- Ruby on Rails
	- Rust
	- RxJS
	- SAS
	- Scala
	- Scheme
	- Scratch
	- Spring Framework
	- SQL
	- Swift
	- Symxfony
	- VHDL
	- Visual Basic
	- Visual Basic .Net
	- Windows
	- Zend
+ skill set:
	- You revel in building features quickly and iterating in a data-driven fashion
	- You lay awake thinking about improving the design, implementation and maintenance of large software systems with millions of users
	- Passion to hack social commerce
	- Data & Relevancy engineers work on our massive semi-structured datasets. They have domain experience in data mining, information retrieval, or machine learning, and a strong system orientation. Key product initiatives include product feed relevance, ad targeting, information extraction, and recommendations.
	- Infrastructure engineers scale a massive, highly-available platform end-to-end. They design distributed systems, validate performance, factor in security, and proactively monitor every corner of our stack. When things do go wrong, they are on-hand to fight the fires.
+ Hibernate ORM is an object-relational mapping tool for the Java programming language
+ skill set:
	- Data/Model Validation Engineer
	- We are looking for someone passionate about learning how machine learning systems are developed to assist with validating and processing training data to evolve our state of the art systems.
	- Engage with software engineers on the Perception team to identify and collect training data to evolve our machine learning systems.
	- Working with engineers on the Perception team, train new machine learning models and perform analysis to quantify how they perform based on changes to the training data sets.
	- Provide feedback on tools and processes for efficient workflow of training data creation and validation.
+ skill set:
	- Knowledge of robotics concepts and tools (ROS)
	- Understanding of and ability to implement machine learning methods, particularly for applications in autonomous vehicle decision making and prediction
	- Experience in production C++ development
+ skill set:
	- Experience with Robotics perception
	- At Starsky, the Perception team is responsible for processing sensor information and making it available to the other teams in a clean and consistent format. The models and algorithms developed aim to achieve robust real time detection and tracking of our truck and other objects in the local environment, including lane lines, vehicles, and pedestrians.
	- As a Robotics Perception Engineer, you will be responsible for filtering, fusing and post-processing the outputs of different deep learning models and sensors. You will apply state of the art tracking and fusion algorithms which are robust to sensor noise and environmental variability. This requires working with teams across the driving stack to scope requirements and understand the strengths and limitations of different modules.
	- Background in linear algebra, probability, 3D geometry
	- Experience with multi-camera sensor fusion and camera-radar sensor fusion
	- Experience with real time tracking of objects and lanes
	- Experience with real time mapping and localization
	- Experience with camera and radar sensor calibration
	- Ability to write efficient real time algorithms in C++
	- Experience in developing on linux environment
	- 2+ years experience in C++/Python development in a fast paced production environment
	- 2+ years experience in perception system of mobile robots
	- Experience with ROS
	- Experience in sensor noise analysis
	- Experience in machine learning/deep learning
+ skill set:
	- Starsky Robotics is looking for a full-time Senior Data Scientist. Your job will tackle a wide variety of problems in autonomous vehicles. From finding every time a car cut in front of our truck, to figuring out how to report on the quality of autonomous driving, to creating new tools and statistical methods for robotics engineers to characterize the behavior of their systems, we’re looking for someone motivated to attack self-driving problems with mounds of data. Tackling these problems will require learning about the whole suite of robotics fields applied to make autonomous vehicles: motion planning, controls, perception, and behavior planning.
	- You’ll own high-level decisions such as “How do we determine if a route fits our current driving capabilities”. Day-to-day projects may have mission statements as technical as “Help us solve this spike in cross-track error on curves”, or as business-focused as “Can we get a heatmap of all the places our trucks have driven over the last year”.
	- Additionally, you can bring best-practices for data-science to the company, including helping build up the base platform and infrastructure necessary to speed up data-centric work. Starsky has a solid base of tooling around our data, but it is ripe for improvement.
	- Demonstrated expertise in the data scientists modern toolkit: Pandas, R, SQL, etc, and don’t mind sharing your experience with the team
	- Deep quantitative thinker: Masters or PhD in a quantitative field, or multiple years of experience in a quantitative-focused position
	- Relish delivering answers and metrics and seeing change affected by your work
	- Can take high level directives and take them through from research project, proof of concept, to applied & implemented feature.
	- Are constantly looking for problems that could be solved with liberal application of data
+ skill set:
	- Develop pipeline for data tagging, labelling and munging to be consumed for training of ML models for vision based tasks.
	- Architect and train machine learning models for object detection and tracking
	- Build testing environment to test the model and simulate edge-case performance scenarios
	- Experience with at-least one of Tensorflow/Caffe/Theano/Torch
+ skill set:
	- Qt (or PyQt)
	- Microcontroller firmware
+ Infrastructure as code experience (we use terraform)
+ Knowledge of TypeScript, React, Jest, Cypress is a plus
+ Background in linear algebra, probability, 3D geometry and abstract problem solving skills
+ Experience in Computer Vision / Computational Geometry / Structure from Motion / SLAM
+ advanced optimization algorithms:
	- Evolutionary Algorithms
	- surrogate model optimization
	- particle swarm optimization
	- Bayesian optimization
+ [Numerical Library](https://en.wikipedia.org/wiki/List_of_numerical_libraries)
+ skill set:
	- Experience in developing and debugging multi-threaded/parallel applications.
	- Experience in image processing, computational geometry, large data application, high performance computing and scientific simulation is a good plus.
+ Research, evaluate, and present statistical or Machine Learning methods to provide actionable insights.
+ Direct or indirect experience in OPC (Optical Proximity Correction), including rogorious lithography simulation (Hyperlith, Prolith), RET, and advanced mask technology.
+ AWS DynamoD
+ Enforce SOX & GDPR compliance across the analytics database and reporting tools
+ Solid understanding of imaging theories (Abbe, Hopkins).
	- Abbe-PCA (Abbe-Hopkins): microlithography aerial image analytical compact kernel generation based on principle component analysis
	- Hybrid Hopkins-Abbe method for modeling oblique angle mask effects in OPC
	- Application of the hybrid Hopkins–Abbe method in full-chip OPC
	- transmission cross coefficients (TCCs)
+ Good grasp of statistical concepts (e.g. hypothesis testing, regression)
+ Love Github, Slack, Asana, AWS, Meteor, Node
+ skill set:
	- Experience with data processing frameworks and data warehouses such as Hadoop, Spark, Redshift
	- Experience with designing, implementing, and optimizing ETL in Pentaho
+ skill set:
	- Technical fluency in one language and tool such as Python, Java or Scala, AWS (S3/EMR/Athena/Glue) and SQL.
	- Experience with big data processing tools including Spark, Hadoop, Hive, Yarn, and Airflow.
	- Experience working with either a MapReduce system of any size/scale.
+ skill set:
	- GatsbyJS
	- ElasticSearch
+ skill set:
	- Experience in development using MEAN stack (Node.js, Angular.js, Express.js, MongoDB)
	- Data exchange technologies like JSON
	- Familiarity with No SQL databases (i.e. MongoDB, Hadoop, Hive Spark, etc.), data streaming and integrating unstructured data will be plus.
	- Exposure to rules engines e.g. drools, ESBs e.g. MuleSoft & integration with enterprise systems
	- Highly preferred Web UI or dashboarding experience (ie CSS, HTML, Tableau, Qlik, etc.
	- Knowledge and hands on experience on implementation of Chatbot using Microsoft Bot Framework, API.AI or Watson
	- Experience working in a DevOps environment, and using industry standard tools (GIT/OneStash, JIRA)
+ skill set:
	- Minimum 3 years of designing, building and operationalizing large scale enterprise data solutions and applications using one or more of Azure / AWS / GCP data and analytics services in combination with custom solutions -  Spark, Azure Data Lake, HDInsights, SQL DW, DocumentDB, Search, Elastic Pool etc.  
	- Minimum 3 years experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on AZURE.
+ skill set:
	- Big Data platforms e.g. Cloudera, Hortonworks MapR
	- Big Data Analytic frameworks and query tools such as: HDINsight, Spark, Storm, Hive, Impala
	- IoT protocols, gateways, queues, messaging hubs such as IoT Hub, MQTT, XMPP, CoAP, etc.
	- IoT development experience on at least one of the industry leading platforms (Azure IoT, AWS IoT, GE Predix, Siemens Mindsphere, PTC Thingworx, SAP Leonardo, GCP)
	- Streaming data tools and techniques such as Apache Kafka, Azure Streaming Analytics, AWS Kinesis
+ skill set:
	- Experienced and interested in Ruby, Elixir, Java, Python, Node.JS, Phoenix Elixir, or other backend programming language or framework
+ skill set:
	- Minimum 1 year of building and coding applications using at least two Hadoop components – MapReduce, HDFS, Hbase, Pig, Hive, Spark, Scoop, Flume, etc
	- Minimum 1 year coding one of the following: Python, Pig programming, Hadoop Streaming, HiveQL
	- Minimum 1 year understanding of data modelling & data pipeline design: iterative data pipeline development from raw, curated, integrated to published data, with fit for use data modelling on Hadoop and NoSQL platforms
	- Minimum 1 year of experience implementing large scale cloud data solutions using Cloud Service Providers:  AWS data services (e.g. EMR, Redshift, GLUE) or Azure (Data Lake Store/Analytics, SQL Data Warehouse) or Google Cloud Platform Google Cloud (Big Data:  Big Query, Big Insights)
	- Minimum 1 year of experience delivering an operational Big Data solution using one or more of the following technologies: Hadoop, HortonWorks, Cloudera, Cassandra
	- Minimum 1 year of experience throughout the SDLC of a Hadoop implementation technologies including HortonWorks, Cloudera, Hive, Pig, MapReduce 
	- Minimum 1 year of experience throughout the SDLC of a HortonWorks, Cloudera, Cassandra / Hbase implementation 
	- Minimum of a Bachelor’s Degree or 3 years IT/Programming experience
	- Minimum 1 year of experience developing REST web services
	- Industry experience (financial services, resources, healthcare, government, products, communications, high tech)  
	- Experience leading teams
	- Machine Learning tools, interfaces & Libraries: R, R-Studio, Spark R, sparklyr, MLlib, H2O etc.  
	- Experience with other tools, databases and Apache projects: Google BigQuery, Presto, Drill, Kylin, OpenTSDB, Spark Streaming
	- Enterprise data integration, BI and analytics platforms: Informatica, Talend, InfoSphere, SAS, RevoR, QlikView, Qlik Sense, Tableau, Spotfire, D3.js
	- Processing frameworks & programming tools: Spark (Scala/Python/Java), Kafka, Flink
	- Client facing skills: ability to build trusted relationships with client stakeholders and act as a trusted adviser
+ skill set:
	- Familiarity with No SQL databases (i.e. MongoDB, Hadoop, Hive Spark, etc.), data streaming and integrating unstructured data will be plus.
	- Experience working in a DevOps environment, and using industry standard tools (GIT/OneStash, JIRA)
	- Exposure to rules engines e.g. drools, ESBs e.g. MuleSoft & integration with enterprise systems
+ Experience with higher education SIS and LMS systems (Banner, Colleague, Jenzabar, Canvas, Blackboard, etc.) strongly preferred
+ Experience with monitoring and tracking tools such as  Splunk, NewRelic, Adobe/Google Analytics
+ Experience with relational databases (MySQL, DB2 or Oracle) and NoSQL databases (Redis, Cassandra or DynamoDB)
+ skill set:
	- Experience with Java, Spring Boot
	- Experience with React, Backbone, Marionette or equivalent framework
	- Experience with Protractor, RSpec or equivalent integration test framework
	- Experience with relational databases (MySQL, DB2 or Oracle) and NoSQL databases (DynamoDB) is a plus
+ skill set:
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
+ skill set:
	- A fascination with the PoseNet research since its release in 2015.
	- Fundamental understanding of Bundle Adjustment or Non-Linear Optimization.
	- PhD or exceptional MSc involving 3D-Reconstruction, SLAM, Camera Calibration or Computational Geometry from a top ranking university or lab.
	- Geometry from Vision is at the heart of Scape Technologies. As a research engineer on the Scape Technologies team, you will take a key role in designing and building the pipeline for cloud-based 3D-reconstruction and real-time global localization. This will require implementing and building upon existing research in Structure-from-Motion, Dense 3D-reconstruction, Camera Calibration and SLAM.
	- This can range from non-linear optimization to efficient graph traversal, considering optimized computational parallelization.
	- Making sure that Scape’s large scale reconstruction and localization pipeline is the most efficient in the world.
	- We are looking for curious and enthusiastic computer vision scientists who are keen on working on moonshot projects.
+ skill set:
	- 7+ years Experience with large-scale distributed systems and client-server architectures.  Examples include Java/Spring Boot, CQRS, event streaming, Kafka, Spark
	- Deep experience with Cloud Computing platforms (e.g. Amazon AWS, Microsoft Azure, Google App Engine)
	- Knowledge and understanding of relevant legal and regulatory requirements, such as SOX, PCI, HIPAA, Data Protection, etc.
	- Demonstrated experience implementing and managing high capacity, redundant, and mission critical environments
	- Knowledge in databases and comfortable with various databases technologies.  Examples include relational database (Oracle) and/or NoSQL data technologies (Mongo, Cansandra, Couchbase) and related toolsets.
	- Proficiency in TCP/IP networking, architecture and core technologies ( DNS, routing, iptables, tc, etc.)
	- Experience running and maintaining a 24x7 production environment
+ skill set:
	- Demonstrates knowledge of the data engineering domain with experience in building and supporting non-interactive (batch, distributed) or real-time, highly available data, data pipelines.
	- Able to build fault tolerant, self-healing, adaptive computational pipelines
	- Contribute to the decision-making process related to the selection of software solutions that make up the architecture
+ skill set:
	- At least 5 years demonstrated results in areas of Operations Research and/or Supply Chain Projects (inventory optimization, network design, and S&OP) in sophisticated and complex environments including the use of simulation and modeling tools (Llamasoft, CPLEX, Gurobi, or other similar)
	- At least 5 years performing data analytics and modeling with advanced languages (e.g. Python or R)
+ skill set:
	- Minimum of 3 years’ delivery experience in advanced modeling environment: strong understanding of statistical concepts and predictive modeling. (e.g., AI neural networks, multi-scalar dimensional models, logistic regression techniques, machine-based learning, big data platforms, SQL, etc.).
	- Minimum 3 years’ experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
	- Possess a blend of marketing acumen, consulting expertise and analytical capabilities that can create value and insights for our clients.
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Knowledge of Deep Learning (CNN, RNN, ANN) using TensorFlow.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 1 year of building Business Data Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies (e.g  Alation, Informatica or custom portals).
+ You’re familiar with business intelligence reporting platforms like OBIEE, Tableau, MicroStrategy, and Business Objects
+ skill set:
	- You know how to work with data engineering technologies like Spark, no SQL DB or Lambda
	- You know everything there is to know about Robotic Process Automation
	- A minimum of 7 years experience in deep learning, machine learning or artificial intelligence applications like virtual agent, RPA, or video/image/text analytics
+ skill set:
	- Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments, using Spark, pySpark, SparkSQL, with  Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
	- Minimum 1 year of designing and building performant data models at scale for using Hadoop, NoSQL, Graph or Cloud native data stores and services.
	- Minimum 1 year of designing and building secured Big Data ETL pipelines, using Talend or Informatica Big Data Editions; for data curation and analysis of large scale production deployed solutions.
	- Minimum 6 months of experience designing and building data models to support large scale BI, Analytics and AI solutions for Big Data.
	- Knowledge of Auto Machine Learning tools ( H2O, Datarobot, Google AutoML).
	- Minimum 6 months of expertise in implementation with Databricks.
	- Experience in Machine learning using Python ( sklearn) ,SparkML , H2O and/or SageMaker.
	- Minimum 2 years designing and implementing large scale data warehousing and analytics solutions working with RDBMS (e.g. Oracle, Teradata, DB2, Netezza,SAS) and understanding of the challenges and limitations of these traditional solutions.
	- Minimum 1 year of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale, Jethro and others.
	- Minimum 1 year of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for big data platforms on premise or on AWS, Google and Azure cloud.
	- Minimum 1 year of Re-architecting and rationalizing traditional data warehouses with Hadoop, Spark or NoSQL technologies on premise or transition to AWS, Google clouds.
	- Experience implementing data preparation technologies such as Paxata, Trifacta, Tamr for enabling self-service solutions.  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
+ skill set:
	- Minimum 3+ years of architecting, implementing and successfully operationalizing large scale data solutions in production environments using Hadoop and NoSQL ecosystem on premise or on Cloud (AWS, Google or Azure) using  many of the relevant technologies such as  Nifi, Spark, Kafka, HBase, Hive, Cassandra, EMR, Kinesis, BigQuery, DataProc, Azure Data Lake etc.  
	- Minimum 2+ years of experience implementing SQL on Hadoop solutions using tools like Presto, AtScale and others
	- Minimum 3+ years of architecting data and building performant data models at scale for Hadoop/NoSQL ecosystem of data stores to support different business consumption patterns off a centralized data platform  
	- Minimum 3+ years of Spark/MR/ETL processing, including Java, Python, Scala, Talend; for data analysis of production Big Data applications
	- Minimum 3++ years of architecting and industrializing data lakes or real-time platforms for an enterprise enabling business applications and usage at scale
	- Minimum 2+ years of experience implementing large scale BI/Visualization solutions on Big Data platforms
	- Minimum 3+ years of experience implementing large scale secure cloud data solutions using AWS data and analytics services e.g. S3, EMR, Redshift
	- Minimum 2+ years of experience implementing large scale secure cloud data solutions using Google data and analytics services e.g. BigQuery, DataProc
	- Minimum 2+ years of experience building data management (metadata, lineage, tracking etc.)  and governance solutions for modern data platforms that use Hadoop and NoSQL on premise or on AWS, Google and Azure cloud
	- Minimum 2+ years of experience securing Hadoop/NoSQL based modern data platforms on-premise or on AWS, Google, Azure cloud
	- Minimum 2+ years of Re-architecting and rationalizing traditional data warehouses with Hadoop or NoSQL technologies on premise or transition to AWS, Google clouds
	- Experience implementing data wrangling and data blending solutions for enabling self-service solutions using tools such as Trifacta, Paxata
	- 4 years industry systems development and implementation experience OR Minimum of 3 years of data loading, acquisition, storage, transformation, and analysis
	- Minimum 2+ years of using Talend, Informatica like ETL tools within a Big Data environment to perform large scale metadata integrated data transformation
	- Minimum 1+ years of building Business Catalogs or Data Marketplaces on top of a Hybrid data platform containing Big Data technologies
	- Architect modern data solutions in a hybrid environment of traditional and modern data technologies such as Hadoop, NoSQL
	- Create technical and operational architectures for these solutions incorporating Hadoop, NoSQL and other modern data technologies
	- Implement and deploy custom solutions/applications using Hadoop/NoSQL
	- Lead and guide implementation teams and provide technical subject matter expertise in support of the following:
	- Designing, implementing and deploying ETL to load data into Hadoop/NoSQL
	- Security implementation of a Hadoop/NoSQL solutions
	- Managing data in Hadoop/NoSQL co-existing with traditional data technologies in a hybrid environment
	- Troubleshooting production issues with Hadoop/NoSQL  
	- Performance tuning of a Hadoop/NoSQL environment
	- Architecting and implementing metadata management solutions around Hadoop and NoSQL in a hybrid environment
+ skill set:
	- Work in an agile, CI/CD based, test-driven development environment
	- Semantic Web (RDF/SPARQL)
+ skill set:
	- At least 2 years designing and building healthcare data analysis solutions for the business payer or provider industry
	- At least 2 years using new developments in AI, machine learning, cognitive systems, and robotics to build amazing analytical tools
	- At least of 2 years working with tools like SAS, Python, SPSS, R, or SQL
	- At least 2 years working with data integration tools to streamline processes in platforms like Cerner EMR, Apache Spark, MapReduce, MongoDB and Couchbase
	- You can use data mining techniques to solve real world business problems
+ skill set:
	- Project-based analytics including but not limited to: Machine Learning, Predictive Analytics, Comparative Effectiveness Analysis, Failure Analysis, Big Data Analytics, Optimization, Demand Forecasting, Customer Segmentation, Customer Analytic Record.
	- Minimum 3 years’ experience with predictive analytics tools, including at least two of the following: R, SAS, Alteryx, Python, Spark, and Tableau.
	- Experience in the following areas: Applied Statistics/Econometrics, Statistical Programming, Database Management & Operations, Digital, Comparative Effectiveness Research.
+ skill set:
	- You've got a Master’s degree in statistics, econometrics, mathematics, or deep learning architectures including convolutional, recurrent, autoencoders, GAN’s, and ResNets
	- You're a coding wizard with Python, C# (.NET), Scala, MxNet, CNTK, R, H2O, TensorFlow, PyTorch, cuDNN, NumPy, and SciPy
+ skill set:
	- At least 4 years’ experience in deep learning, machine learning or artificial intelligence applications like virtual agent, robotic process automation and video/image/text analytics
	- Minimum of 2 years’ experience in AI/ML/RPA functional expertise with developing use cases and building/leading Proofs of Concept
	- At least 2 years architecting AI Pipelines orchestrating multiple analytics engines
+ skill set:
	- Minimum 5 years of developing machine learning methods, including familiarity with techniques in clustering, regression, optimization, recommendation, neural networks, and other.
	- Strong quantitative and analytical skills with minimum 3 years of experience with data science tools, including Python, R, Scala, Julia, or SAS
	- Ability to technically lead data science projects
	- Deadline-driven, organized and able to multi-task
	- Familiarity with using cloud services (AWS, Google, Azure) or Big Data tools (Hadoop, Hive, Spark) in data science solutions
+ skill set:
	- Proven experience with caching, queuing, RPC frameworks and other building blocks of a large scale distributed systems.
	- Experience with NoSQL AWS data stores like DynamoDB, CloudSearch or their open source equivalents like Cassandra, HBase, Solr or ElasticSearch
	- Experience with React or other modern javascript frameworks.
	- Experience with MySQL, Redis, Memcache and related web-backend technologies.
	- Experience with data pipelines (Kafka, AWS Kinesis, AWS Data Pipeline)
	- Experience building web applications, widgets, or interactive experiences.
+ skill set:
	- Project management skills - JIRA, roadmapping, etc.
	- Experience with any of Ruby, Java, or modern web frameworks like React
+ skill set:
	- Investigate the feasibility of applying scientific principles and concepts to business problems.
	- Understand the Goodreads/Amazon data structures (MySQL/Data Lake/Redshift).
	- Acquire data by building the necessary SQL ETL queries.
	- Import processes through various company specific interfaces for RedShift and Data Lake storage systems.
	- Analyze data for trends and input validity by inspecting univariate distributions, exploring bivariate relationships, constructing appropriate transformations, and tracking down the source and meaning of anomalies.
	- Build models using statistical modeling, mathematical modeling, econometric modeling, network modeling, social network modeling, natural language processing, machine learning algorithms, genetic algorithms, and neural networks.
	- Validate models against alternative approaches, expected and observed outcome, and other business defined key performance indicators.
	- Develop metrics to quantify the benefits of a solution and influence project resources. Partner with Engineering/Data Engineering to improve the quality of existing data and bring additional data sources in line. Audit metric data and measure project progress and success. Build/automate reports/dashboards (in Tableau) that allow the business leaders to get a clear snapshot of their operations. Design and analyze A/B tests to quantify impact of customer-facing changes. Develop innovative experimental design and measurement methodologies to understand customer growth and business efficacy. Participate in discussions, team planning, office hours, and metric reviews. Design and implement scalable and reliable approaches to support or automate decision-making throughout the business. Communicate insights to the business partners, Goodreads leadership, and Amazon stakeholders, with an emphasis on clarity, completeness, and actionability.
+ skill set:
	- Experience designing and operating very large Data Warehouses
	- Deep understanding and knowledge of AWS stack - Redshift, EMR, S3
	- Ability to work with search technologies such as Elasticsearch
	- Knowledge of graph databases such as AWS Neptune
+ Lead evaluate of next generation technologies by conducting RFI, RFP, and POCs.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- Participate in cutting edge research in machine intelligence and machine learning applications.
	- Develop solutions for real world, large scale problems.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
+ skill set:
	- Solid Machine Learning background and familiarity with standard speech processing and machine learning techniques
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
	- Industry or academic experience in deep learning research.
	- Strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, KDD, PAMI, JMLR, TACL, IJCV).
+ skill set:
	- You have industry experience with writing code (e.g., Python, Scala, PySpark, Java) and taking ML models/ algorithms to production. Preference for 5+ years of industry experience (without PhD); at least 2-3+ years of industry experience with PhD. This is not an entry level / new college graduate role.
	- Experience with Apache Spark platform (including Datasets, SparkML) and/or experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe or PyTorch).
+ skill set:
	- **Salesforce Research and Einstein.AI (formerly MetaMind) are looking for extraordinary deep learning or research engineers.**
	- As a deep learning or research engineer, you will work with research scientists and engineers to develop and productize new cutting edge models and associated artifacts such as data preparation pipeline and model characterization logic. You will ensure these models are developed to support accuracy, performance or other specific customer requirements.
	- You will work with platform team to support deployment of these models. In other words, you are problem solver, a deep learning model designer, and an engineer who makes sure the model is deployed at scale to serve our customers with state-of-the-art speech, vision, and language technologies.
	- You have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, voice, or reinforcement learning. Additionally, applicants should have in-depth experience with problems such as text categorization, information extraction, question answering, text summarization, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Partner with product managers to understand customer requirements
	- Conduct research (including reviewing relevant literature) and collaborate with our research team to identify appropriate solution candidates
	- Develop prototypes, then design and carry out experiments to validate and improve the prototypes
	- Bring the ideas to production
	- Monitor model behaviors in production and iteratively improve quality of services over time
	- Work on cutting-edge research in machine learning
	- MA/MS or PhD degree in computer science, artificial intelligence, machine learning, speech recognition, natural language processing, or related technical field such as operations research, computational mathematics, etc.
	- Research experience or contributions in deep learning, machine learning, NLP, computer vision, reinforcement learning, or optimization.
	- Solid Machine Learning background and familiarity with machine learning techniques
	- Problem solving and ability to reuse, customize, and implement latest research
	- Experience with one or more general purpose programming languages including but not limited to: Python, Java, C/C++
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, or PyTorch)
	- Industry experience in deep learning research
	- Can thrive in team environments; using agile methodology and interacting with Product Leaders, Scientists and Engineers to solve technology's greatest challenges
	- In particular, we are looking for experienced engineers with Deep Learning experience and domain expertise around Automatic Speech Recognition (ASR), Natural Language Understanding (NLU), and Vision to provide the best possible experience for our customers.
	- Experience designing and implementing machine learning pipelines in production environments.
	- Experience in building speech recognition and natural language processing systems (e.g. commercial or government-funded speech products) is a huge plus.
	- We value professional industry experience; advanced degrees alone do not replace real world experience.
	- Excellent communication, leadership, and collaboration skills.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Candidates should have a strong publication record in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).
	- In addition to their own research agenda, senior research scientists will have the opportunity to take on additional responsibilities leading project teams, mentoring interns, and advising junior research scientists.
	- Strong publication record in machine learning, NLP, computer vision, reinforcement learning, or optimization, especially at venues like NIPS, ICML, ICLR, ACL, and CVPR.
	- Experience with one or more general purpose programming languages including but not limited to C/C++ or Python.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
+ skill set:
	- As a research engineer at Salesforce Research, your role will be at the intersection of software engineering and research, and may range from implementing novel research models to rapid-prototyping demos that show off applications of deep learning on production data. You will work closely with research scientists to develop models, prototypes, and experiments that push the state of the art in AI research, paving the way for innovative products for the Einstein AI Platform. You will have the opportunity to take on real-world problems from Salesforce’s enterprise customers with the latest deep learning models.
	- You have strong programming skills and a background in one or more of the following domains: deep learning, machine learning, natural language processing, or computer vision, with applications such as: text categorization, text summarization, sentiment analysis, information extraction, question answering, dialogue learning, language and vision, image classification, image segmentation, and object detection.
	- Knowledge of linear algebra, calculus, statistics, and machine learning.
	- Practical experience in natural language processing, computer vision, crowdsourcing, or information retrieval.
	- Exposure to industry or academic research, particularly in deep learning, neural networks, or related fields.
	- Experience with one or more deep learning libraries and platforms (e.g., TensorFlow, Caffe, Chainer or PyTorch).
	- Experience with Amazon Web Services and Mechanical Turk.
	- Strong computer systems experience in topics such as filesystems, server architectures, and distributed systems.
	- Experience in GPU programming, data visualization, or web development.
+ skill set:
	- Ideal candidates have a strong background in one or more of the following fields: deep learning, machine learning, natural language processing, computer vision, or reinforcement learning. Additionally, applicants should have in-depth experience with one or more of text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, object detection or reinforcement . Our postdoctoral researchers have the ability to give talks, attend conferences and build relationships with academic institutions if desired.
	- Collaborate on research to advance the science and technology of artificial intelligence.
	- Contribute to cutting edge research projects in machine intelligence and machine learning applications that can be infused into our world-class CRM.
	- Develop solutions for real world, large scale problems.
	- Influence progress of relevant research communities by producing publications.
	- Find and build ambitious, long-term research goals.
	- As needed or desired, lead teams to deliver on more complex pure and applied research projects.
	- Create a year long project proposal with research managers.
	- First-author publications at AI conferences and journals (e.g. NIPS, ICML, ICLR, ACL, CVPR, KDD, PAMI, JMLR, TACL, IJCV).
+ skill set:
	- Salesforce Research Asia is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision,
		* speech recognition, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue systems, language and speech, machine translation, language and vision, image classification, object detection, or image semantic segmentation, etc.
	- Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, ECCV, SIGKDD, PAMI, JMLR, TACL, IJCV) are preferred.
	- Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	- Experience with one or more deep learning libraries and platforms, e.g. PyTorch, TensorFlow, Caffe, or Chainer
	- Strong background in machine learning, natural language processing, speech, computer vision, or reinforcement learning
	- Strong algorithmic problem solving skills
	- Programming experience in Python, Java, C/C++, Lua, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for outstanding research interns. Ideal candidates have a strong background in one or more of the following fields:
		* deep learning,
		* machine learning,
		* natural language processing,
		* computer vision, or
		* reinforcement learning
	- Applied to, for example: text categorization, text summarization, information extraction, question answering, dialogue learning, machine translation, language and vision, image classification, image segmentation, or object detection.
	- Candidates that have published in top-tier conferences or journals (e.g. NIPS, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV) are preferred.
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area
	-  Excellent understanding of deep learning techniques, i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods
	-  Experience with one or more deep learning libraries and platforms, e.g. Torch, TensorFlow, Caffe, or Chainer
	-  Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	-  Strong algorithmic problem solving skills
	-  Programming experience in Python, Lua, Java, or a similar language
+ skill set:
	- Salesforce Research (previously MetaMind) is looking for an outstanding entry level research scientists focused on ethics in AI. It is our belief in the words of our CEO Marc Benioff, “The business of business is improving the state of the world." The way we behave — with integrity, transparency, alignment, and accountability — builds trusted relationships. We believe that companies can do well and do good in the world. We know technology is not inherently good or bad. It’s what we do with it that matters. With AI, we believe that we can go even further to advance and support its effectiveness by ensuring equality, transparency, and accountability in the models we create and how we implement them in our products.
	- As a research scientist, you discover new research problems, develop novel models, design careful experiments and generally advance the state of the art in AI. At Salesforce, the research team is committed to collaboration with the wider research community. In this unique role, you will have the opportunity to work directly on advancing technologies that nonprofits use to solve problems in the real world that create positive impact for the world while accomplishing publications at major conferences. We believe that making substantive progress on hard problems can drive and sharpen the research questions we study, and, in turn, scientific breakthroughs can spawn entirely new applications. With this in mind, the team maintains a portfolio of projects, some with an immediate path to production, others that may not find an application for several years. Research scientists have the freedom to set their own research agenda and move between pure and applied research.
	- As a research intern, you will work with a team of research scientists and engineers on a project that ideally leads to a submission to a top-tier conference.
	- PhD/MS candidate in a relevant research area (e.g., Machine Learning, AI, AI ethics, law and policy)
	- Excellent understanding of deep learning models and techniques (i.e., CNN, RNN, LSTM, GRU, attention models, and optimization methods)
	- Experience with one or more deep learning libraries and platforms (e.g. PyTorch, TensorFlow)
	- Strong background in machine learning, natural language processing, computer vision, or reinforcement learning
	- Programming experience in Python or a similar language
	- Strong algorithmic problem-solving skills
	- Demonstrable experience implementing machine learning models and algorithms, e.g., through open-source implementations, or shareable code
	- Strong presentation and communication skills
	- Experience applying deep learning models to ethical issues in AI or social causes (e.g., racial disparity in facial recognition, explainability of AI for redress and remediation)
	- Experience researching artificial intelligence ethics, including areas such as fairness, safety, privacy and transparency in artificial intelligence
	- Published in top-tier conferences or journals (e.g., FAT*, NIPS, AIES, ICML, ACL, EMNLP, CVPR, ICCV, SIGKDD, ICDM, ICLR, PAMI, JMLR, TACL, IJCV)
	- Open-source implementations of machine learning research projects.
	- The ideal candidate will have a keen interest in producing new science to understand intelligence and technology and how to apply it safely and fairly in real-world settings.
+ skill set:
	- Experience testing web services with tools such as SoapUI.
	- Excellent knowledge of web technologies such as React, NodeJS, AngularJS, D3JS, JavaScript, GWT, EXTGWT, CSS3, HTML5
	- Excellent knowledge of at least one server-side programming language
	- Excellent knowledge of web services development (SOAP, REST, Web Socket, RPC)
	- Proficiency with major development tools and processes such as revision control, requirement specs, unit & system tests, etc.
	- Proven ability to deliver on time working in a fast-paced agile environment
	- Ability to work with team to clarify and prune requirements, strong verbal and written communication skills
	- OO design and Java/Scala development experience.
	- Experience with Java Spring Framework
	- Good understanding of virtualization, Linux Container, Docker
	- Working knowledge of Big Data frameworks such as Hadoop, Storm, Spark, Flume, Kafka
+ skill set:
	- Proficient programmer that can code efficient algorithms (like map-reduce, preferably in Java) that traverse data partitioned in a distributed architecture
	- Design, build, and deploy distributed querying strategy to achieve highly scalable and resilient transactional processing and reporting for different size and shape workloads
	- Perform analysis on data access patterns to uncover opportunities to improve query throughput and drive decision making on new architectures. Recommend best practices.
	- Design efficiently distributed query service for low latency access and traversal for transactional and reporting use cases
	- Influence and collaborate cross functional teams in coming together towards a common, data architecture
	- Learn and fundamentally understand the Workday technology stack including a home-grown meta-data driven application development environment
	- Be responsible for system stability by proactively identifying and diagnosing issues and rapidly deploying code to address production issues
	- Strong coding experience in any language
	- Good working experience of distributed systems gossip protocols and consensus algorithms
	- Experience implementing distributed computing frameworks and architectures
	- Good knowledge of network protocols, routing and handshaking
	- Good experience performance tuning/ garbage collection / JVM internals
	- Proficient knowledge of maintaining and debugging live, business critical software systems
	- Good understanding and hands on experience with SQL, especially in the area of data aggregation and query performance tuning
	- Communicates clearly to engineering peers including ability to identify and communicate data-driven insights
+ skill set:
	- Strive for high code standards (continuously improving testability and code quality).
	- Disciplined, methodical, minimalist approach to design and construct layered software components that can be embedded within larger frameworks or applications.
+ skill set:
	- Projects around data ingestion, detection and the MITRE ATT&CK framework
	- Handle alarms and alerts, across multiple clouds and all infra (prod/corp/dev)
	- Tune alarms; we’re allergic to false positives
	- Create the glue between systems to make your life easier
	- Automate responses
	- Build runbooks
	- Perform triage and incident response
	- Build reporting and present to leadership
	- Technical depth, a desire to get things done, and be recognized for your achievements
	- Experience at a SOC and you want to do more
	- Knowledge of Linux and/or Mac
	- Exposure to at least one cloud environment (AWS, GCP, Azure)
	- Some prior Security experience
	- Ability to script in a language like Python, Ruby or Perl is a plus
+ Proven capability to create maintainable, adaptable software that is non-brittle and capable of change
+ Take pride in the quality of the code you write. Your code is readable, testable, and understandable six months later. You adhere to the Zen of Python.
+ **Experience and knowledge of programming languages, data analysis packages (e.g., Python, R, SAS, MatLab, Stata, GAMS, SPSS, Hadoop, BigML, Pandas). Experience and knowledge of visualization tools (e.g., Tableau, Sigma JS) is preferred.**
+ Experience working with distributed computing tools like Spark or Hadoop
+ Working knowledge of data analysis packages (e.g., SAS, MatLab, Stata, GAMS) is strongly preferred.
+ Proficiency with prototyping and design tools (e.g., Sketch, InVision, Adobe Creative Suite, Axure).
+ Create product backlog/requirement documents (epics, stories, scope docs, etc) to guide engineering in developing the platform features, and work closely with the engineering and QA teams to execute the product feature per specification
+ Experience with DFIR techniques and tools (FTK, EnCase, SIFT, including Volatility) and eDiscovery
+ Familiar with orchestration components (Chef-Puppet-Ansible-Kubernetes-VSTS)
+ Supports the full lifecycle of a capability from Early Adopter Programs (EAP) to General Availability (GA) and across a full range of commerce events including new sales, upgrades, renewals, downgrades, and cancellations with robust monitoring and instrumentation.
+ skill set:
	- Achieve security architecture compliance on requirements, including: Sarbanes-Oxley, payment card industry standards, HIPAA/HITECH, global data privacy requirements, as well as state and federal regulations.
	- Exceptional experience in designing cloud security architecture for Azure and/or AWS.
	- Establish a strategic security architecture vision, including standards and frameworks that are aligned with overall business strategy.
	- Provides architectural oversight and direction for enterprise-wide security technology.
	- Review existing architecture, identify design gaps, and recommends security enhancements.
+ skill set:
	- Application systems, network architecture, multiple platforms and new technologies from a security perspective to include, but not limited to, Firewalls; Intrusion Detection/Protection Systems; Operating Systems (UNIX, Windows); Networking (switches, routers, protocols, etc.); Network Services and Security Vulnerabilities; Network Architecture; Remote Access; Multiâ€factor Authentication; Platform Security (Application, Database, OS); Antivirus; Federated Identity Management; Cryptography; Active Directory; and high-level programming languages.
	- System and network exploitation, attack pathologies and intrusion techniques (such as denial of service, sync attacks, malicious code, password cracking, etc).
+ skill set:
	- Experience managing enterprise monitoring solution; System Center Operations Manager (SCOM), Solarwinds, and/or CA UIM preferred
	- Experience managing server automation tool and server patching tool; System Center Configuration Manager (SCCM) preferred
	- Experience creating or modifying scripts or automation, such as Perl, PowerShell, Python, TCL/TK, Ruby or similar for cloud orchestration required
+ Experience working with SOAP, REST APIs and micro services
+ The ideal candidate will have at least 5 years of experience as a UX designer including user research, prototyping and using visual communication tools (e.g. Sketch, InVision, Axure).
+ Ideal candidate will have at least 5 years of experience as a UX designer, including; user research, prototyping and using visual communication tools (eg. Axure, Balsamiq, Sketch).
+ skill set:
	- 2+ years SQL working experience (Redshift/PostgreSQL/MySQL)
	- Experience with BI & reporting dashboards (Periscope, Tableau, etc)
+ skill set:
	- Experience developing, troubleshooting, tuning and managing hardware systems including:
		* LIDARs
		* Standard or Depth Cameras
		* Light systems
		* IMUs, encoders or other odometry methods
	- Experience designing/developing one or more of the following software systems/algorithms:
		* Mapping, Localization, and/or SLAM
		* Robot perception, Sensor calibration and/or fusion
		* Motion planning and/or Autonomous navigation
		* Production level configuration, log and system management
		* Remote monitoring and control
		* Performance verification and monitoring
		* Test automation of combined software and hardware components
		* Automated release pipelines
+ Modern networking: HTTP, TCP, MQTT and can write software that tolerates network outages
+ skill set:
	- Develop and maintain scalable codebase used to calibrate sensors and actuators used on a robot system including:
		* Camera: Intrinsic, Extrinsic, White-balance calibration of both standard and fisheye cameras
		* Time-of-Flight cameras
		* Lidar
		* IMU
		* Odometry
	- Improvement of existing calibration processes and procedures
	- Develop calibration methods for new sensors added to robot
	- Hands-on deployment of calibration fixtures at contract manufacturing line
	- Hands-on calibration of robot system to validate new calibration processes
	- Assist mechanical, electrical and other teams to design fixtures needed for calibration
	- Work with manufacturing and field engineers to debug field issues related to calibration
	- Work with hardware and software teams to provide well calibrated robot system
	- Good C++, C and python coding skills
	- 2 years of industrial experience
	- Previous experience with extrinsic calibration of at least two of the following sensors: RGB Camera, ToF Camera, Lidar, Odometry, IMU
	- Understanding of standard camera calibration methods
	- System level understanding of how calibration affects robot performance
+ skill set:
	- Work with product management to write visualizations and searches in ELK.
	- Design queries and system to monitor data quality from our robot fleet
	- Extend the ELK platform to support ongoing use cases through adding infrastructure
	- Support ongoing management of the ELK cluster
	- Build tooling and infrastructure to increase observability of services.
	- Support cloud operations by performing triage and responding to incidents.
	- Production experience with the ELK platform
	- Experience managing infrastructure and services on a public cloud provider; AWS, GCP, or Azure
	- Experience with configuration as code; Puppet, SaltStack, Ansible, or Chef
	- Proficiency with Java, Python, or Go
+ skill set:
	- Exposure to containers or orchestration services:  Kubernetes, Mesos, or Docker Swarm
	- Experience with configuration as code; Puppet, SaltStack, Ansible, or Chef
+ skill set:
	- Experience using native APIs from higher ed core systems (SIS, ERP, LMS) a plus
	- 3+ years’ experience with enterprise level data integration working with multiple systems simultaneously; Including extracting data utilizing API integration from a variety of platforms, performing data mapping, data transformation, and loading data to the target system
+ skill set:
	- Talend ETL, SQL, Postgres, AWS
	- Design, develop, and implement advanced ETL pipelines that bring together data from disparate sources, making it available to users using a variety of ETL tools.
	- Facilitate cross-functional data-integration efforts upstream and downstream
	- Detect data quality issues, identify their root causes, implement fixes, and design data audits to capture issues
	- Extract data from multiple sources, and integrate them into a target database, application, or file using efficient programming processes.
	- Implement and deploy solutions in a CI/CD pipeline
	- Write and refine code to ensure performance and reliability of data extraction and processing.
	- Communicate with all levels of stakeholders as appropriate, including product managers, application developers, business users.
	- Participate in requirements gathering sessions with product managers and technical staff to distill technical requirements from business requests.
	- Recommend process improvements to increase efficiency and reliability in ETL development.
	- Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.
	- Some of our technologies might include: Talend as well as various data stores such as Postgres SQL, S3, Aurora and AWS services.
	- 2+ years of experience on Data Warehousing and building data pipelines.
+ Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Knowledge of Big Data, SAP ERP, Docker, Kubernetes, CXF or another ETL product is a plus;
	- AWS, Azure, Google cloud, Apache Beam, NoSQL
	- Experience of working with JDBC, XML, Junits, Maven, Avro and JSON;
	- Good understanding of Web Services (SOAP/REST), knowledge of CXF is a plus.
+ skill set:
	- Experienced in data sanitization, data import and export (ETL).
	- Familiar with SQL Server products, i.e. SQL Integration Services and Reporting Services.
	- Work with the application team to create and maintain effective database-coupled application logic stored procedures, triggers and user-defined functions (UDFs); these are programs that are under the control of the DBMS (SQL, MySQL, Postgres, MongoDB)
+ skill set:
	- Using different protocols as needed for different data services (NoSQL/JSON/REST/JMS/JMX) and providing related tests (Test-driven development/Unit Testing and automation)
	- Designing, implementing, testing, and deploying a data processing infrastructure that is fault tolerant and scalable to support multiple Talend Products
	- Building High-Availability (HA) architectures and deployments
	- Between 3 and 5 years in professional Java programming with RESTful, Message-/Event-Driven technologies, Multi-threaded applications
	- An understanding of distributed and cloud computing, incl. deployment related experience (ideally including Docker, AWS or Azure)
	- Experience with OAuth and other IAM related technologies would be a plus
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
	- Document structure, process and design of all implemented solutions
	- Between 3 and 5 years in professional Java programming with RESTful, Message-/Event-Driven technologies, Multi-threaded applications
	- Experience with development using Scala/Akka
	- An understanding of distributed and cloud computing, incl. deployment related experience (ideally including Docker, AWS)
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Experience with Apache Big Data technologies such as Hadoop, Spark, Hive, Flink, Kafka, Beam etc
	- Experience with messaging systems such as ActiveMQ
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- TMC runs as load-balanced application server instances on Amazon Web Services (AWS). After building a data pipeline in Talend Studio, the job is executed on one or several so-called “Cloud Engines” or “Remote Engines”.
	- Cloud Engines are Java-based runtimes deployed via an Amazon Machine Image (AMI) based on CentOS.
	- Remote Engines are optional Java-based runtimes deployed by the customer to process data behind the firewall or on a Virtual Private Cloud (VPC), e.g. this can be on-premises or in third party clouds like Google, Azure or AWS.
	- Within this position the development will mainly be focused on server-side Java development, leveraging technologies from the Hadoop Ecosystem (like HDFS, Spark, Kafka, Zookeeper), Log Data Mgmt (like ElasticSearch), Identity & Access-Management, Metric Collection Solutions, and others. This provides the applicant with the ability to work with up to date technologies, in a modern cloud-based development and deployment environment.
	- Built High-Availability (HA) architectures and deployments
	- Worked and communicated in a cross-functional geographically dispersed team environment comprised of software engineers, product managers, software test engineers, and product support engineers.
	- Good knowledge in Java ecosystem (Java 8, Spring, junit, logging)
	- Skills in Restful Service and the microservice architecture (SpringBoot)
	- Some basic knowledge in databases like NoSQL DB (MongoDB)
	- Knowledge in CI tools (Maven, Jenkins)
	- Some knowledge in AWS and Docker would be a plus
	- Join a passionate team and work with the latest technologies (Hadoop, K8s, Terraform, AWS, GCP to name a few)
+ skill set:
	- Experience building or maintaining databases (MySQL, Hive, etc.)
	- Experience building or maintaining Big data & streaming systems (Hadoop, HDFS, Kafka, etc.)
	- Cross-platform coding
	- Security
	- Large-scale, large-user base website development experience
	- Data mining, machine learning, AI, statistics, information retrieval, linguistic analysis
	- Strong mathematical background
+ skill set:
	- Help us iterate quickly and deliver high-quality software releases, on-time
	- Understanding of service based cloud architectures
	- Work closely with the software engineering, product management and customer support teams to design, deliver, and manage our services with high uptime
	- Implement monitoring, develop automated provisioning, and develop self-healing automation
	- Perform incident resolution and root cause analysis of critical outages
	- Experience with design and maintenance of a cloud based highly-available (HA) architecture
	- Experience with configuration management and monitoring tools
+ skill set:
	- Partner with engineering leadership to buildout data driven roadmap items to address performance in critical areas
	- Established performance test environments and frameworks
	- Experience evangelizing performance engineering techniques within a data driven engineering culture
	- Deep hands on experience with JVM tuning techniques
	- Supported efforts in performance testing and improvement in common JavaScript frameworks (Angular, React, JQuery)
	- Experience with Ruby (JRuby) and JavaScript
	- Extremely well versed in solving data access performance challenges across SQL data stores
	- Experience in AWS and other cloud providers when exploring different approaches to performance engineering
	- Experience with distributed architectures
	- Passionate about driving a performance engineering culture
+ skill set:
	- Looker is seeking a Senior Software Engineer to join our Data Model team (database semantics, programming languages, and integrated development environment (IDE)). This team’s core responsibilities include Looker’s SQL normalization and code generation engine (the heart and lungs of our application), the LookML language itself, Looker’s in-browser IDE for composing and versioning LookML, and the data pipeline within the Looker application.
	- The ideal candidate will take an active role in contributing to our long-term technical roadmap and have a deep background in programming language fundamentals (e.g. compiler design) or databases (e.g. building SQL optimizers) or building IDEs, in addition to tried and true experience with software engineering best practices.
+ skill set:
	- Machine learning. You should be able to understand and apply major machine learning methods, such as logistic regression, SVM, Decision Trees, Principal Component Analysis and K-means. Completion of Andrew Ng’s Machine Learning course on Coursera is sufficient to meet this criterion.
	- Deep learning. You should be able to understand and apply major deep learning methods, including neural network training, regularization, optimization methods (gradient descent, Adam), and be familiar with major neural network architecture types such as Convolutional Networks, RNN/LSTM. Completion of the deeplearning.ai specialization is sufficient to meet this criterion.
	- Implementation. You should have prior experience taking a dataset, cleaning it if necessary, and applying a learning algorithm to it to get a result. You should be able to implement a learning algorithm “from scratch” using a framework such as NumPy, Tensorflow, Pytorch, Caffe, etc.
	- General coding. You should be able to code non-trivial functions in object-oriented programming, such as popular sorting or search algorithms.
	- Mathematics (including probabilities and statistics.) You should be able to use mathematical notations and linear algebra (matrix/vector operations, dot products, etc.), and understand basic probability theory (distributions, independence, density functions, etc.) as well as statistics (mean, variance, median, quantiles, covariance, etc.)
	- Software Engineering. You should know how to use your terminal, work with version control systems (Git), relational databases, APIs, and build the back-end of web or mobile applications.
	- Mean Stack
		* MEAN is a free and open-source JavaScript software stack for building dynamic web sites and web applications.
		* The MEAN stack is MongoDB, Express.js, AngularJS (or Angular), and Node.js.
+ LAMP is an archetypal model of web service stacks, named as an acronym of the names of its original four open-source components: the Linux operating system, the Apache HTTP Server, the MySQL relational database management system (RDBMS), and the PHP programming language.
+ LYME and LYCE are software stacks composed entirely of free and open-source software to build high-availability heavy duty dynamic web pages. The stacks are composed of:
	- Linux, the operating system;
	- Yaws, the web server;
	- Mnesia or CouchDB, the database;
	- Erlang, the functional programming language.
+ skill set:
	- The steps of an end-to-end machine learning project. This includes, but is not limited to:
		* Conducting a structured and deep literature review of a specific field.
		* Strategizing your machine learning project end-to-end.
		* Collecting, cleaning, labeling, and augmenting your own dataset.
		* Training a model for a real-world application.
		* Setting-up an efficient and organized experimentation process.
		* Defining task-specific metrics to optimize in your experiments.
		* Performing error analysis to improve your models.
		* Deploying an AI product.
		* Exposure to real-world problems that multiple AI teams in our community work on.
	- Hands-on experience in designing, building, and deploying end-to-end AI solutions through curated content and instructor-led workshops.
	- Career mentorship and connections with teams aligned with your career aspirations.
	- Meet and share experiences with other machine learning engineers and data scientists.
	- Everyone who successfully completes the Bootcamp will be awarded a certificate of completion and join the AI Bootcamp Alumni community.
	- Machine learning Engineers and Data Scientists who have already worked on Machine Learning projects and want to get exposed to different Machine Learning problems.
	- Demonstrated AI, data science and/or data analysis experience from previous work experience or publications.
	- Demonstrated strong coding from previous work experience or publications. This means you’re able to write a non-trivial program in Python, Java, or C++.
	- Solid CS foundation (including but not limited to Operating Systems, Computer Networks, Database, etc.)
+ skill set:
	- This person will assist in developing data migrations, writing SQL and reports and mentoring other engineers in optimizing and writing efficient queries.
	- Develop and proactively review the monitoring of production PostgreSQL databases
	- Participate in system capacity planning
	- Participate in database design, data modeling and provide recommendations for improvement or optimizations
	- Provide query / index optimizations
	- Participate in an agile software development life cycle including providing testing guidelines for database related changes
	- Provide SQL development support and query tuning
	- Mentor other engineers in developing efficient SQL queries
	- Follow the Quality Management System for developing and deploying software
	- Write reports
	- 3+ years experience managing a production RDBMS including experience with PostgreSQL
	- Experience in SQL development and database design
	- Expertise in SQL DML and DDL
	- Have a solid understanding of query planning
	- Understand PostgreSQL tuning and optimization parameters
	- Excellent interpersonal and communication skills in both oral and written English
	- Able to collaborate with cross functional team members
	- Familiarity with PostgreSQL replication techniques, data warehouse design, and Amazon RDS support beneficial
+ skill set:
	- The Project Portfolio Analyst oversees the activities that support the company’s most complex strategic projects by ensuring reporting and governance alignment, providing portfolio performance measurements and recommendations, and trending analytics. This position is responsible for the Portfolio’s intake, supply and demand (resource management) methods, analytics and recommendations. Reporting responsibilities includes detailed analysis, dashboards and decision support updates (KPI’s) and recommendations on a regular cadence. The Portfolio Analyst will at times provide project support to Project and Change managers.  This position reports to the Associate Director, Organization Engineering.
	- Owns and administrates the project collaboration toolset including project portfolio management software and various collaboration tools
	- Develops and maintains a project risk tracking mechanism to centralize risk tracking across the company’s project portfolio
	- Drives agile governance process that maintains control and compliance but improves project team effectiveness and improves decision making
	- Assists in developing project management processes, tools and training as well as project status presentations and reports
	- Supports the strategic planning process and cross-functional business reviews to include development of in-depth analysis, executive presentations, and resource modeling
	- Owns the project idea intake process to ensure project goal and scope are clearly defined, reasonable budget and timelines are established and the project is resourced effectively to deliver the desired solution
	- Owns the resource planning process to include maintaining resource management information and resource utilization modeling
	- Generates, validates, distributes, archives and supports project management documentation and reporting collateral
	- Monitors quality assurance of project implementation across a large portfolio of business and process improvement projects by establishing, monitoring and reporting on metrics to determine whether projects meet quality, cost and schedule targets
+ skill set:
	- Identify, recommend and implement improvements on the project management process and tools – examples:
		* Project idea intake process
		* Resource planning process
		* Project prioritization process
		* Project close-out process
		* Project portfolio management software and collaboration tools
		- Project management process, templates and tools documentation and training
	- Demonstrated experience working with project and portfolio practice, including project ideation
- 	Demonstrates basic understanding of project management methodology
	- Strong influencing skills; demonstrated ability to challenge and persuade, directing a group of stakeholders to the best decision
	- Solid analytical and problem-solving skills; ability to think strategically
	- Demonstrated ability to communicate complex ideas clearly and concisely as well as proven ability to effectively interface and influence at all levels of an organization
	- Moderate proficiency with MS Project as well as MS Visio, Excel and PowerPoint
	- Minimum of 2 years of experience with transactional-based continuous improvement projects
+ skill set:
	- Software Engineer (Media Streaming) - Periscope
	- We are a small team that develops media streaming services and client libraries for Periscope and Twitter's professional live streams. The service ingests thousands of concurrent live video feeds and serves them to viewers in a way that scales to large audiences while maintaining low latency. The service also provides low-latency transcoding and stores live streams for on-demand viewing. On the client side, our cross-platform libraries power low-latency broadcasting and playback in Periscope and Twitter mobile apps. You can learn more about our techniques for low-latency streaming at scale here (https://medium.com/@periscopecode/introducing-lhls-media-streaming-eb6212948bef) and here (https://youtu.be/RbH_2l77Pm8).
	- The role of our service and client libraries is expanding beyond the Periscope use case to handle content from an ever-increasing set of professional broadcast sources and for increasingly high-profile events. We are growing the capabilities, reliability, and quality of our service libraries to power more and more of Twitter's live video.
	- We are looking for a generalist software developer. The ideal candidate has distributed systems experience but would also be comfortable contributing on the Android and iOS client side. While experience in developing media streaming systems is a plus, the position doesn't require domain knowledge in media codecs and streaming.
	- The majority of our server-side codebase is in Go. If you have never worked in Go but you are comfortable in C, C++, or Java, you’ll pick it up quickly. Experience with AWS is a plus. Our service uses EC2, DynamoDb, S3, SQS, and Redis, for example. On the client side, our codebase is primarily in C++. Experience in Objective-C and Java would also be a plus. In your day-to-day work, you will encounter media technologies including RTMP, RTP, HLS, H.264, AAC, Opus, the WebRTC native stack, and CDN infrastructure.
+ skill set:
	- Backend development experience with a strong interest in work involving data pipelines, distributed systems, performance analysis, and/or large-scale data processing Experience with software engineering practices (e.g. unit testing, code reviews, design documentation)
	- Able to take on complex problems, learn quickly, and persist towards a good solution
	- Experience designing fault-tolerant distributed systems
	- Experience with data pipelines
	- Experience with Hadoop or other MapReduce-based architectures
	- Experience with Kafka, Druid or other Streaming Compute based technologies is a plus
	- Experience with ad tech is a plus
+ skill set:
	- Experience with asynchronous I/O and coroutines
	- Experience with event driven service architecture
+ skill set:
	- The Consumer Data Science organization works closely with our cross-functional partners, and Twitter’s leadership to understand user behavior, inform product decisions, safeguard the health and integrity of our services, and to influence company strategy. We are currently hiring for the following subteams on Consumer Data Science. These high-impact teams value creativity, critical thinking, and teamwork.
	- The Consumer Data Science organization is hiring Senior Data Scientists in the following areas:
		* Health (SF, Boulder) - The goal of this team is to improve the health of the public conversation, ensuring that users feel safe while using our platform. As part of our team, you'll help the Health Organization make strategic decisions that ensure that Twitter is a safe and informative experience for our customers. You will do this by performing and mentoring others through analyses, metrics, experimentation, research, and more.
		* Growth (SF) - Their mission is to increase Twitter’s daily utility for new and returning users through impactful and creative applications of experimentation and data analysis. As a key member of Growth Data Science, your work will directly influence exciting new product areas and help grow Twitter usage around the globe.
		* Metrics (SF) - This team works to support company strategy by helping to define, maintain, and understand key success metrics to ensure that we continue to meet the demands of our customers.
		* Video (NY) - This team works with the Live Video, Video on Demand, Publishers, and Camera products. The team is involved in opportunity sizing, experiment setup and analysis, and metric design in order to influence video and media strategy at Twitter.
	- Support the entire product development lifecycle from product ideation to opportunity sizing to measurement design to experimentation and causal analysis to post-launch learning and iteration into the next development cycle.
	- "Design and implement experiments or other econometric methods to understand how changes to the platform affect user behavior."
	- Build novel metrics, identify the impact of product and policies, and study causal impact of our Product launches and Health initiatives.
	- Work in tandem with team members, applying advanced statistical methods; writing complex data flows using multiple languages/frameworks such as SQL, Scala (Scalding, Spark), Python; and using data visualization tools.
	- Communicate findings to executives and cross-functional stakeholders.
	- You are a self-starter who is capable of learning on the job, taking initiative, and thriving within a large team.
	- You are excited to learn and apply new data analysis techniques and tools. You are passionate about insights, not just data and methods. You are a strategic thinker and are able to synthesize technical concepts into actionable recommendations for a diverse audience.
	- You communicate your findings clearly and effectively to a wide audience of relevant partners and are capable of building meaningful presentations and analyses that tell a story.
	- You are rigorous, care about data quality, and strive to understand surprising results and underlying mechanisms in your analyses. You combine business insight with detailed data knowledge and statistical expertise to ensure an accurate interpretation of results.
	- You are a capable mentor. You enjoy knowledge sharing and working with junior teammates to up-level their skills and take the time to learn from them.
	- You value teamwork and teammates. You contribute positively and meaningfully to cultivate an inclusive team culture. You are personable, empathetic, and able to connect with each and every person on the team and throughout the company.
	- Experience using SQL, R, or Python for analysis, modeling, and data visualization.
	- 5+ years experience working with and analyzing large datasets to understand behavior, solve problems, and answer business questions.
+ skill set:
	- Cortex is a team of software engineers and machine learning scientist to developing state-of-the-art machine learning capabilities to refine and transform our products.
+ tech stack:
	- Netflix culture resonates with you.
	- You can communicate effectively with experts of all backgrounds.
	- You are an expert analyst and can pick up any tool (e.g. Tableau, D3) to get the job done.
	- You dream in SQL and Python (or other similar languages).
	- You are comfortable with Big Data technologies like Hadoop, Spark, Hive, Presto etc.
+ Expertise in SQL, programming (e.g. Python, Scala), ETL and data warehousing concepts at scale (TBs of data)
+ Expertise in broad technical skills spanning data access, data storage, data processing, and data visualization.  Skills include: SQL, logical / semantic data modeling, ETL and data warehousing concepts, programming languages (Python)
+ Expertise in statistical inference including experimentation and observational methods to causal inference
+ Strong coding experience. Experience with open-source ML packages (specifically sklearn, TensorFlow/Keras/PyTorch).
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Experience in Recommendation Systems, Personalization, Search, or Computational Advertising
	- Experience using Deep Learning, Bandits, Probabilistic Graphical Models, or Reinforcement Learning in real applications
	- Experience in optimization algorithms and numerical computation
	- Experience with Spark, TensorFlow, or Keras
	- Experience with cloud computing platforms and large web-scale distributed systems
	- Open source contributions
+ tech stack:
	- 5+ years of research experience with a track record of delivering quality results
	- Expertise in machine learning spanning supervised and unsupervised learning methods
	- Experience in contextual multi-armed bandit algorithms and/or reinforcement learning
	- Experience in successfully applying machine learning to real-world problems
	- Strong mathematical skills with knowledge of statistical methods
	- Strong software development experience in languages such as Scala, Java, Python, C++ or C#
	- Great interpersonal skills
	- PhD or MS in Computer Science, Statistics, or related field
	- Recommendation Systems, Personalization, Search, or Computational Advertising
	- Deep Learning or Causal Inference
	- Optimization algorithms and numerical computation
	- Spark, TensorFlow, or Keras
	- Cloud computing platforms and large web-scale distributed systems
+ tech stack:
	- Write C++ code to tackle scientific algorithmic problems such as: transforming 3D coordinates, Metropolis Monte Carlo simulation, Gradient Descent minimization, and others.
	- Implement highly optimized multi-threaded C++ or CUDA code that scales well on cloud infrastructure.
	- Work closely with other software engineers via code reviews and testing to foster high-quality software and systems.
	- Solid computer science fundamentals, with strong competencies in data structures, algorithms, and compilers.   
	- Experience profiling C/C++ code to find and fix performance bottlenecks.
	- Comfort with the Linux command-line environment.
	- Background in Biology, Chemistry or Physics.
	- Familiarity working with Docker and Kubernetes.
	- Knowledge of parallel computing paradigms and demonstrated proficiency in some of the following: openMP, CUDA, or openCL.
	- https://www.atomwise.com/jobs/senior-software-engineer-scientific-computing/
+ tech stack:
	- You should think about joining us if you care about making a difference in treating disease and saving human lives. But also, if you are up to tackling some of the hardest open challenges in deep learning today:
		+ Non-stationary, unbalanced, and noisy data: Our training data is seldom i.i.d.; new medicines are unlocked by pushing out into newly-discovered biology. Classes are extremely unbalanced, ratios of 1 positive to 70,000 negatives are typical. Help us reason about how to learn appropriately without dismissing nor overfitting to the data; identify when we can trust a label or have confidence in a prediction; and develop techniques to find and correct for systemic biases.
		+ Extreme scaling: Medicinal chemists can synthesize about a trillion trillion molecules today. Help us scale predictive algorithms to orders of magnitude beyond those contemplated in any other problem domain today.
		+ Multi-parameter optimization: Medicine has to be both safe and effective, so we have to concurrently optimize a number of criteria such as potency, selectivity, solubility, toxicity, synthesizability, etc. Help us efficiently explore the Pareto frontier and avoid mode collapse.
		+ Adversarial generation of synthetic data: Data augmentation has shown utility in improving the robustness of predictions. Help us find ways to best integrate molecular physics simulation and machine learning to impute new data.
		+ Explainability and visualization: Subtle patterns govern molecular recognition. Help us to understand how they lead to the discovery of fundamental chemistry by AI.
	- Ph.D. or M.Sc. in computer science, statistics, data science, or related field
	- 5+ years of extensive practical experience and proven track record of developing, implementing, debugging, and extending machine learning algorithm
	- Knowledge of modern neural network frameworks such as Tensorflow, Torch, or Theano
	- Strong analytical and statistical skills
	- Scientific rigor, healthy skepticism, and detail-orientation in running and analyzing experiments
	- Familiarity with processing large data sets in a Linux environment
	- Software engineering skills and coding experience in at least one high-level programming language (Python, R, Java, C/C++, etc.)
	- Biomedical knowledge or experience in processing chemical or biological data is preferred but not required
	- Experience with cloud computing environments (AWS/Azure/GCE)
	- https://www.atomwise.com/jobs/senior-machine-learning-research-scientist/
+ tech stack:
	- Interact with customers to understand their project requirements.
	- Generate and analyze predictive results, and deliver these to our clients.
	- Communicate our results and capabilities through scientific publications.
	- Analyze, curate, and automate the processing of our biochemical databases.
	- Help to develop our modeling software.
	- Ph.D. in chemistry, biology or a related field.
	- Extensive knowledge of medicinal chemistry.
	- Minimum 3 years of experience in lead optimization at a pharmaceutical company.
	- Experience in Computer-Aided Drug Design: structure/ligand-based drug design, molecular docking, virtual screening, QSAR, pharmacophore modeling, PK/PD data analysis and modeling.
	- Comfortable on the Linux command-line.
	- Undergraduate knowledge of statistics.
	- Good knowledge of Python, Perl, Bash, or related scripting languages.
	- Statistical modeling.
	- Experience with cloud computing environments (AWS/Azure/GCE).
	- https://www.atomwise.com/jobs/senior-computational-chemist/
+ tech stack:
	- M.Sc/Ph.D. in Computer Science, Statistics, Cheminformatics, Bioinformatics, Computational Biology or B.S. in Computer Science with 7+ years experience.
	- Strong computer science fundamentals
	- 5+ years of experience in database engineering, data processing pipelines, and HPC
	- Strong database design and software-engineering best practices
	- Strong knowledge of statistics, data analytics, and data visualization
	- Strong coding skills in at least one high-level programming language (Python, Java, C++, etc)
	- Good familiarity with Linux command-line environment
	- Experience in bioinformatics or cheminformatics, working with ingestion of third-party and internal data sources
	- Experience working with scalable algorithms utilizing large amounts of data
	- Experience with cloud computing environments (AWS/Azure/GCE)
	- Experience with non-relational databases
	- Familiarity with organic chemistry
	- https://www.atomwise.com/jobs/senior-data-engineer-cheminformatics-bioinformatics/
+ tech stack:
	- Hadoop ecosystem and its components.
	- Hadoop, Hive, HBase, and Pig
	- Working experience in HQL
	- Pig Latin Scripts and MapReduce jobs
	- Hands-on experience in backend programming, particularly Java, and Node.js
	- Analytical and problem-solving skills
+ Expertise in additional statistical methods (e.g., Bayesian approaches, dyadic analysis, causal inference approaches, factor analysis, SEM)
+ Use of Jira and A-Ha planning tools.
+ Build data tooling to enable data lake, data warehouse, and analytics workflows within the AWS cloud (S3, Redshift, DynamoDB, Spark, Kinesis, Kubernetes, etc.)
+ Experience with SAML, OAuth
+ Working with Jira and Confluence a plus.
+ Experience working with modern deep learning software architecture and frameworks including: Tensorflow, MxNet, Caffe, Caffe2, Torch, and/or PyTorch.
+ Experience with configuration management systems (Ansible and/or Puppet, Saltstack)
+ tech stack:
	- Remote hardware administration with IPMI
	- Configuration and management of
	- SGE/Univa, Slurm, LSF or other DRMS
	- Jenkins CI
	- Phabricator
	- FlexLM licensing
	- Puppet, Ansible, Nagios
	- LLVM, GCC
	- DVCS e.g. Git
	- AWS, Azure, Google Cloud
	- XML and XPath/XSLT
	- Web programming – HTML/DOM, JavaScript, SQL
	- A solid knowledge about how orchestration tools (Kubernetes, Swarm, OpenStack, etc) can be used to deploy, scale, and operate virtualized entities
	- Understand CPU virtualization and container technology from the inside out (hypervisors, Xen, LXC, Docker)
	- The role involves using a range of technologies, such as Python, CMake, BuildBot, Phabricator, AWS etc.
	- Good knowledge of management and security frameworks (SNMP/MIB agents, CLI, RESTful API, OpenBMC) is very useful
	- Knowledge of storage systems (File, Block) is a plus (Local/Network/Cloud Attached)
	- Knowledge of ILOM, BMC, and OCP (Open Compute) is a plus
	- Test automation experience (Some exposure to CTest is desirable)
+ Bonus: Prior experience with Zendesk, Jira or Asana, SQL, in Customer Service or Hospitality
+ Producing effective and interactive data visualizations (Tableau, Shiny, D3)
+ Experience with Databricks or Spark, EMR
+ Bonus points for experience building interactive data visualizations using libraries like D3, Highcharts, and Leaflet, and for experience working with big data systems like Hive, Hadoop, Scalding and Spark.
+ Experience with Scala, Scalding, Luigi,Hive, machine learning pipelines and model training is a plus
+ Automate DB (Oracle, Postgres, MongoDB, ...) configuration, deployment, backups, ...
+ Experience with at least one of: Oracle, Postgres, MongoDB, Solr
+ Chef/Puppet/Ansible/Terraform experience is nice to have
+ Experience with full-text search engines (Solr, Elasticsearch).
+ Familiarity with Docker (and Kubernetes/Mesos Marathon)
+ Familiarity with Angular framework
+ You will work with technologies like: AWS, Docker (Mesos/Kubernetes), HashiCorp tools (Terraform, Consul, Vault,...), Chef, Ansible, SQL and NoSQL databases, Nginx, ...
+ Elasticsearch, Hadoop, Big Data experience is a plus
+ workflow management tools like Airflow
+ search backends like ElasticSearch
+ Spark and/or other big data architectures (Hadoop, MapReduce) in high-volume environments
+ VM embeddings in other systems (e.g., DBMSs, Big Data frameworks, Microservices, etc.)
+ Virtual machines: Managed runtime systems (e.g., JVM, Dalvik VM, Android Runtime (ART), LLVM, .NET CLR, RPython, etc.)
+ We preferred students experienced in the use of ROS (Robot Operating System) and simulation engines such as Unity3D and Unreal Engine 4.
+ ***Knowledge of parallelism in shared (Intel TBB, OpenMP) and distributed (Intel MPI, Apache Spark, Dask) memory***
+ Speech (NLP: ASR, MT, NLP, NLU, TTS, DM, and ASP)
+ Knowledge of OpenCL/SYCL languages
+ ***Experience with large-scale, distributed data processing frameworks (e.g., Spark, Kafka, YARN, Tachyon, Mesos, etc.) is a plus***
+ Domain knowledge and project experience in below area will be a plus: x86 architecture; Linux kernel; Virtualization; Cloud SW stacks; Big data; Machine Learning, compiler and run time optimization, etc.
+ Knowledge of Linux and/or Windows programming and computer graphics including OpenCL\*, OpenGL\*, DirectX\*
+ ***Open-source projects that demonstrate relevant skills  and/or publications in relevant conferences and journals (e.g. NIPS, ICML, ICLR, CVPR, ICCV, ECCV, ICASSP)***
+ ***Experience working with analytics tools such as Google Analytics, Heap Analytics, Chartmogul, Baremetrics, Periscope, Tableau, Mode Analytics, Looker, or similar***
+ tech stack: Golang, AWS (DynamoDB, Lambda, EC2, Kinesis, SQS, S3), ReactJS, Snowflake, Terraform, Redis, SolrCloud, Kafka, Riak, Docker/Kubernetes, and Linux
+ tech stack:
	-  Solid knowledge in control theory, especially model predictive control
	- Experience in one or more of the following areas:
		* Robust control
		* Adaptive control
		* Nonlinear control
		* State estimation
		* Parameter estimation
		* Model identification
		* Optimization
	- Experience in one or more of the following areas:
		* Control theory
		* Motion planning
		* Optimization
		* Formal logic
		* Game AI development
	- Experience with sensors: cameras, lidars, ultrasonic, etc.
	- Computer vision experience, image processing experience
+ You know what the CAP theorem is and you feel confident that you can speak to what it does and does not cover in systems design.
+ Expert in prototyping traditional ML (GBMs, scikit, etc.) and AI frameworks (keras, tensorflow, mxnet, pytorch, etc.) for a variety of applications
+ tech stack:
	- Knowledge of dsx, IGC, and IA
	- Web service development with NodeJS (Backend server Javascript), front-end Javascript and Flask (Open source python library)
	- Both relational database and NoSQL database technologies
	- The main responsibility will be to enable different personas like Data Scientist, Data Curator, and Data Engineer to collaborate with each other on their Data Journey and work with various products like dsx, IGC, IA, and DFD.
+ tech stack:
	- PHP5, PHP7, HTML5, CSS3, JavaScript, Jquery, MySQL, NoSql
	- Should have in depth understanding in LAMP stack
	- Must have Good OOP (Object Oriented Programming) concepts
	- Knowledge in MVC Framework e.g (Zend Framework, Laravel)
	- Clear understanding of JSON, AJAX, XML, CURL, Web Service
	- Knowledge of the Linux command line tools optional
	- Good to have experience in Angular JS and Node JS, concept of UI/UX
	- Experience in handling large database, including designing & advanced querying in MySql
	- Good knowledge of version control tools like GIT, SVN
+ Versatile with languages and technologies. You pick the right technology for the job. You might have dabbled with iOS, Android, React Native or Flutter; you tinkered with TensorFlow, Caffe or Torch; you know when to use Mongo vs Redshift; and you have an opinion on web frameworks.
+ OpenStack:
	- Dashboard (Horizon)
	- Compute Service (Nova)
	- Networking (Neutron)
	- Object store (Swift)
	- Identity service (Keystone)
	- Metering & Data Collection Service (Ceilometer)
	- Orchestration (Heat)
	- Bare Metal Provisioning Service (Ironic)
	- Container Orchestration Engine Provisioning (Magnum)
	- Computable object storage (Storlets)
	- Deploys OpenStack using OpenStack itself (Tripleo)
	- Billing and chargebacks (Cloudkitty)
	- Optimization Service (Watcher)
	- Distributed SDN controller (Dragonflow)
	- OpenStack Networking integration for containers (Kuryr)
	- NFV Orchestration (Tacker)
	- Networking Automation for Multi-Region Deployments (Tricircle)
	- Command-line interface for all OpenStack services (Openstackclient)
	- Instances High Availability Service (Masakari)
	- Lightweight OCI containers (LOCI)
	- EC2 API proxy (EC2API)
	- Official Python SDK for OpenStack APIs (Openstacksdk)
	- Block Storage (Cinder)
	- Image service (Glance)
	- Big Data Processing Framework Provisioning (Sahara)
	- Application Catalog (Murano)
	- Containers Service (Zun)
	- Puppet modules to deploy OpenStack (Puppet-openstack)
	- Clustering service (Senlin)
	- Event, Metadata Indexing Service (Panko)
	- Root Cause Analysis service (Vitrage)
	- Load balancer (Octavia)
	- Accelerators resource management (Cyborg)
	- Deploys OpenStack in containers using Helm (Openstack-helm)
	- OpenStack Storage integration for containers (Fuxi)
	- Client library for interacting with OpenStack clouds (Shade)
	- Database as a Service (Trove)
	- Shared filesystems (Manila)
	- DNS service (Designate)
	- Key management (Barbican)
	- Governance (Congress)
	- Software Development Lifecycle Automation (Solum)
	- Deploys OpenStack in containers using Ansible (Kolla-ansible)
	- Monitoring (Monasca)
	- Workflow service (Mistral)
	- Functions Service (Qinling)
	- RPM package specs to deploy OpenStack (RPM-packaging)
	- Messaging Service (Zaqar)
	- Ansible playbooks to deploy OpenStack (Openstack-ansible)
	- Benchmark service (Rally)
	- Application Data Protection as a Service (Karbor)
	- Backup, Restore, and Disaster Recovery (Freezer)
	- Packaging-rpm (Packaging-rpm)
	- Indexing and Search (Searchlight)
	- Deploys OpenStack in containers using Charms and Juju (Openstack-charms)
	- Resource reservation service (Blazar)
	- Alarming Service (Aodh)
	- Ansible playbooks using ironic (Bifrost)
	- Chef cookbooks to deploy OpenStack (Chef-openstack)
	- EC2 API compatibility layer for OpenStack (Ec2-api)
	- Python Software Development Kit (Python SDK)
	- Ansible playbooks and roles for deployment (OpenStackAnsible)
+ Silicon bring-up
+ Silicon characterisation
+ Massively parallel computing systems
+ Laravel
+ SQL working experience (Redshift/PostgreSQL/MySQL)
+ Experience working with a CI system is preferred (ex. TeamCity, Concourse, Jenkins, etc.)
+ Functional test automation tool experience is preferred (ex. Junit, TestNG, Serenity, etc.)
+ Experience with profiling tools like PerfView (CPU, Memory, Garbage collection)
+ Expert knowledge of debugging and crash dump analysis in Windbg
+ Experience wrangling very large datasets by writing and maintaining data processing pipelines with Hadoop, Spark, BigQuery, Redshift, or similar
+ Google Data Studio
+ The successful candidate would be strong in SQL, AWS, Snowflake, Databricks, and Python.
+ Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
+ Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
+ Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
+ [Plus] Familiarity with interactive data visualization using tools like D3.js
+ Experience with MPP databases, such as Snowflake, Redshift, BigQuery, Vertica, etc.
+ A fluidity with tools commonly used for data analysis such as Python (numpy, pandas, and scikit learn), R, and Spark (MLlib).
+ Experience with at least one prototyping tool (eg. Axure, Framer, Principle)
+ Proficiency in developing pixel perfect mockups using Sketch and/or Adobe Design tools.
+ Analyze all aspects of the Snowflake Query Engine and drive initiatives to understand what bottlenecks may exist and to improve them.  
+ Build integration code with many cutting-edge technologies and processes, including Python 3, Go, Presto, AWS, ML, NLP.
+ Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ Prior experience with infrastructure automation frameworks (Ansible, Terraform, Chef or Puppet, etc.)
+ ***Experience with one of the ML platforms: Python / scikit-learn, Spark, vowpal wabbit, etc***
+ Virtualization and containerization (Xen, LXC, cgroups, Docker, Kubernetes)
+ tech stack:
	- Well-versed in one or more of the following languages and functional programming in general: Scala, Java, Python, JavaScript
	- Expert in SQL and comfortable designing, writing and maintaining complex SQL based ETL.
	- Experience with building large-scale batch and real-time data pipelines; ETL design, implementation, and maintenance.
	- Experience with schema design and data modeling, and the analytical skills to QA data and identify gaps and inconsistencies.
+ tech stack:
	- Write complex data flows using SQL, frameworks (e.g., Scalding, Spark), and scripting languages (e.g., Python, R)
	- Use data visualization tools (e.g., Tableau, Zeppelin) to share ongoing insights.
	- Skilled with Figma (or Sketch) and prototyping tools such as Framer, Principle
	- You have a deep and nuanced understanding of statistics, especially involving class imbalance problems.
	- Scalding
	- Full Stack Development
	- Presto or Hive
	- Spark
+ Knowledge of source control tools (Git, CodeCommit, SVN, and TFS), build/release tools (Jenkins, CodeBuild, CodeDeploy, CodePipeline), and infrastructure as code tools (Terraform, CloudFormation)
+ Experience in working with large data sets and distributed computing tools (Hive, Redshift)
+ skill set:
	- B.S. or M.S. in Economics, Statistics, or a similar field and 1+ year work experience in data science or analytics, or Ph.D. in a quantitative social/behavioral science (e.g. Economics, Sociology, Psychology, Statistics, or a similar field)
	- Coursework in experimental design, causal inference, and/or econometrics
	- Experience running and analyzing behavioral experiments
	- Statistical intuition and knowledge of various hypothesis testing and regression approaches, e.g. hierarchical modeling, difference-in-differences
	- Familiarity with Python or similar scripting language
	- Experience communicating technical statistical concepts clearly, for example, teaching or consulting
	- Demonstrated ability working effectively with cross-functional teams
	- Experience using git and pushing to a codebase
	- Experience with software engineering projects or coursework
+ skill set:
	- B.S., M.S., or Ph.D. in a quantitative field
	- 4+ years work experience in an analytical or quantitative role as a Data Scientist
	- 2+ years experience working on product analytics in a two-sided marketplace
	- Extensive experience generating insights using statistical techniques (e.g. regression, hypothesis testing)
	- Demonstrated ability to clearly explain data results to cross-functional teams
	- Experience using a procedural programming language (e.g. Python, R) to manipulate, clean, and analyze data
	- Ability to exercise judgment and combine quantitative skills with intuition and common sense
	- Experience evangelizing best practices and process improvements on your team
	- Experience working with large data sets and distributed computing tools (e.g. Redshift, Presto)
	- Experience pushing code and navigating a complex codebase
	- Active Quora user with curiosity about the product
	- Deep experience with MySQL, NoSQL data stores like HBase or similar
	- Strong grasp of Configuration Management (Chef, Puppet, Ansible, Salt Stack)
- skills to develop:
	- Deep understanding of at least one popular server side MVC Framework (e.g Django, Rails, AngularJS etc).
	- Knowledge of backend storage systems like MySQL, HBase, Memcached, Redis, Kafka etc.
	- Experience working with open source technologies like Kafka, Hadoop, Hive, Presto, and Spark
	- Take end to end ownership of Machine Learning systems - from data pipelines and training, to realtime prediction engines.
	- General understanding of Machine Learning at the level of a semester-long ML class (college or multiple MOOCs)
+ skill set:
	- Deep knowledge of web technologies, e.g. HTML, CSS. Experience with LESS or SASS is a plus.
	- Deep knowledge of JavaScript frameworks, e.g. jQuery. Experience with pure Javascript is a plus.
	- Some knowledge of server-side languages and web frameworks. Experience with Python is a plus.
	- Experience debugging across multiple browsers. Experience with UI testing tools like Selenium or phantomJS is a plus.
	- Experience optimizing the speed and performance of websites.
	- Experience maintaining large and growing code bases in a fast-moving environment.
	- Interest in staying current with new and evolving web technologies.
+ skill set:
	- 7+ years of industry/academic experience in Machine Learning or related field
	- You will be expected to have a good understanding of a broad range of traditional supervised and unsupervised techniques (e.g. logistic regression, SVMs, GBDTs, Random Forests, k-means and other clustering techniques, matrix factorization, LDA . . .) as well as be up to date with latest ML advances (e.g. Deep Neural Networks, or non-parametric Bayesian methods).
	- Previous experience building end to end scalable Machine Learning systems
	- Software engineering skills. Knowledge of Python and C++ is a plus.
	- Knowledge of existing open source frameworks such as scikit-learn, Torch, Caffe, or Theano is a plus
	- BS, MS, or PhD in Computer Science, Engineering, Statistics or a related technical field
	- Love of the Quora product
+ skill set:
	- BS, MS or PhD in Computer Science, Machine Learning, NLP or a related technical field
	- 5+ years of industry experience preferred
	- Good mathematical understanding of popular NLP and Machine Learning algorithms
	- Experience building production-ready NLP or information retrieval systems
	- Hands-on experience with NLP tools, libraries and corpora (e.g. NLTK, Stanford CoreNLP, Wikipedia corpus, etc)
	- Knowledge of Python or C++, or the ability to learn them quickly
	- Love of the Quora product
+ ***Experience building shallow or deep learning models (GBDT, CNN, RNN, LSTM), toolkits e.g. OpenCV, Matlab, RStudio, Weka, MLLib and frameworks PyTorch, TensorFlow, CNTK***
+ ***Expertise in multivariate analysis, graphical models, Bayesian hierarchical modelling, Markov chain Monte Carlo (MCMC), mixture models, stochastic processes, generalized linear models (GLMs), dimensionality reduction (PCA/CCA/MDS/tSNE) and other machine learning techniques***
+ Experience working with Atlassian products (JIRA, Confluence)
+ Knowledge of Internet protocols (e.g., TCP/IP, BGP, OSPF, TACACS, IPSEC, SNMP, SYSLOG)
+ Speech (NLP: ASR, MT, NLP, NLU, TTS, DM, and ASP)
+ Experience with SQL and Statistical/mathematical programming software packages (R, SPSS, CPLEX, LONDO or Xpress etc)
+ ***Programming skills sufficient to extract, transform, and clean large (multi-TB) data sets in a Unix/Linux environment.***
+ Experience with NLP libraries such as SpaCy, Stanford CoreNLP, OpenNLP, or NLTK
+ ***Experience with big data techniques (such as Hadoop, MapReduce, Hive, Pig, Spark)***
+ ***Familiar with one or more machine learning, statistical modeling tools such as R, Matlab, scikit learn and deep learning frameworks, such as tensorflow, keras, caffe, torch.***
+ skill set for data science:
	- ***Technical mastery in one or more of the following languages/tools to wrangle and understand data: Python (NumPy, SciPy, scikit-learn), Spotfire, Tableau.***
	- ***Experience with Spark (MapReduce, PIG, HIVE)***
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
	- 5+ years of experience with R or Python and some knowledge of SQL and experience with other software environments e.g. SAS, Matlab, Spotfire, Tableau, Qlikview, SPSS, KNIME and/or other data mining tools. Experience with other software components for data preparation and integration e.g. Data Virtualization and Big Data tools such as Hadoop and Spark and/or further programming or scripting environments e.g. .Net, Java, IronPython, Javascript, C++ is a plus.
+ stress testing (locust.io)
+ designing high availability systems
+ application security hardening
+ distributed tracing (OpenTracing/Zipkin)
+ collecting and analyzing performance metrics (InfluxDB, Prometheus, statsd, Grafana)
+ Docker orchestration systems and cluster managers (Kubernetes, Mesos/Marathon, ECS)
+ Experience in the technologies we use is helpful but not required. They are: Go for core infrastructure; ObjC, Java and C# for native UI development on iOS, OSX, Android and Windows; Node.js and IcedCoffeeScript for Web development; FUSE for client file systems; MySQL/InnoDB, DynamoDB, S3 and EC2 for hosting.
+ Load testing frameworks/tools like JMeter, Gatling, Locust
+ Java, Selenium, JUnit, Cucumber-JVM
+ API Testing experience
+ BDD (Cucumber, Gherkin)
+ Experience implementing search solutions with technologies such as SOLR, Elasticsearch, Lucene is preferred.
+ Python, Gherkin, Cucumber, Espresso, XUI Test
+ Experience with testing technologies (JUnit, Espresso, Mockito, Robolectric)
+ Unit Testing Tools  –  Google Test or CPPUnit ; Code quality tools
+ Familiarity with Linux, Maven, Git/Stash, Jira, Bamboo/Jenkins
+ Experience with distributed messages systems ( Apache Kafka)
+ Experience in CFD combustion or other reacting-flow simulations.
+ tech stack
	- Jira, Confluence, DevOps, Continuous Integration and Continuous Delivery, Microsoft Development Tools
	- Git, MS Build, Team Foundation Server, Jenkins, Unit Testing, Powershell, Perl, C#, .NET, Visual Studio, Python
+ tech stack:
	- Excellent skills in creating high-fidelity prototypes using Invision, Principle, Code or similar
	- Relevant experience in agile methodologies (Scrum, Agile, etc) and PM tools (e.g. Jira, Pivotal Tracker, Confluence etc.)
	- Experience with relational (e.g. MySQL, PostgreSQL) and NoSQL (e.g. MongoDB, ElasticSearch) databases
+ tech stack:
	- Expertise in Go preferred, but not required. If you’re new to Go, then proficiency in a mainstream language such as Java, Python, C++, Scala, etc.., and a willingness to learn Go required.
	- You've got experience writing, deploying and monitoring microservices.
	- Working knowledge of SQL and relational databases(we use Postgres)
	- You've used an RPC framework like gRPC or Thrift.
	- You have high level experience working in a containerized infrastructure deployed in the cloud(AWS, GCP, Azure)
+ tech stack:
	- Experience with NoSQL databases. MongoDB is a plus
	- Experience with real-time and streaming data processing
	- Experience with queuing platforms like Kafka
	- Knowledge of BigQuery
	- Familiarity with GCP/AWS cloud services
	- Familiarity with TensorFlow
	- Comfortable with CI/CD Pipelines
	- Experience with Git version control
+ tech stack:
	- Ability to configure and maintain webservers (e.g. apache & nginx), DNS servers, Firewalls, LDAP servers, Tomcat servers
	- Ability to back up the Data infrastructure
	- Ability to manage/configure  Git, Maven and Jenkins
	- Managing QA/production release and deployment
	- Ability to Install/Configure/Manage VM servers using OpenStack
	- Ability to install configure or manage Monitoring servers using Opensource softwares
	- Experience with Amazon Web Services:
	- autoscaling, & use of Netflix Asgard
	- ELB management,
	- EBS storage management
	- S3
	- RDS
	- Manage configuration using Puppet
	- Familiar with Cloud Computing in genera
+ tech stack:
	- ReactJS
	- GraphQL
	- Apollo Client & Server
	- Some Redux
	- Using ES6/7 features throughout the app so knowledge on those is a plus.
	- We use Cypress for testing
	- CircleCI for continuous integration.
	- Functional programming principles in React with Recompose
+ Celery
+ Elasticsearch and ELK pipeline
+ LibreOffice, Apache OpenOffice, and NeoOffice.
+ Tech stack is described as:
	- Front­end: JavaScript (ES5/ES6), AJAX, jQuery, React/Angular/Vue, Bootstrap, templating, markdown/markup, built tools, task runners, PWAs, etc...
	- Middle­tier: REST and RESTful interfaces, AJAX, RPC, WebSockets/Socket.io, Web Workers, Node.js/Express, etc…
	- Back­end: SQL/No­SQL databases, Message Queue Systems, Big Data systems, Node.js, MongoDB, Redis, etc...
+ data Science:
	- Knowledge of ElasticSearch/Solr/Lucene is a big plus.
	- Understanding in Java server platform and system tuning is a plus.
	- Knowledge with vector space models, text classification and categorization.
	- Implement high-quality code in an agile software development environment.
+ data science skill set:
	- Implement scalable algorithms and services using technologies such as Scala, Akka, elasticsearch, Kafka, Cassandra and Hadoop technologies such as Hive, Spark or MapReduce
	- Hands-on experience in analyzing large datasets (e.g. with SQL, Python, R, Hive, etc.)
	- Some knowledge and experience in working with technologies such as Kafka, Cassandra, Elasticsearch, Akka, Kubernetes, etc.
	- Experience in Scala or Java is a plus
	- You are fluent in English; German skills are a plus
+ AWS cloud services: EC2, EMR, RDS, Lambda, Redshift
+ NoSQL databases, such as HBase, Cassandra, MongoDB, or DynamoDB
+ messaging systems, such as AWS SQS, AWS Kinesis, Kafka, or RabbitMQ, ZeroMQ
+ big data tools and stream-processing systems: Hadoop, Spark, Storm, Spark-Streaming
+ **Expertise and experience in Revit, Dynamo and/or other Revit scripting languages**... Strong background in computational design and design analysis... Fluency in a technical programming language (python, javascript, C#) is highly desired.
+ Understanding of standard networking protocols and components such as: TCP/IP, HTTP, DNS, ICMP, the OSI Model, Subnetting, and Load Balancing
+ Knowledge of routing protocols such as BGP and OSPF
+ data pipeline and workflow management tools: Azkaban, Luigi, Airflow
+ Very well versed with ADT, ORU, ORM and document exchange messages specification
+ Develop public APIs on either APIGEE
+ DBT experience
+ Object oriented programming experience (e.g. using Java, J2EE, EJB, .NET, WebSphere, etc.).
+ data interchange formats like JSON and XML
+ Knowledge in machine learning framework - Tensorflow, Caffe, Torch or Theano
+ Django, Ruby on Rails, Flask
+ Must have experience with working on few technologies such as spring framework, SpringBoot, SpringMVC, JPA, MyBatis, Tomcat, Nginx
+ Experience with performance optimization of queries in Redshift & Postgres
+ Knowledge of authentication protocols such as basic and digest authentication, SAML, LDAP, and OAuth.
+ In-Memory caching technologies, such as memcached or Redis
+ Cutting edge C++ knowledge (C++17, C++20)
+ stream pipelines and all sorts of data stores (SQL, NoSQL, triplestores, wide column, graph)
+ Knowledge of data standards, file formats, and biomedical ontologies and vocabularies such as SNOMED-CT, UMLS, etc. DICOM
+ all types of data stores - NoSQL, wide column, Graph, triplestores
+ Spark, Kafka
+ Experience with stream pipelines and data store technologies (nosql, wide column and graph). We are Currently using Cassandra, Kafka, Amazon dynamoDB, Redis, Neo4j and Mysql.
+ NLP library: spaCy, NLTK, GATE, CoreNLP, gensim
+ Deep Learning applied to NLP, for example through distributed representations (e.g. Word2Vec, fastText, etc)
+ large databases (e.g. THIN)
+ Monitoring solutions experience (ELK, NewRelic)
+ Infrastructure-as-code and automation tools (e.g. Terraform, Ansible/Chef, Cloud Formation)
+ Configure and Monitor SIEMS and DLP systems
+ RxJava, Kotlin, Dagger
+ big data platform tools such as Hadoop, Hive, Druid, Kafka, Ambari, Spark
+ Experience with common security tools such as nmap, Burp Proxy, Brakeman, etc.
+ Experience with bug bounty programs and reporting issues to them (send examples, please!)
+ Familiarity with search domain (Information retrieval, NLP, Solr/ Lucene or related tech)
+ data management tools in on a big data plate form such as Atlas, Ranger , Knox
+ implementing BI solutions in a heavily regulated environment e.g. PII, GDPR, HIPPA & SOX
+ big data platform tools such as Hadoop, Hive Druid, Kafka, Ambari, Spark, Zeppelin
+ PowerBI, Tableau, Qlikview
+ Production experience with AWS tools including at least some of the following: EC2, S3, Kinesis, CloudFormation, Redshift
+ Experience with at least one data warehousing platform (Redshift, Athena, Hive, Snowflake, etc.)
+ Knowledge of a majority of the following: Elixir, Erlang, Ruby, JavaScript, PHP, Postgresql, MySQL, Apache Solr, Elasticsearch.
+ Knowledge of web frameworks (like Sinatra/Rails), testing frameworks (like Rspec/Minitest) and Javascript. Experience with Ruby, MySQL and Apache Solr is a plus.
+ Experience with Java, Boost, QML, Jira, JavaScript, React, or DDP
+ Demonstrated proficiency with Docker and container orchestration technologies (Kubernetes, ECS, etc.)
+ Expertise with AWS services such as EC2, IAM, S3, etc.
+ Expertise with several continuous integration technologies (Jenkins, Ansible, CloudFormation, Terraform, etc.)
+ Experience with load balancing technologies such as ELB, NGINX, etc.
+ Experience with network technologies like DNS, AWS security groups, VPCs, etc.
+ Extensive experience manipulating and analyzing complex data with SQL, Python and/or R. Knowledge of Google BigQuery and Java/Scala is a plus.
+ Tools: Slurm, Docker, Grafana.
+ skill set:
	- Bachelor’s degree in Computer Science, Math, Statistics, Economics, or other quantitative field; Masters or PhD strongly preferred
	- Significant experience in custom ETL design, implementation and maintenance, including serving machine learning models in production for multiple high-growth companies, preferably those with technical products
	- Track record of developing and evolving complex data environments and intelligence platforms for business users
	- Demonstrable ability to relate high-level business requirements to technical ETL and data infrastructure needs, including underlying data models and scripts
	- History of proactively identifying forward-looking data engineering strategies, utilizing cutting-edge technologies, and implementing at scale
	- Hands-on experience with schema design and dimensional data modeling
	- Understanding of statistical modeling, machine learning and data mining concepts
	- Demonstrable critical thinking and analytical skills, including the ability and confidence to make conclusions and recommendations from data
	- Experience interacting with key stakeholders in different fields, interpreting challenges and opportunities into actionable engineering strategies
	- Experience with Big Data/distributed frameworks such as Spark, Kubernetes, Hadoop, Hive, Presto,
	- Experience with job schedulers; Airflow, Luigi, Azkaban, etc.
	- Experience with continuous integration and automation tools and processes
	- Advanced SQL and relational database knowledge (MySQL, PostgreSQL) in addition to warehousing and dimension modeling
	- Scripting in Python required, experience with Scala/Go a plus
	- Programming against APIs required
	- Experience with Snowflake and/or Looker a plus
	- Effective communication and interpersonal skills
+ skill set:
	- Bachelor’s degree in Computer Science, Math, Statistics, Economics, or other quantitative field; Masters or PhD strongly preferred
	- Previous experience in data science or quantitative analytics role, preferably in a high-growth company
	- Comprehensive understanding of statistical modeling, machine learning and data mining concepts, and experience applying these methods within a business environment
	- Strong knowledge of Python. Familiarity with at least one statistical modeling / machine learning tool such as R or Matlab is a plus, as well as experience with languages such as Scala or Go
	- Expert knowledge of, and hands-on experience with, SQL
	- Demonstrable critical thinking and analytical skills, including the ability and confidence to make conclusions and recommendations from data
	- Experience interacting with key stakeholders in different fields, interpreting challenges and opportunities into actionable data-driven analysis and implementing science-driven data products
+ skill set:
	- Agile development approaches
		* Lean-startup and design-thinking inspired methods incorporating short product development, business-hypothesis-driven experimentation, iterative product releases, and validated learning
	- Programming languages and operating systems
		* Python
		* Javascript
		* Unix/Linux/MacOSX
		* RDF, JSON-LD, SPARQL
	- Tools
		* Git
		* Slack
		* JIRA
	- Web app development
		* Django
		* Django REST Framework (for API development)
		* Node
	- Backend
		* SOLR/Lucene or ElasticSearch
		* Amazon Web Services (AWS) or other cloud service providers
		* “Serverless” computing (e.g., AWS Lambda)
		* Application containerization (such as Docker or Kubernetes)
	- Javascript frameworks
		* Vue and/or Nuxt
		* Webpack
	- Mobile development
		* Using responsive design and development techniques, possibly including the use of Progressive Web Application (PWA) techniques and technologies
	- Other
		* Topic modeling (ideally using Mallet)
		* Video formats and metadata (for both archiving and streaming)
		* RDF, JSON-LD, Sparql and GraphQL for knowledge graph development and use
		* Content markup including:  HTML, XML, ePUB, PDF
		* Named Entity Recognition (NER)
		* Experience applying statistics, modeling, and machine learning
+ skill set:
	- SQL, javascript, and google appscript
	- Familiarity with HR systems and tools (e.g. Workday, Reflektive, Zugata, Culture Amp)
	- You are a skilled engineer. You've got a strong background working in our stack (Python, Django or React, ES6 or Node.js, etc.) -- even if you aren't a daily coder, you regularly exercise your coding muscles and try to be an asset on any technical context your team may need.
	- Help the team build tools for partners and developers that come to our platform to create Apps that live on Zapier. Imagine Postman meets OpenAPI on steroids.
+ tech stack:
	- Research publications at relevant conferences such as SIGGRAPH, ACM Trans on Graphics, CVPR, ICCV, ICCP, SPIE, JOSA a major plus.
	- Expertise in Deep Learning, Machine learning and familiarity with tools like Scipy, Boost, Caffe, TensorFlow, OpenCV, DLIB etc. and related areas.
+ Because compilers, interpreters, JIT, pre-processors, grammars, register allocation, term rewriting, LLVM and more are what brought us to computer science in the first place, Raincode Labs forms the largest independent compilation technology company in the world.
+ tech stack:
	- Publication record in top conferences (ICML, ICLR, NIPS, KDD, IJCAI, AAAI etc )
	- Good knowledge and handson experience in distributed technologies such as Hadoop, Hive, Spark Experience in Scala programming language.
	- Publications in relevant top venues (e.g., KDD, NIPS, ICML, AAAI, IJCAI, ICDM, ACL etc.)
	- You have publications in communities such as WWW, SIGIR, FAT*, NeurIPS, WSDM, SIGDIAL, RecSys, CHI, KDD, AAAI, ACL, ICML, or related.
	- You have hands-on experience implementing production machine learning systems at scale in Java, Scala, Python, or similar languages. Experience with XGBoost, TensorFlow is also a plus.
	- You preferably have experience with data pipeline tools like Apache Beam or even our open source API for it, Scio and cloud platforms like GCP or AWS.
	- Extensive experience manipulating and analysing complex data with SQL, Python and/or R. Knowledge of Google BigQuery and Java/Scala is a plus.
	- Familiarity with marketing tracking platforms (e.g. DoubleClick, Google Tag Manager, Google Analytics) preferred
	- Become an expert on leveraging existing state-of-the-art tooling into the Spotify eco-system (TensorFlow, TFX, Kubeflow Pipelines, Cloud Bigtable)
	- Contribute to new and existing Spotify open source machine learning and data processing products (scio, zoltar)
	- You preferably have experience with data processing and storage frameworks like Google Cloud Dataflow, Hadoop, Scalding, Spark, Storm, Cassandra, Kafka, etc.
	- Extensive publication record at peer-reviewed ML conferences (e.g. NIPS, ICML, AISTATS, UAI, COLT, ICLR, AAAI, etc) as well conferences with applied ML (e.g. KDD, WSDM, WWW, CIKM, RecSys, etc).
+ tech stack:
	- Security and privacy
	- Virtualization and container technologies (e.g., Xen and Docker)
	- Cloud services (e.g., AWS and Azure)
	- Distributed programming tools (e.g., Hadoop, Cassandra, and ZooKeeper)
	- In-home wireless network protocols (WiFi, Bluetooth, Zigbee, and Z-wave)
	- Systems for machine learning training and inference (Tensorflow, MXNet, Caffe etc)
	- Storage systems
+ Experience with open source platforms like Hadoop, Spark, Hive, Pig; and/or ML life-cycle/collaboration/automation platforms like AirFlow, FB Learner, MLFlow; and/or assistants like Alexa, a plus.
+ "Knowledge of Bayesian Global Optimization tools and technique"
+ Working with Big Data, ML, AI. Keras, TensorFlow, Python, Redshift, S3, Spark, Random Forests and Vowpal Wabbit
+ Experience implementing production-ready machine learning solutions is a plus
+ You’ll lead, analyze, implement, and socialize a robust A/B/multivariate testing program, collaborating closely with product, engineering, marketing, and content.
+ You’re familiar with all aspects of SEO: on-page, external, and technical, and you have used tools such as ahrefs, DeepCrawl, Screaming Frog, SEMRush, and Google Search Console to optimize for search.
+ Set of skills:
	- Experience with modern programming languages (Java, Scala, Go, TypeScript)
	- Database / Data Storage experience (SQL / MySQL, MongoDB, DynamoDB)
	- Interest in Infrastructure Tooling (Docker, Nomad, Consul, Vault, Prometheus)
+ RStudio packages: The tidyverse, R Markdown, and Shiny
+ A sample of the technologies you’ll be exposed to: Python, Javascript/Angular, Impala (Big data data database), AWS, Docker, Kubernetes, Git.
+ Experience with Python ORMs like SQLAlchemy and Python libraries like Pandas, Scikit-Learn, Numpy and Scipy
+ Should have experience in dealing with XML and JSON data formats.
+ Knowledge in Hadoop (HDFS, YARN), its programming models (MapReduce, Spark), and its services such as Hive, HBase etc.
+ Technical Fluency.  Languages and tools such as Python/Java/Scala, AWS (S3/EMR/Athena/Glue) and SQL. Experience with big data processing tools including Spark, Hadoop, Hive, Yarn, and Airflow. Experience working with either a MapReduce system of any size/scale.
+ Experience writing production datasets in SQL/Hive OR building internal/production data tools for ETL, experimentation, or exploration in a scripting language (Python, R, etc.)
+ Very strong experience in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in the OLTP, OLAP and Data Warehouse environments
+ Passionate about various technologies including but not limited to SQL/No SQL/MPP databases etc.
+ Hands-on experience with Big Data technologies (e.g Hadoop, Hive, Spark)
+ Ansible: it’s not that bad, and helps us move quickly, but any configuration management tool is applicable.
+ Elasticsearch / Kibana: You can readily access information & love metrics
+ Familiarity with common web application testing tools for DAST, SAST, and IAST analysis such as Burp Suite, Checkmarx, Veracode
+ Completed graduate-level coursework in survey statistics—bonus points if you've completed coursework in adjacent fields/methods (e.g., econometrics, NLP, experimental design, political science, or quantitative social psychology)
+ Exposure to container technologies - container orchestrators (Kubernetes, Mesos, Docker Swarm Mode) is a plus
+ Experience with Cloud based services, Microservices a Cloud Computing class or similar experience
+ Technical Skills needed: vSphere, vSAN, NSX, vROps, Storage, Database, Middleware, and Scripting
+ Experience of Unity, C# and 3D application development.
+ Working knowledge of HMD (ie Oculus, HTC Vive, Hololens)
+ Experience with Hololens, HTC Vive, Oculus, Google Cardboard and other leading AR/VR platforms
+ Knowledge of NoSQL technologies (e.g. Cassandra, MongoDB, Redis, etc.) and/or search-based data stores and libraries (Lucene, Solr, etc.)
+ Experience within the domain of Advanced Analytics and Data Science is highly desirable, e.g. hands-on experience with solutions such as Spark, MapReduce, Python, Redshift, Hive, Pig and visualization tools.
+ Hadoop data platform is capable of supporting a growing list of downstream platforms like Tableau, Zeppelin etc.
+ Expertise with Hive, YARN, Spark, Presto, Kafka, SOLR, Oozie, Sentry, Encryption, Hbase, etc.
+ API development
+ You highly experienced with JavaScript/Node.js, SQL/NoSQL databases
+ We are fans of the Lean Startup methodology, we love Trello, Jira, Slack
+ We are cloud agnostic and run our infrastructure and systems on Azure, AWS, as well as dedicated servers.
+ Experience utilising Portfolio & Project Management (PPM) tools such as CA PPM (Clarity), ServiceNow, JIRA, Microsoft Project Server, etc.
+ project management tools (JIRA, Confluence),
+ big data platform tools such as Hadoop, Hive, Druid, Kafka, Ambari, Spark
+ web analytics platforms such as Google Analytics, Appsflyer or Mixpanel
+ NoSQL databases, such as MongoDB, Cassandra, HBase
+ Proficiency in using query languages such as SQL on a big data platform e.g. Hadoop, Hive
+ data visualisation tools, such as D3.js, GGplot, Tableau etc.
+ Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.
+ Apache Kafka
+ vw / xgboost
+ Knowledges of Web test frameworks like Selenium, React.js, Headless Chromium is a plus
+ set of skills:
	- Statistical analysis and modeling
	- Database architectures
	- Hadoop-based technologies (e.g. MapReduce, Hive and Pig)
	- SQL-based technologies (e.g. PostgreSQL and MySQL)
	- NoSQL technologies (e.g. Cassandra and MongoDB)
	- Data modeling tools (e.g. ERWin, Enterprise Architect and Visio)
	- Python, C/C++ Java, Perl
	- MatLab, SAS, R
	- Data warehousing solutions
	- Predictive modeling, NLP and text analysis
	- Machine learning
	- Data mining
	- UNIX, Linux, Solaris and MS Windows
	- Python (3.5>=), packages: argparse, shapely, Munkres, numpy, cv2, logging, Pillow
* ES6
* Plotly.js
* OpenLayers
+ UI/UX:
	- Experience using design tools such as Photoshop, Illustrator, Sketch, InDesign, etc. for creating highly-detailed mockups
	- Some awareness of the technology which will serve your designs and implementations, such as apache/nginx, Flask/django, PostgreSQL/MySQL, git, websockets, etc.
	- Bootstrap, bulma, etc.
+ Areas of interest:
	- Distributed and parallel systems
	- Information retrieval
	- Large software systems
	- Web application development
	- Database management
	- Cloud computing
	- Cloud security
	- DevOps
+ technologies/frameworks:
	- ReactJS
	- Java/Scala
	- Spark
	- Ruby/JRuby
	- ElasticSearch
	- MySQL
	- Kubernetes
	- Amazon Web Services
	- MS Azure
	- Google Cloud Platform
+ Knowledge of NoSQL technologies (e.g. Cassandra, MongoDB, Redis, etc.) and/or search-based data stores and libraries (Lucene, Solr, etc.
+ Experience with Cloud based services, Microservices a Cloud Computing class or similar experience
+ Produce high quality and well-documented code in an automated CI/CD environment
+ Collaborate with engineering and product teams to design, develop, and publish software supporting a highly available, fault-tolerant SaaS platform
+ skill set:
	- Expertise in Golang and proficiency in other languages (Preferably C/C++,NodeJs, Python).
	- Commercial experience with REST, RPC and message exchange protocols.
	- Experience with frameworks such as: Gin, Gorilla, Dep, Ginkgo
	- Knowledge around message queuing and distributed tasking (SMS,ZeroQ, RabbitMQ etc)
	- Working knowledge in Kubernetes, Rancher or Docker swarm.
	- Ability to write clean and effective Godoc comments
+ Preferred Skills: Tensorflow, Slurm, Kubernetes
+ data science skills:
	- Excellent understanding of ML, NLP, and statistical methodologies
	- Excellent programming skills (Java/Python/R/Sas)
	- Ability to test ideas and adapt methods quickly end to end from data extraction to implementation and validation
	- Experience with search engines, classification algorithms, recommendation systems, and relevance evaluation methodologies a plus
	- Specific Big Data experience on cloud computing platforms with technologies such as Hadoop, Mahout, Pig, Hive and Spark a plus
	- 7+ years of experience with Data Science and Statistics, preferably in Life Sciences, and more specifically, in pharmaceuticals.
	- The ability to tell a story about data, in particular with visualization.
	- Solid understanding of statistics and the design and analysis of experiments. Solid skills in statistical language, SAS.
	- Provides automated and ad-hoc analysis of experiments.
	- Assesses and validates reliability of source data and business systems used to develop performance metrics.
	- Prepares recommendations and conclusions based on data summaries and communicates this information in a credible, convincing and timely manner.
	- Explores existing data for insights and recommends additional sources of data for improvements.
	- Guide the architecture of “big-data” business processes with an eye towards robustness, parsimony and reproducibility (at senior levels)
	- Define and develop software for the analysis and manipulation of large and very large data-sets
	- Narrate stories (sometimes to a non-technical audience) about our content and processes by data analysis and visualization
	- Collaborate with scientists, product groups and content groups to perform “big data” aggregations, fusion and manipulations of important data-sets
	- This is a thought leader.
	- Define, manipulate, aggregate and use both structured and unstructured "big data" in order to support descriptive and predictive analytics across the businesses.
	- Adept at all aspects of technical communications, including using presentations technologies (e.g. WebEx, PowerPoint) and software demonstrations.
	- Data Collections: Expertise in large data collection and processing including ETL, workflow and delivery of data
	- Business Intelligence (BI) tool like Qlik or Tableau
+ data science skill set:
	- Advanced knowledge of ElasticSearch/Solr/Lucene.
	- Advanced knowledge of backend paradigms
	- Knowledge with vector space models, text classification and categorization
	- Implement high quality code in an agile software development environment.
	- Able to respond and present work to peers, answer in-depth questions, accept constructive feedback, and modify product accordingly.
+ Testing and directly mitigating against common application security issues such as the [OWASP Top 10](https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project).
+ Experience in a UGC (user generated content) environment
+ Design services for performant application of machine-learned models
+ Polyglot developer (e.g. Java, NodeJS, Python)
+ Experience with any one of segmentation, object detection, image classification, GANs, monocular depth estimation or a related field
+ Successful record of publication in top-tier international research venues (e.g. ICLR, AAAI, NeurIPS, CVPR, ECCV, ICCV, SIGGRAPH)
+ Very strong programming skill in C++14. You will be expected to know and use C++ templates, lambdas, and high-performance data structures.
+ Research and develop CNN/RNN neural network compression algorithms, focusing on quantization and pruning
+ embedded deep learning:
	- Analyze, test and improve neural network compression algorithms
	- Experience with at least one deep learning algorithm, such as CNN/LSTM/GRU
	- Experience with at least one deep learning framework, such as Tensorflow, Pytorch, Caffe, Kaldi
+ Demonstrated expertise with C++ with at least one of std::thread / OpenCL / CUDA
+ You know your way around SQL-like databases (e.g. PostGres, Impala, Hive) and even better if have experience with Spark and other big data platforms.
+ ***Strong publication record in top-tier research publications and conferences such as IJCV, CVPR, ICCV, ECCV, ICRA.***
+ https://www.openrobotics.org/interns
+ machine learning:
	- Develop backend services and infrastructure to expand our answer engine to support 10M+ documents and 100K+ QPS
	- Ship web applications and APIs using Python, Flask, MongoDB, MySQL, Lucene, Spark, React, Go, and/or TensorFlow
	- Optimize the performance of our indexing, processing, and query pipelines
	- Take product ideas from ideation to implementation
	- Implement state-of-the-art algorithms in Question Answering, Machine Reading Comprehension, Text Summarization, in a scalable, production-ready fashion using Tensorflow and Spark
	- Build systems to evaluate and tune performance of a real world deep learning system, from data collection to processing to model implementation to post-processing and visualization
	- modern Big Data stack (Spark or Hadoop, Kafka or RabbitMQ, ZooKeeper, Redis, Memcache, Lucene, MongoDB, MySQL)
	- Familiarity with containerization and dev-ops (Docker, Kubernetes, Docker Swarms, Jenkins, Phabricator, Continuous Integration, Continuous Delivery) is a plus
	- Familiarity with modern Deep Learning and Natural Language Processing / Natural Language Understanding (NLP, NLU), including Neural Networks, RNNs, seq2seq models, and real world machine learning in TensorFlow (incl. regularization, cross-validation, dropout) are a huge plus
	- Adaptable, humble, and interested in pushing the boundaries of what's possible
	- Work with world class talent (our team consists of former Facebook, Palantir, Dropbox, and LinkedIn Engineers; we have 2 ACM ICPC World Finalists)
+ data science:
	- Experience in modern Deep Learning and Natural Language Processing / Natural Language Understanding (NLP, NLU), including Neural Networks, RNNs, seq2seq+attention models, and real world machine learning in TensorFlow (incl. regularization, cross-validation, dropout)
	- Experience building production-ready NLP systems
	- Familiarity with non-standard machine intelligence models (Reinforcement Learning, Hierarchical Temporal Memory, Capsule Networks) is a plus
	- Familiarity with Distributed systems (Docker, Kubernetes, Kafka, Spark, Redis, AWS S3/EC2/RDS/KMS, MongoDB, or Lucene) is a plus
	- Adaptable, humble, and interested in pushing the boundaries of what's possible
	- Proficiency in Python, R, or Java
+ Installation, setup and calibration of MOCAP system, binocular cameras
+ Unigraphics or Pro-engineer fluency.
+ robotics engineering:
	- Background in multiple of the following: SLAM, mapping, localization, calibration, sensor fusion, tracking, scene understanding, robotic systems.
	- Experience in multiple of the following: non-linear optimization, 3D geometry, state estimation.
	- Experience with advanced algorithms, data structures and working with real sensor data.
	- Experience with Python and developing in the Linux and/or Mac OS environment.
	- Familiarity with real-time, multi-process and multi-threaded coding.
	- Strong C++ development skills.
	- Be an essential part of a team of engineers and scientists developing state-of-the-art estimation algorithms used for a variety of tasks, including calibration, localization and tracking.
+ Perform analysis of security incidents & threat actors for further enhancement of Detection Catalog and Hunt missions by leveraging the MITRE ATT&CK framework.
+ Experience with information security tools such as an enterprise SIEM solution (Splunk preferred), IDS/IPS, endpoint security, EDR and security monitoring solutions (NSM,DLP,Insider, etc).
+ Scikit-learn, Keras/Tensorflow, Spark MLlib, Spacy or other ML libraries
+ tech stack:
	- Docker (Kubernetes)
	- Spark (on Hadoop)
	- Kafka
	- Cassandra (or other NoSQL DBs)
	- AWS and some of its services
	- Azure and some of its services
+ Interesting projects with technologies like Scala, Java, Groovy, Akka, SpringBoot, Cassandra, MongoDB, Apache Kafka, JavaScript, TypeScript, React, Angular.
+ tech stack:
	- Python (numpy, pandas, sklearn, xgboost, TensorFlow)
	- MySQL, Hive
	- Java
	- Google Cloud Platform
	- Tableau, Looker
+ tech stack:
	- Python (numpy, pandas, scikit-learn, tensorflow, keras)
	- Google Cloud Platform
	- Machine Learning (e.g. regression, ensemble methods, deep learning, etc.)
	- Statistics (Bayesian methods, experimental design, causal inference)
	- Tableau, Looker
	- Snowflake (SQL)
+ skill set:
	- Familiarity with containerization technologies (docker, lxc, rkt, etc.).
	- Familiarity with container orchestration technologies (Kubernetes, Marathon, etc.).
	- Experience working with cloud computing services providers (AWS, Google cloud platform, Azure, etc.).
	- Experience with Elasticsearch /Apache Solr and Logstash
	- Experience working with Real-time messaging and NoSQL infrastructures: Redis, RabbitMQ, Celery, Kafka, etc.
	- Scalable data processing techniques: Kafka, Spark, ElasticSearch, Celery
	- Real-time messaging and NoSQL infrastructures: Redis, RabbitMQ
	- Have proven experience with ORMs (e.g. Django) and RDBMS (e.g. MySQL) including development of complex SQL queries.
+ skill set:
	- A fluidity with tools commonly used for data analysis such as Python (numpy, pandas, and scikit learn), R, and Spark (MLlib).
	- Experience with MPP databases, such as Snowflake, Redshift, BigQuery, Vertica, etc.
	- Familiarity with data visualization tools/frameworks as well as notebooks.
	- Experience with time-series forecasting.
	- Experience building and deploying production-grade models in a real-time setting.
	- Expert-level abilities building and deploying unsupervised, semi-supervised, and supervised models on large-scale data (in that order of importance).
	- A degree of comfort at the command line. That means a thorough understanding of basic file-system commands, as well as the ability to ssh into remote machines and troubleshoot without a GUI, grep through logs, and deploy scripts and applications.
+ skill set:
	- Exceptional SQL skills and experience working with granular web clickstream data and behavior tracking tools like SiteCatalyst
	- Fluency in data analysis, including defining KPIs, statistical and predictive modeling concepts, descriptive statistics, experimental design and multivariate A/B testing
+ skill set:
	- Hadoop, HDFS, Hive, HBase, MapReduce, and Mahout.
	- Large-scale graph algorithms, clustering, page-rank, and community detection.
	- Apache Spark, SparkSQL, MLlib, and Scala Actors.
	- Ensemble Methods, Deep Learning, and other trendy topics in the Machine Learning community.
+ skill set:
	- Extensive hands on experience with administering some of the following: HBase, Impala, Spark, EMR, Hive on Tez or Presto
	- Experience operating large scale Hadoop clusters running Cloudera distribution
	- Operational mindset with ability to do Problem, SLA and Incident Management
	- Experience installing and managing Kafka is good to have
	- Experience managing infrastructure in AWS using EMR
	- Experience with SQL and Python(Boto3 Library)
	- Experience with AWS products including EC2, S3, RDS, ElastiCache, ElasticSearch, Kinesis, Lambda, etc
	- Exposure to Big Data on AWS (DataPipeline, Batch, AWS Glue, S3, EMR/EC2)
	- Hands on expertise in AWS (S3, EMR, EC2, Hadoop, Hive, Spark, Kafka, Storm, Druid, Cassandra, Columnar Databases and Graph Databases like DSE Graph is huge plus.
+ skill set:
	- Efficient in SQL, Hive, SparkSQL, etc.
	- Serve as technical “go to” person for our core technologies – Hadoop, Spark, AWS, Vertica, Tableau, Cassandra, Graph Databases and others
	- Advanced SQL skills to perform data segmentation and aggregation from scratch; experience working with granular web clickstream data a plus!
	- Knowledge of programming languages and stats packages (e.g. python, R); comfortable running multiple regression analyses
+ Strong hands-on experience with at least one of the main stream deep learning frameworks such TensorFlow, PyTorch, BLVC Caffe, Theano
+ skill set:
	- Expert knowledge of either Python or R, strong experience with database management systems like SQL, preferably also Spark ML, Scala, Hive and Impala
	- Confidence and comfort working on projects and goals that are happening in a hypothesis driven environment (build – measure – learn mindset)
	- An excellent understanding of SQL.
	- Experience with big data technologies (Hive, Impala, Spark, etc.) would be a plus.
+ Automated testing: Karma, Protractor
+ skill set:
	- Proxmox, KVM virtualization, LXC and Docker containers
	- large scale object storage (Ceph, cloud-based object storages)
	- Puppet
	- PostgreSQL
	- Distributed architecture (RabbitMQ, Kafka)
	- Icinga/Prometheus/ELK monitoring
+ Knowledge of container (docker or others) and orchestration (K8S or others)
technology is a plus+ At least 2 years of experience using deep learning techniques (CNN, RNN, LSTM) on computer vision tasks (object detection and tracking, classification, action recognition)
+ skill set:
	- 2+ years of cloud experience using AWS (e.g., EC2, ECS, Batch, Lambda)
	- Familiarity with Continuous Integration tools (e.g., Jenkins, Travis)
+ Knowledge of distributed machine learning framework (distributedTensorFlow/MXNET) or cloud/edge federation is a plus
+ At least 2 years of experience using deep learning techniques (CNN, RNN, LSTM) on computer vision tasks (object detection and tracking, classification, action recognition)
+ skill set:
	- 2+ years of cloud experience using AWS (e.g., EC2, ECS, Batch, Lambda)
	- Familiarity with Continuous Integration tools (e.g., Jenkins, Travis)
+ skill set:
	- Technologies : AWS Batch,  Spark, Hive, EMR, Presto, Docker, Jenkins, Bitbucket
	- Databases: RDS MySQL, Redshift
	- Machine Learning: Distributed TensorFlow, Keras, PyTorch, Caffe2, scikit-learn, Apache MxNet, SageMaker
	- Developing DAOs (data access objects) and APIs
	- Extensive practical experience using a wide range of AWS technologies, including: S3, EC2s, Lambda, Step Functions, Glue, EMR, API Gateway
+ Experience with CoreAudio, Audio Unit, ASIO and VST APIs.
+ Familiarity with automated build systems such as Jenkins or buildbot.
+ Experience with distributed analytic processing technologies (Hive, Pig, Presto, Spark)
+ skill set:
	- Linux
	- Puppet
	- nginx
	- AWS
	- Datadog
	- Cloudformation
+ skill set:
	- API rest
	- Ruby on Rails
	- CDNs
	- Microservices
	- Python
	- MySQL
	- Bento4
	- FFmpeg
	- Docker
	- We are looking for students or graduates with strong knowledge of objected-oriented programming, SQL and NoSQL.
+ skill set:
	- Experience with advanced ML models and concepts: HMMs, CRFs, MRFs, deep learning, regularization etc.
	- Experience with distributed databases such as HBase, Redis, CouchBase etc.
	- Experience in search platform such as Solr, Elastic Search
+ Cross platform C++17, OpenCL, Qt/QML, git, Python 3, clang-tidy, clang-format, Jenkins, CMake, Catch2, Docker, Vagrant, KVM, C++/CLI.
+ Work with cutting edge technologies such as C++17 (anything that passes VS2017, latest Clang, GCC, clang-tidy, ...). C++/CLI, GPGPU, Python, Jenkins, docker, KVM, libclang, boost, Qt and CMake
+ skill set:
	- Java, mahout, Hadoop
	- Scala, Spark, SparkML
	- Spark with Python, numpy and pandas
+ skill set:
	- As part of the Advanced Product Development team, immediate responsibilities include:
		* Exploration and development of machine learning algorithms for spatiotemporal analysis, including multiclass classification, clustering, temporal segmentation, sequence labeling, and spatial segmentation.
		* Development of new technologies and digital products to improve surgeon and team performance on robotic surgery platforms.
		* Support clinical and academic collaborations in related fields.
	- Additional responsibilities include:
		* Contributing to multiple areas of research, including but not limited to the following:
		* Designing and applying machine learning algorithms to novel, surgical applications
		* Characterizing surgeon and team behavior and workflow to optimize new technologies
		* Fully integrating machine learning into core digital products and intelligent systems
		* Conducting user studies to evaluate digital product concepts
	- Establishing strong academic collaborations across research disciplines
	- Presenting research at international conferences and publishing research in top academic journals
	- Qualifications... Skill/Job Requirements:
		* Competency Requirements: (Competency is based on: education, training, skills and experience).
		* In order to adequately perform the responsibilities of this position the individual must have:
			+ Doctoral degree in Computer Science, Statistics, Applied Mathematics, or Neuroscience, or Master's degree with minimum (5) years industry experience developing machine learning applications
			+ Demonstrate excellent communication skills both written and verbal
			+ Interested in early research and development through to product roll-out
			+ Solid understanding of statistics, machine learning, and deep learning algorithms and techniques is required
			+ Experience with sequence modeling, image analysis, activity recognition, and/or temporal segmentation on real-world data is required
			+ Experience with Python is required
			+ Hands-on experience with deep learning frameworks such as Tensorflow, Theano, Caffe, and/or Torch is required
			+ Hands-on experience with CNNs, RNNs, and LSTMs is ideal
			+ Experience with R, SQL is ideal
			+ Experience with C/C++ is ideal
			+ Experience with clinical studies is a plus
			+ Ability to travel domestically and internationally (10%)
+ skill set:
	- Intuitive Surgical designs and manufactures state-of-the-art robot-assisted systems for use in minimally-invasive surgery. These systems are revolutionizing the way in which surgery is being done and offer a unique platform that is being used routinely at hospitals worldwide for exploring the potential of digital surgery. Joining Intuitive Surgical means joining a team dedicated to using technology to benefit patients by improving surgical efficacy and decreasing surgical invasiveness, with patient safety as our highest priority.
	- The Applied Research group within Intuitive Surgical has an immediate opening in Sunnyvale, CA for a research scientist with focus on Computer Vision, Deep Learning and Image Analytics, contributing to new technology development in the area of 3D scene understanding/reconstruction and spatial AI systems for next-generation robot-assisted surgery platforms. This role is an exciting opportunity to join a newly formed team and contribute to its growth and it will give you an opportunity to test your knowledge in a challenging problem solving environment.
	- Research, design and implement algorithms in deep learning for computer vision and image analytics
	- Contribute to research projects that develop a variety of algorithms and systems in computer vision, image and video analysis.
	- Advance the state-of-the-art in the field, including generating patents and publications
	- Develop prototypes of 3D recognition models that scale to large clinical datasets
	- Develop prototypes of dense 3D reconstruction systems based on multi-view image sensors
	- Contribute to building new clinical datasets and data pipelines
	- Participate in integration of new ML/CV algorithms into existing and future robotic platforms
	- Experiment with several users and clinical advisors to iterate prototype designs based on feedback and performance.
	- Develop new technologies and digital products to improve surgeon and team performance on robotic surgery platforms.
	- Support academic collaborations in related fields.
	- Contribute to multiple areas of research, including but not limited to the following:
		* Design and apply CV/ML algorithms to novel, surgical applications

		* Design/bring-up of novel sensing technologies

		* Characterize surgeon and team behavior and workflow to optimize new technologies
	- Establish strong academic collaborations across research disciplines
	- Doctoral degree in computer science, electrical and computer engineering, or Master's degree with minimum (5) years industry experience developing computer vision and machine learning applications
	- Strong understanding of machine learning: you should be familiar with the process (data collection, training, evaluation, and making iterative improvements) of building effective learning systems.
	- Strong hands-on experience with at least one of the main stream deep learning frameworks such TensorFlow, PyTorch, BLVC Caffe, Theano
	- Strong hands-on experience with Python (proficiency), C/C++ (proficiency), Shell Script, Matlab
	- Strong engineering practices, debugging/profiling skills, familiarity with multi- threaded programming.
	- Train machine learning and deep learning models on a computing cluster to perform visual recognition tasks, such as segmentation and detection
	- Hands-on experience with GPU accelerated algorithms and implementations
	- Hands-on experience with state-of-the-art models based on CNNs, RNNs, and LSTMs
	- Excellent communication skills both written and verbal
	- Interested in early phases of product exploration and iteration based on incomplete requirements.
	- Solid understanding of computer vision, machine learning, and deep learning algorithms and techniques is required
	- Experience with visualization tools is a plus
	- Self-starter and able to work in a collaborative and results-oriented environment
	- Ability to travel domestically and internationally (5-15%)
	- Able to view live and recorded surgical procedures
+ Experience with CUDA, VTK/ITK, and/or physics based simulation
+ skill set:
	- Experience developing virtual world environment for games, such as Lumberyard, Unity3d, Unreal, CryEngine and/or other world simulation environments
	- Experience with graphics APIs and frameworks such as OpenGL, DirectX, or Vulkan
	- Experience with physics engines (e.g. Bullet, Havok, PhysX)
	- Experience developing agent behaviors, physics, gameplay, tools, or GUIs
+ skill set:
	- Google Dialogflow
	- The Visual Fusion Engine, VFE
	- algolia - Search Made Powerful
	- Cruzr - Humanoid service robot
	- Descartes Labs
	- Blue River Technology - Smart Agricultural Machines
	- NLPBOTS - Intelligent Chat-bots
	- BrainShop - AI for developers
	- Flyr
	- Sisense
	- Mookkie
	- NanoNets
	- Workfusion - Rpa Express
	- MSG.AI
	- Keepers - Advanced Child Monitoring
	- Neura - User Awareness with AI
+ Distributed Deep Learning
+ Automate and integrate security into CI/CD pipelines, such as static code analysis and dynamic code analysis.
+ Enjoys working and deploying technologies such as Chef, AWS, Ruby, Rails, MySql, and Redis
+ Docker, Heroku, Kubernetes
+ Experience with monitoring (NewRelic, Prometheus, PagerDuty)
+ Authentication: CAS (Java)
+ skill set:
	- Experience in at least one of the following sequencing methodologies: cell-free DNA sequencing, methylation sequencing, single cell sequencing
	- Deep understanding of next generation sequencing methods, platform-specific bias and errors, and data interpretation
	- Hands-on experience with large-scale genomic data (e.g. whole genome sequencing, exome-seq, target enrichment, amplicon sequencing, RNA-seq, etc.)Hands-on experience with genome alignment, mapping, variant calling, and annotation tools
	- Familiarity with human and mammalian genomics, standard genomic databases (UCSC, Ensembl, NCBI), formats, and tools.
	- Proficiency in python, ruby, Perl, or another high-level scripting language (Python highly preferred)
	- Familiarity with UNIX-based operating systems and shell scripting
	- Excellent data analysis and visualization skills (Tableau, R, matplotlib, or similar)
	- Excellent understanding of molecular biology and cell biology
	- Experience with Seven Bridges, DNA Nexus or similar platform
+ Experience with applied ontology development (RDF(S)/OWL, SPARQL, the Semantic Web or Frame based KR systems).
+ Good understanding of high-throughput single cell assays such as single cell transcriptomics, single-cell epigenomics, FACS, and CyTOF.
+ Experience with relational (e.g. postgres, SQL) and/or distributed (e.g. MongoDB) databases
+ Experience with RNA-seq, ATAC-seq and/or ChIP-seq experiments and data at the single cell level.
+ skill set:
	- Developing scalable bioinformatics algorithms, pipelines and tools to solve complex, distributed computing challenges
	- Providing statistics, scientific computing and data analysis support to research, product development and manufacturing operations
	- BS in bioinformatics or related field with 6-8 years industry experience or MS/PhD in bioinformatics or related field with 4-6 years industry experience
	- Advanced proficiency in Python, R or Rust in a Linux environment
	- Broad experience with NGS data analysis, manipulation and presentation (in Python or R)
	- Excellent data analysis and visualization skills (in Python, R and using tools like Tableau)
	- Excellent understanding of molecular biology, with an emphasis on DNA chemistry preferred
	- Proven ability to synthesize and present conclusions from complex analyses in a clear and visually compelling way
	- Familiarity with common tools and file formats for genome-scale interval data manipulation and analysis (e.g. bedtools, bedops, etc.)
	- Formal software development experience, the software development lifecycle, unit and functional testing (via py.test) and version control via Git
	- Familiarity with database (PostgreSQL, NoSQL and Redshift) design, implementation and efficient query construction
	- Experience building and using Docker containers
	- Familiarity with cloud-based computing environments, especially Amazon Web Services
	- Experience using workflow management tools (e.g. Airflow, Galaxy, Luigi)
	- Familiarity with web application development including building REST-ful interfaces
	- Familiarity with DNA analysis software (MacVector, VectorNTI, SnapGene)
+ skill set:
	- M.S. or Ph.D. degree in Genetics, Biology, Bioinformatics, Biostatistics, Computational Biology, Computer Science, or related field
	- Knowledge and experience in the field of cancer or rare disease genomics.
	- Experience in at least one of the following sequencing methodologies: cell-free DNA sequencing, methylation sequencing, single cell sequencing
	- Deep understanding of next generation sequencing methods, platform-specific bias and errors, and data interpretation
	- Hands-on experience with large-scale genomic data (e.g. whole genome sequencing, exome-seq, target enrichment, amplicon sequencing, RNA-seq, etc.)
	- Hands-on experience with genome alignment, mapping, variant calling, and annotation tools
	- Familiarity with human and mammalian genomics, standard genomic databases (UCSC, Ensembl, NCBI), formats, and tools.
	- Proficiency in python, ruby, Perl, or another high-level scripting language (Python highly preferred)
	- Familiarity with UNIX-based operating systems and shell scripting
	- Excellent data analysis and visualization skills (Tableau, R, matplotlib, or similar)
	- Excellent understanding of molecular biology and cell biology
	- Strong interpersonal communication skills
	- Excellent publication track record
	- Additional desired qualities include:
		* Familiarity with version control (git, subversion)
		* Experience with target enrichment
		* Experience in clinical laboratory testing
		* Experience with Seven Bridges, DNA Nexus or similar platform
+ skill set:
	- Design and architect the backend for our computer aided genetic circuit pipeline, including the genetic compiler and simulation engine
	- Develop our cloud infrastructure, data warehouse, build and deploy system, and machine learning platform. You will drive discussion and decisions on microservices vs monoliths, managed services vs deployed, etc.
	- Work on tooling to help improve the efficiencies of our synthetic biologists including data analysis platforms and robotic automation
	- solid at communicating architectures internally through clear design documents
	- embracing ambiguity and driving for impact
	- excited to partner closely with and learn from our synthetic biology team
	- interested in helping define the company product, direction and culture
	- You are a highly skilled technical lead in software engineer with years of experience in industry. You are passionate about joining an early stage startup where autonomy, passion to learn and excitement to engineer biology take precedence over process and ego. If you have a background in genetics or cellular biology, great! If not, you have strong experience with ETL systems, data storage and access, cloud infrastructures, build and deploy systems with a passion to learn the science.
+ skill set:
	- Architect the overall automation software, hardware, wetware ecosystem including hardware configurations, experimental protocols, and software control and scheduling systems.
	- Mentor other automation engineers.
	- Design, create, and deploy software to schedule, control, and manage the automation system.
	- Design, configure, and assemble a state-of-the-art automation system for biological engineering.
	- Work with synthetic biologists to translate manual protocols into automated processes and validate them at high throughput.
	- Work with hardware engineers to develop custom automation hardware to physically perform synthetic biology's experimental procedures.
	- Work with software engineers to develop custom software tools to schedule, control, and manage the automation system.
	- Develop metrics to measure and track the success of the automation platform.
	- Partner with automation vendors to vet software platforms and work to integrate them into our larger pipeline though the development of new APIs and methods.
	- Partner with automation vendors to obtain new hardware, develop new prototype hardware elements, build interfaces, and integrate hardware into the automation platform.
	- Embrace ambiguity and drive for impact.
	- Contribute to the company’s product, direction, and culture.
	- B.S. or higher degree in Bioengineering, Computer Science or a related field. PhD preferred.
	- 5+ years direct experience in biotechnology industry. 2+ years directly with automation.
	- Strong coding skills in modern software development environments. GitHub (or similar) portfolio of work ideal.
	- Publication or patent record related to biotechnology.
	- Strong written and oral communication skills.
	- Ability to physically relocate to Cambridge, Massachusetts.
	- You are a highly skilled programmer with years of experience in industry or research. You want to work on software development projects with strong algorithmic and research foundations. You are highly skilled technical lead in laboratory automation with years of experience in industry or research. You are passionate about joining an early stage startup where autonomy, passion to learn, and excitement to engineer biology take precedence over process and ego. You ideally have some laboratory automation and biology wetlab experience. You are interested in learning more about how to integrate hardware, software, and wetware.
+ skill set:
	- Design, configure, and assemble a state-of-the-art automation system for biological engineering.
	- Work with synthetic biologists to translate manual protocols into automated processes and validate them at high throughput.
	- Work with software engineers to develop custom software tools to schedule, control, and manage the automation system.
	- Develop metrics to measure and track the success of the automation platform.
	- Partner with automation vendors to obtain new hardware, develop new prototype hardware elements, build interfaces, and integrate hardware into the automation platform.
	- Embrace ambiguity and drive for impact.
	- Contribute to the company’s product, direction, and culture.
	- You are a highly skilled technical lead in laboratory automation with years of experience in industry or research. You are passionate about joining an early stage startup where autonomy, passion to learn, and excitement to engineer biology take precedence over process and ego. You ideally have some software programming and biology wetlab experience. You are interested in learning more about how to integrate hardware, software, and wetware.
+ Working knowledge of industry leading configuration management tools and methods (Salt, Ansible, Chef, Puppet, etc)
+ Ideal candidate would have experience with metabolomic datasets and familiarity with analytical chemistry/assay methods, automation systems, and LIMs
+ Good understanding of protocols like AXI, ACE, ACP, CHI etc.
+ Experience with SQL and Statistical/mathematical programming software packages (R, SPSS, CPLEX, LONDO or Xpress etc)
+ skill set:
	- Knowledge or hands-on experience in Container technologies such as Docker is a plus.
	- Knowledge of Virtual machines, Hypervisors is a plus
+ At least one graphics API (OpenGL, Direct 3D, Vulkan)
+ Either in AI tools; or autonomous embedded area; or FPGA
+ Seeking motivated graduate interns in the intersection of computer vision, 3D graphics and machine learning with applications to interactive visual effects processing. In this position, you will collaborate with VFX artists and fellow researchers in NEXT team to invent algorithmic solutions to VFX challenges. This is a full-time position located in Hillsboro, Oregon.
+ Programming experience in Python and MATLAB. Experience with LabView, VPItransmissionMaker, Tensorflow is a plus.
+ You find the 10% work that gets us 90% of the value
+ skill set:
	- Experience with large scale messaging systems like Kafka or RabbitMQ or commercial systems.
	- Experience with working with and operating workflow or orchestration frameworks, including open source tools like Airflow and Luigi or commercial enterprise tools.
	- Experience with pipelines that are used by many downstream teams, including non-engineering functions.
	- Experience with streaming data frameworks like spark streaming, kafka streaming, Flink and similar tools a plus.
	- Experience working with Apache Spark and data warehousing products.
	- Direct experience with a log collection and aggregation system at scale.
	- Demonstrated execution at a growth stage technology company.
+ skill set:
	- Experience with distributed data processing systems like Spark and Hadoop
	- Familiarity with interactive data visualization using tools like D3.js
+ skill set:
	- Design new and extend existing components of MLflow, such as experiment tracking, project management, and model deployment
	- Implement proprietary integrations of MLflow into the core Databricks product
	- Has designed and developed APIs used in production systems.
	- Deployed production web services using container and orchestration technologies, such as Docker and Kubernetes to public or private clouds.
	- Developed services leveraging SQL backend stores.
	- Demonstrates customer obsession: has altered designs for frontend or APIs with the user experience in mind
	- Developed and debugged software running on Linux OS
	- Experience with Continuous Integration/Continuous Deployment frameworks.
	- [Preferred] Experience working on a SaaS platform or with Service Oriented Architectures
	- [Preferred] Experience with software security and systems that handle sensitive data
+ Experience with: Large scale distributed computing, Database internals, Big Data engines e.g. Spark, Hadoop
+ skill set:
	- Passion for process automation
	- Build system experience like Maven, Bazel, or Gradle
	- Continuous integration and testing experience like Jenkins
	- [Preferred] Kubernetes and Docker
	- [Preferred] Experience on working with services provided by AWS, Azure, or GCP
+ Strong familiarity with server-side web technologies (eg: Nodejs, Java, Python, Scala)
+ Experience with our web stack (React, Redux, TypeScript, protobuf, Apollo, GraphQL) and Spark
+ skill set:
	- Passionate about developer experience: you want tools to be fast, CLIs intuitive, giving your colleagues (and yourself) the best experience possible doing development at Databricks.
	- Passionate about automation: automated workflows, automated testing, automated deployments, automated monitoring, our job is to automate away any tedious work out of our own lives and that of our fellow engineers
	- Comfortable in a heterogeneous environment and able to quickly learn new technologies. The team deals with a variety of environments from Kubernetes services to Jenkins automation, Scala libraries to Python scripting, writing graph algorithms to debugging 3rd-party Javascript, Dev-Tools does it all.
	- Able to quickly learn their way around unfamiliar codebases: Dev-Tools routinely dives into unfamiliar third-party projects (in unfamiliar languages!), pushing them far beyond what they say they can do “on the box”
	- Experience with Continuous Integration/Continuous Deployment frameworks
	- Experience with monitoring frameworks for large systems
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
+ skill set:
	- Develop and extend the Databricks platform. This implies, among others, writing clean, efficient code in Scala or Python and/or interacting with: cloud APIs (e.g., compute APIs, cloud formation, Terraform), with open source and third party APIs and software (e.g., Kubernetes) and with different Databricks services
	- Experience architecting, developing, deploying and operating large scale distributed systems at scale
	- Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP or an advanced private cloud such as Google, Facebook)
	- Experience working on a SaaS platform or with Service Oriented Architectures
	- Experience with Continuous Integration/Continuous Deployment frameworks
	- Experience with Docker and Kubernetes
+ skill set:
	- 2+ years experience with R or Python
	- 2+ years experience with predictive modeling
	- Familiarity with data visualization in R or Javascript
	- Understanding of relational data or SQL
	- French is not required and all European languages are appreciated
	- Experience with PySpark, SparkR or Scala
	- Experience developing WebApps
	- Experience building APIs
	- Experience with HDFS and NoSQL databases (MongoDB, Cassandra, etc)
	- Construct end-to-end data flows from raw data to predictions
	- Crunch, analyze and investigate any kind of data
	- Explore new machine learning algorithms
	- Build attractive visualizations
	- Communicate results to non-technical colleagues and clients
	- Provide data science expertise to sales, marketing, and R&D teams
+ skill set:
	- DSS is an on-premises application that connects together all big data technologies. We work with SQL databases, MongoDB, Cassandra, ElasticSearch, Hadoop, Spark, MLLib, scikit-learn, Shiny, … and many more. Basically, our technological stack is made of all technologies of the big data and machine learning landscape.
	- Our backend is mainly written in Java but also includes large chunks in Scala, Python and R. Our frontend is based on AngularJS and also makes vast usage of d3.js
	- One of the most unique characteristics of DSS is the breadth of its scope and the fact that it caters both to data analysts (with visual analytics) and data scientists (with deep integration in code and libraries, and a web-based IDE).
	- You are the ideal recruit if:
		* You are a full stack engineer. You know that low-level Java code and slick web applications in Javascript are two sides of the same coin and are eager to use both.
		* You know that ACID is not a chemistry term.
		* A first experience (either professional or personal) building a real product would be a big plus. Experience with some technologies of the Big Data and analytics stack (distributed databases, large-scale data processing, statistics, machine learning, JS visualizations, ...) is also desirable.
+ You are curious and pragmatic: you want to explore extensions to our product but are motivated by delivering production code for business use cases
+ skill set:
	- 2-4 years of experience in C/C++ development
	- Object oriented programming experience
	- Experience with applications design and implementation
	- Experience in multi-threaded programming
	- Proven track record of finding bottlenecks and delivering optimized, high-quality code
	- Knowledge in algorithms development and implementation
	- Fast learner, team player, reliable, motivated, hard worker
	- Experience in Android NDK development
	- Experience in image processing algorithms
	- Experience in runtime optimizations on embedded accelerators (e.g. Neon, DSP, GPU).
	- Experience in writing OpenCL kernels
	- Experience in Matlab
+ Open64 is a free, open-source, optimizing compiler for the Itanium and x86-64 microprocessor architectures.
+ Skill set:
	- Psyco, Nukita, Shed skin.
+ skill set:
	- Elastic Search, Lucene, SQL Server, Kibana, or similar experience
	- Prior experience aligning platform architecture with security, data
	- Demonstrated ability to document architectural standards and decisions
	- SaaS or high scale cloud experience
+ skill set:
	- Databases / Data warehousing
	- Writing complex SQL
	- Strong Microsoft Excel skills
	- Experience with Big Data platforms such as Hadoop
	- Experience with other reporting/visualization tools such as Tableau
	- Experience with monitoring and tracking tools such as  Splunk, NewRelic, Adobe/Google Analytics
+ skill set:
	- Experience with relational databases (MySQL, DB2 or Oracle) and NoSQL databases (Redis, Cassandra or DynamoDB)
	- Experience writing crisp, clean REST APIs
	- Experience with RSpec or equivalent integration test framework
	- Self-motivated individual who proactively identifies team bottlenecks and works with the team to resolve them
+ skill set:
	- Experience with data streaming technologies (Kinesis, Storm, Kafka, Spark Streaming) and real time analytics
	- Working experience and detailed knowledge in Java, JavaScript, or Python
	- Knowledge of ETL, Map Reduce and pipeline tools (Glue, EMR, Spark)
	- Experience with large or partitioned relational databases (Aurora, MySQL, DB2)
	- Experience with NoSQL databases (DynamoDB, Cassandra)
	- Agile development (Scrum) experience
	- Other preferred experience includes working with DevOps practices, SaaS, IaaS, code management (CodeCommit, git), deployment tools (CodeBuild, CodeDeploy, Jenkins, Shell scripting), and Continuous Delivery
	- Primary AWS development skills include S3, IAM, Lambda, RDS, Kinesis, APIGateway, Redshift, EMR, Glue, and CloudFormation
+ skill set:
	- Experience with Backbone, Marionette or equivalent framework
	- Experience with Protractor, RSpec or equivalent integration test framework
+ skill set:
	- Experience with system level monitoring tools such as Nagios or Zabbix and application performance monitoring tools such as NewRelic, AppDynamics or Dynatrace.
	- Understand configuration management tools such as Puppet, Chef or Ansible.
	- Strong understanding of the DevOps landscape from orchestration to instrumentation , from VCS to CI tools, and from APM to monitoring tools
+ skill set:
	- Working experience with AWS services (Aurora DB, Dynamo DB, Athena, EMR, Redshift, Data Catalog etc)
	- Experience with log analysis tools like Splunk
	- Experience with Issue trackers tools like Jira etc
+ You approach problems with a scrappy, creative, and entrepreneurial mindset.
+ You are comfortable with Big Data technologies like Hadoop, Spark, Hive, Presto etc.
+ Familiar with the operation of instrument -- VSG, VSA, Signal analyzer, channel emulator.
+ skill set:
	- LLVM compiler internals.
	- Polyhedral models.
	- Familiarity with HPC kernels and their optimization.
+ skill set:
	- Experience working with modern deep learning software architecture and frameworks including: Tensorflow, Pytorch, ONNX, MxNet, Caffe/Caffe2, and/or Torch;
	- Experience working with modern deep learning models including: Resnet, Mask-RCNN, RNN/LSMT, Bert, Transformer etc
+ Help the current effort of the AI research community, and contribute to cutting edge research in machine intelligence, starting from areas including Deep Learning, Generative Models, Reinforcement Learning, Evolutionary Computing, Sequence Modelling, Large-Scale Distributed Optimization and Low-Precision Numerical Formats.
+ Working on the IPU architecture compiler. Understanding code generation & optimization of C / C++ code to the instruction set of the machine. The architecture compiler and its ability to target the IPU for maximum performance and flexibility, is a fundamental component of the Poplar framework.
+ skill set:
	- Candidates should have a solid background with standard networking protocols (TCP, RPC, UDP, IPSec), low-latency protocols (RDMA, RMA) and Clustering.
	- Preferably, you should also have a background or interest in host device and network virtualisation (SR-IOV, Xen, Containers)
+ The role involves using a range of technologies, such as Python, CMake, BuildBot, Phabricator, AWS etc.
+ skill set:
	- Knowledge of storage systems (File, Block) is a plus (Local/Network/Cloud Attached)
	- Knowledge of ILOM, BMC, and OCP (Open Compute) is a plus
	- Good knowledge of common development tools such as yocto/git/gtest
+ Experience working with PCIe form-factor accelerators such as GPUs, DSPs or FPGAs
+ skill set:
	- SGE or other DRMS
	- XML and XPath/XSLT
+ skill set for DevOps:
	- Remote hardware administration with IPMI
	- Configuration and management of
		* SGE/Univa, Slurm, LSF or other DRMS
		* Jenkins CI
		* Phabricator
		* FlexLM licensing
	- Puppet, Ansible, Nagios
	- LLVM, GCC
	- DVCS e.g. Git
	- AWS, Azure, Google Cloud
	- XML and XPath/XSLT
	- Web programming – HTML/DOM, JavaScript, SQL
+ skill set:
	- Hardware-In-the-Loop (HIL) Systems
	- Development of test automation projects
	- Modeling of physical systems for HIL simulation
	- Specialized support of HIL systems and models
	- Development, testing, or familiarity of embedded control systems (mechatronics, automotive control systems)
	- HIL simulation systems
	- In-vehicle communication networks such as CAN, LIN, FlexRay
	- MIL/model-in-the-loop, SIL/software-in-the-loop, PIL/plant-in-the-loop, and HIL/hardware-in-the-loop
	- The implementation of software is done in project-specific technologies and thus extends over a wide area, including C/C++, C # (including WPF, WCF), Python, MATLAB ®, VHDL.
+ skill set for HPC Infrastructure Engineer:
	- Management of an on-premise computing cluster in a High Performance Computing (HPC) setting
	- Develop usage policies for deep learning training
	- Develop tools and infrastructure to scale deep learning
	- Maintain network infrastructure for local and cloud compute
	- Data management and backups
	- Familiarity with environments including LDAP, NFS, bare metal GPU servers, deployments and automation / conﬁguration management, modular user shell environments, networking
	- Hands on server hardware configuration experience
	- Experience with cluster management software
	- Comfortable with GPU servers
	- Strong knowledge of Linux systems and internals (Debian preferred) with a good understanding of networking and related protocols, OS customization, and package management (APT)
	- Hands on Infiniband experience
	- Have used or developed metrics/analytics tools for usage
	- Experience with Slurm or similar job systems
+ skill set for Deep Learning Research Scientist at DeepScale:
	- DeepScale was founded by the deep learning researchers from UC Berkeley who created SqueezeNet. DeepScale is developing perception systems that enable automated vehicles to interpret their environment in real-time using low-cost hardware.
	- A PhD in electrical engineering, computer engineering, or computer science.
	- A track record of advancing the state-of-the-art in an application of deep learning (ideally a computer vision or imaging application … but if you did speech-recognition or text-analysis, that's pretty good too)
	- Published papers that either (a) are in top peer-reviewed conferences such as CVPR, NIPS, ECCV, ICCV, or ICML … or … (b) a significant (>100) number of citations on one of your deep learning research publications
	- The ability to design, implement, train and test models in one or more of the leading deep learning frameworks like PyTorch or TensorFlow.
+ skill set:
	- Machine learning areas of special interests include: CNNs, RNNs, distributed GPU computing, detection, prediction, motion planning, mapping and localization
	- Work with lidar sensor firmware and low level signal processing
	- Perform Multi-Object Tracking & Multi-Sensor Fusion
	- Sensor calibration & Perception algorithms (all types and flavors)
+ skill set:
	- Familiar with the basics of lidars, radars, cameras, and ultrasonic systems.
	- Are familiar with Physics-based modeling and simulation of sensor systems (eg, link budget analysis, wave propagation, sensor noise sources, etc.).
	- At least 1 year of experience in radar, lidar, or camera modeling and/or evaluation.
	- Familiarity with test and calibration processes for autonomous vehicle sensors.
	- Familiarity with EVT, DVT, and PVT processes for sensor bring-up and validation as well as FMEA principles.
+ Working experience with large-scale data platform and pipelines such as Hadoop, Hive, Pig, MapReduce, Spark, Kafka, Flumes, etc.
+ Familiarity with GPU computing (CUDA, OpenCL) is preferred
+ Familiarity with modern planning approaches including randomized search methods and trajectory optimization
+ Solid work experiences with state-of-the-art mapping and localization algorithms
+ Experience with scheduling engines such as Airflow
+ Published papers (e.g. in Supercomputing, IPDPS, or PPOPP) and/or open-source code that demonstrate your skills in writing fast code.
+ Design and implement a set of compiler tools to translate from ML-oriented domain-specific languages (TensorFlow, Caffe, Theano) to proprietary binary format
+ Buzzwords: Gitlab, Docker, MongoDB, AWS, Node.js.
+ You have experience in material characterisation: AFM SEM, TEM, XPS and others.
+ Experience with Analytics tools is appreciated  (Segment, Amplitude)
+ skill set:
	- are as passionate about teaching AI/ML as you are about AI/ML itself
	- are experienced in software engineering, machine learning engineering in Python using scikit-learn: regression, trees, ensembles (nice-to-have: catboost, XGBoost)
	- are comfortable in collecting and manipulating data in Python: APIs, web scraping, Pandas, numpy/scipy
	- have tackled Deep Learning (DL), including LSTMs, RNNs, CNNs (nice-to-have: YOLO) with real-world application experience such as computer vision or NLP
	- have implemented ML and DL at scale using SparkML, Keras, TensorFlow and/or Pytorch
	- have a strong understanding of software engineering best practices, including version control, testing, monitoring and debugging
	- are highly proficient in the curriculum topics in our program
	- are available for weekly, 30-minute video check-ins with students to help them set and achieve learning goals, answer subject matter questions, provide feedback on projects, and career advice
	- have at least 3 years of experience solving real-life machine learning problems and building models
	- are empathetic and have excellent communication skills
+ skill set:
	- Experience in one or more of the following areas:
	- Control theory
	- Motion planning
	- Optimization
	- Formal logic
	- Game AI development
	- Experience in developing safety-critical, embedded or real-time systems
	- Published research in any of the above mentioned areas
	- Experience in machine learning and data analysis
	- Programming in Python, working with Linux
+ Familiarity with neural network framework such as Caffe, Torch, Theano, TensorFlow, CNTK
+ skill set:
	- 4+ years of embedded software applications development, debug, deployment (including drivers development)
	- Superior knowledge of RTOS-based software systems and/or Embedded Linux
	- Comfortable with parallel paradigms (notions of pthread and/or OpenCL/Cuda)
	- Agile methodology ; Git, continuous integration, test driven development
	- Experience of AUTOSAR architecture is valuable
	- Knowledge of Computer Vision libs (OpenCV, OpenVX) and Machine learning technology (TensorFlow, Caffe) would be a plus
	- Knowledge of application middleware (e.g. ROS)and communication layers (TCP/UDP, DDS) would be a plus
+ The Computational Neuroscientist’s primary job function is to work with the CTO to identify innovative technologies, and to research and develop practical Spiking Neural Network based products by using BrainChip’s spiking neural model and previous research.  This involves the development of an architecture that is flexible in its application. The resulting SNN architecture will have a wide application range in areas such as Computer Vision, Olfactory, Auditory and Tactile feature learning and extraction.  Will also work on a development kit and API which will form the foundation for products for the in-house product development team and will also be made available to external research and development facilities. The computational Neuroscientist will work with associated educational facilities such as the Cerco, UCI and UCSD and evaluate technologies that are relevant to BrainChip’s SNN technology.
+ Experience in one application field (Image processing, ADAS, FinTech, CyberSecurity)
+ Familiar with big data processing tools such as Hadoop, Spark, HBase
+ skill set:
	- Ability to performance tune and scale models
	- Experience with Docker/Containerization
	- Experience with Django
	- Experience in a micro-service architecture
+ skill set:
	- 8+ years of meaningful industry experience and a background in high-speed processor design (i.e. Graphics, Microprocessors, Network Processors, or Mobile / Multimedia SOCs)
	- Knowledge of GDDR5, LPDDR4 or DDR3 or related protocols, or knowledge of PCIE and high-speed Serdes
	- Experience with all stages in the ASIC design flow including emulation, prototyping, DFT, timing analysis, floorplanning, ECO, bringup & lab debug, and ATE test development
	- Experience with high speed clocking, cache interfaces and protocols
+ skill set:
	- Monitor and Metrics gathering (Prometheus, StackDriver ...)
	- Experience with HashiCorp tools - Vault, Consul, Nomad
	- Experience with NixOS
+ skill set:
	- Assisting developers to apply best practices to ensure fully working test, training and production environments using Gitlab-CI, Docker, Ansible.
	- Designing, building and maintaining CI/CD, testing, and operations infrastructure for our systems.
	- 5 years of DevOps experience, both building end-to-end automated CI/CD pipelines, as well as application and operations support. This experience should include hard-core hands-on Linux admin, networking, security, and AWS experience in a dev-through-production environment.
	- A proven track record of excellent customer service delivery, including working with developers, ops, and users to troubleshoot and resolve challenging problems in a timely manner, and being an embedded DevOps member of an application scrum team, as well as requirements gathering, design, project planning, and implementation of DevOps process and tooling.
	- Strong architectural level of understanding of software, networks, security, and operations, with the knowledge and know-how to influence software and operations design and process.
	- Strong hands-on familiarity with infrastructure automation tools such as Jenkins, GoCD, Terraform, Artifactory/Nexus, Ansible, Puppet, Chef, InSpec, etc.
+ skill set:
	- Building, maintaining and improving the modeling infrastructure using cutting edge tools including: Hadoop, PIG, Kafka, Flume, Ssamza, Zookeeper, PySpark, Elasticsearch, Python, Django
	- Learning how to build and maintain an ETL pipeline for scalability and stability.
	- Supporting data analysis, mathematical modeling, machine learning and data mining for price testing and its optimization
	- Explore and employ external data sets, finding them, adding them to the system, adjusting and reformatting/cleaning up so that they can be consumed by our infrastructure. Developing models and machine-based understanding of interactions between external datasets and client data.
+ skill set:
	- Experience turning ideas into actionable designs. Able to persuade stakeholders and champion effective techniques through product development
	- 5+ years of industrial data-mining / analytics experience including applied techniques in data mining, machine learning, or graph mining using Python, R, and/or Spark
	- Comfortable working in a dynamic, research-oriented group with several ongoing concurrent projects
+ skill set:
	- Develop ETL operations using Python, Spark, SqlServer, Redshift and Kafka.
	- Develop the core tooling library to support Airflow data pipelines.
	- Design and implement the testing framework for Airflow dags and write test cases.
	- Document our systems for internal and external stakeholders
	- Support business stakeholders, analysts and data scientists on diverse projects.
	- Monitor and debug data pipelines running on Airflow.
	- Participate in code reviews.
	- Deliver quality work on tight deadlines.
	- Experience building systems with a framework, ideally Airflow (web frameworks are helpful, too)
	- Experience with data manipulation tools like Pandas, or ideally Spark
	- Experience with test automation
	- Experience accessing data via an API
	- Total control of Git
	- Working knowledge of Linux
	- Deployment tools (i.e. Ansible, Puppet, Chef)
	- Working knowledge of set-based querying (joins, etc...) ideally indexing, too.
	- Hadoop/Spark experience
	- Flume/Gobblin/Kinesis
	- AWS experience
	- Web analytics experience
	- Docker experience
	- Jupyter (notebook) experience
	- Log processing experience
+ skill set:
	- Knowledge of JIRA, Test Rail, Jenkins, Splunk, New Relic, Allure Reports
	- Knowledge of Selenium or TestCafe or Cypress is a bonus
+ skill set:
	- Experience with modern web frameworks, like Django, Flask, or Rails (Ruby)
	- Experience with modern Javascript technologies like Node.js, React, Webpack and ESLint plugins
	- Culture of code reviews and collaborating closely with other people (not a solo hacker)
	- You are familiar with translation management/memory tools.
	- Continuous integration & continuous deployment of localized product experience.
+ skill set:
	- Design fault tolerant systems that can scale, and allow us to move quickly without impacting customer access
	- Build observable systems that track important metrics and automatically notifies when something is off
	- Experience with multi-region data center architecture
	- Experience working with a distributed service-oriented architecture
	- Experience building admin features in SaaS applications
	- Experience with modern security and access control practices
	- Experience with continuous integration (frequent deployments)
	- Experience with GraphQL
	- Material contributions to open source projects
	- Linux (Ubuntu) configuration and administration
+ skill set:
	- Back End: We write lots of micro-services, primarily with Java 8. Our APIs are RESTful and use the minimal Dropwizard framework. We take advantage of Kafka, Spark, Hadoop for processing volumes of data.
	- Front End: Our web applications are complex, single-page apps written in JavaScript (React, ECMAScript 6, Sass).
	- Core Data, Infrastructure, & Reliability: Building the systems that power thousands of services with Singularity on Apache Mesos, and empowering access to massive datasets with HBase, Elastic Search, ZooKeeper, Redis, MySQL, and Memcached.
+ Are language agnostic. We aren’t overly concerned with tech stack - if you are interested in learning new things, we’re interested in teaching you.
+ Key Performance Indicator (KPI) definition and propagation. Uses statistical inference to determine what KPIs are truly impactful to our customer engagement and adoption efforts. Work cross-functionally to make sure these KPIs are available for all relevant data sources and business units.
+ Refactoring existing C++ libraries for modularity and extensibility.
+ To bring visualization and easy-to-use data analysis tools to our users, our research and development teams work on full data processing cycle: from database’s query processing, to data science (AI, machine learning, etc.), UI design, and cloud computing.
+ Integrate our data pipeline with available Augmented Analytics models, tools, and applications.
+ Leverage and advance our query processing engine to build a data pipeline that enables the development of features and integration with available Augmented Analytics models, tools, and applications.
+ TeamCity: the Hassle-Free CI and CD Server by JetBrains
+ skill set:
	- Strong understanding of core Hadoop concepts including Yarn, MapReduce, Hive, Pig, Sqoop, HDFS
	- Experience implementing large scale data loading, manipulation, processing and exploration using a range of AWS based technologies such as Spark, Kinesis, Athena, RedShift, Postgres etc
	- Extensive experience in data profiling, source-target mappings, ETL development, SQL optimisation, testing and implementation
+ Identify scaling bottlenecks and propose solutions.
+ Preferred (but not essential) experience any Big Data technologies such as languages like R, Hadoop, Machine Learning and Data Lakes
+ semi-supervised learning
+ Experience with or exposure to Big Data (Hadoop/YARN, Spark, Nifi, Storm, Cassandra, Solr) is a definite plus.
+ skill set:
	- Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
	- Experience with any of the following distributions of Hadoop - Cloudera/MapR/Hortonworks.
	- Strong experience with Apache Spark, Hive/Impala and HDFS
	- Familiar with Python, Unix/Linux, Git, Jenkins, JUnit and ScalaTest
	- Proficient in Scala, Java and SQL
	- Other functional Languages such as Haskell and Clojure
	- Big Data ML toolkits such as Mahout, SparkML and H2O
	- Apache Kafka, Apache Ignite and Druid
	- Container technologies such as Docker
	- Cloud Platforms technologies such as DCOS/Marathon/Apache Mesos, Kubernetes and Apache Brooklyn.
+ Familiarity with Agile methodologies, such as Scrum or Kanban, as well as software development practices such as Continuous Integration, Test-Driven Development and DevOps.
+ Advanced level programming skills in Python, ideally developed from experience working on long-term commercial projects, including significant experience using SciPy and machine-learning packages (Numpy, Pandas, scikit-learn, etc) for the development of maintained components. Additional experience using PySpark a big plus.
+ skill set:
	- Working experience of Databricks or similar
	- Knowledge of concepts such as data warehouse, star schemas, KPIs
	- Hands on experience with Linux, HDFS, HIVE, HADOOP
	- Experience with ETL / Informatica, JSON structures and REST Based web services a plus
+ Second, help build a Data Science team to support the delivery of the Predictive Analytics core AI offerings for US-based pharmaceutical companies. These offerings include developing algorithms to find undiagnosed patients (often patients with rare diseases) or algorithms to predict patients likely to experience rapid disease progression and / or switch therapies. These core offerings involve very large datasets (typically many millions of patients) extracted from claims and prescription data and use Python and PySpark tools and libraries as part of IQVIA’s Hadoop cloud environment.
+ In depth experience with Spark/Hadoop and either Theano/Tensorflow/Caffe/Torch.
+ Here are some of the key technologies used to build our document management platform and its applications.
	- Server: REST, Scala, SQL, Java
	- Client: JavaScript, React, HTML5, Less/CSS, Kendo UI, webpack
	- Database & Search: PostgreSQL, Solr
	- Build Automation: sbt, webpack, Bamboo
	- Tools & Infrastructure: JIRA, Crucible+Fisheye, git, nexus
	- Technical/functional expertise .
+ skill set:
	- ***Competence with industry-standard tools like: Git, npm, JIRA, Docker, etc.***
	- Comfortable with Cloud services, specifically AWS services (in particular: ECS, EC2, S3, ECR) and their respective APIs, is a major plus.
	- Experience building for SaaS/PaaS and distributed applications.
	- Intimate knowledge of web services and building and interacting with REST APIs is essential.
+ skill set:
	- Experience of working with Flask (or similar web framework) to build APIs.
	- Excellent understanding of SQLAlchemy, data modelling, Alembic, Postgres, Celery, Redis.
	- 3 to 5 years prior related experience in developing web applications, graduate experience, or demonstrated success in development with DrugDev Spark or equivalent system
+ skill set:
	- Experience with the [Common Workflow Language (CWL)](https://www.commonwl.org/), [Arvados](https://arvados.org/), PERL, C++, AWS, Docker
	- [Dockstore, developed by the Cancer Genome Collaboratory, is an open platform used by the GA4GH for sharing Docker-based tools described with the Common Workflow Language (CWL), the Workflow Description Language (WDL), or Nextflow (NFL)](https://dockstore.org/)
	- [Nextflow](https://www.nextflow.io/)
	- [Workflow Description Language (WDL)](https://software.broadinstitute.org/wdl/)
	- https://fairsharing.org/bsg-s000606/
	- https://dnastack.com/#/
	- https://curoverse.com/about
+ skill set:
	- Java (Spring Boot, Java EE)
	- Google Cloud Platform
	- Relational Databases (MySQL, PostgreSQL, BigQuery)
	- REST and OpenAPI
	- JavaScript (AngularJS)
	- TypeScript (Angular)
	- 12-factor application model
	- Bash
	- Git
	- WDL (Cromwell)
	- Microservices, Docker, and Kubernetes
	- Continuous deployment
+ skill set:
	- Advanced level programming skills in Python, ideally developed from experience working on long-term commercial projects, including significant experience using SciPy and machine-learning packages (Numpy, Pandas, scikit-learn, etc) for the development of maintained components. Additional experience using PySpark a big plus.
	- Ability to integrate and scale solutions that involve large data sources in SQL databases and/or distributed systems such as Hadoop, as well as considerable experience deploying at scale on cloud technologies such as AWS, GCP, Azure.
	- A set of software-development values that ensures high-quality, readable and maintainable code is produced within an open and collaborative environment.
	- A pragmatic approach in scope and design, seeking simple iterative solutions wherever possible to shorten the time-to-value of work.
	- Knowledge of supervised machine learning methods, such as regularised regressions, ensemble tree classifiers (e.g. xgboost), Support Vector Machines, deep learning, etc.
	- Additional experience developing in C++, R, Java, Scala, Java, JavaScript, or advanced ability user of shell scripting commands (grep, sed, awk, etc).
	- A demonstrable interest (e.g. public GitHub repo, or online course completion) in one of the following machine learning libraries (or equivalents): TensorFlow, Spark MLLib or CRAN packages for machine learning.
	- Knowledge of healthcare / life science issues involving Real-World Evidence.
+ skill set:
	- Good understanding of SQLAlchemy, data modelling, Alembic, Postgres.
	- Understanding of RESTful APIs and web services.
	- Experience of working with Flask (or similar web framework)
	- Previous experience with scrum and Jira.
+ skill set:
	- Experience working in an Agile environment using Test Driven Development (TDD) and Continuous Integration (CI)
	- Experience refactoring code with scale and production in mind.
	- Proficient in Scala, Java and SQL
	- Strong experience with Apache Spark, Hive/Impala and HDFS
	- Familiar with Python, Unix/Linux, Git, Jenkins, JUnit and ScalaTest
	- Experience with integration of data from multiple data sources
	- Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
	- Experience with any of the following distributions of Hadoop - Cloudera/MapR/Hortonworks.
	- Other functional Languages such as Haskell and Clojure
	- Big Data ML toolkits such as Mahout, SparkML and H2O
	- Apache Kafka, Apache Ignite and Druid
	- Container technologies such as Docker
	- Cloud Platforms technologies such as DCOS/Marathon/Apache Mesos, Kubernetes and Apache Brooklyn.
+ skill set:
	- You will join a team of highly talented Engineers and Data Scientists, with your main focus being to write highly performant and scalable code that will run on top of our Big Data platform (Spark/Hive/Impala/Hadoop). As such, you will work closely with the Data Science team to support them in the ETL process (including the cohorts building efforts).
	- Working in a cross-functional team
	- Building scalable and high-performant code
	- Mentoring less experienced colleagues within the team
	- Implementing ETL and Feature Extractions pipelines
	- Monitoring cluster (Spark/Hadoop) performance
	- Working in an Agile Environment
	- Refactoring and moving our current libraries and scripts to Scala/Java
	- Enforcing coding standards and best practices
	- Working in a geographically dispersed team
	- Working in an environment with a significant number of unknowns – both technically and functionally.
	- BSc or MSc in Computer Science or related field
	- Strong analytical and problem-solving skills with personal interest in subjects such as math/statistics, machine learning and AI
	- Solid knowledge of data structures and algorithms
	- Experience working in an Agile environment using Test Driven Development (TDD) and Continuous Integration (CI)
	- Experience refactoring code with scale and production in mind.
	Proficient in Scala, Java and SQL
	- Strong experience with Apache Spark, Hive/Impala and HDFS
	- Familiar with Python, Unix/Linux, Git, Jenkins, JUnit and ScalaTest
	- Experience with integration of data from multiple data sources
	- Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
	- Experience with any of the following distributions of Hadoop - Cloudera/MapR/Hortonworks.
	- Other functional Languages such as Haskell and Clojure
	- Big Data ML toolkits such as Mahout, SparkML and H2O
	- Apache Kafka, Apache Ignite and Druid
	- Container technologies such as Docker
	- Cloud Platforms technologies such as DCOS/Marathon/Apache Mesos, Kubernetes and Apache Brooklyn.
+ skill set:
	- Java, Scala & Python
	- Kubernetes & Docker
	- Data analysis tools such as R
	- We rely on Big Data / Hadoop technologies such as Apache Spark, Impala and Amazon Redshift
	- Participate in Agile practices such as daily stands-ups, sprint planning and retrospectives
+ skill set:
	- Serving in the role of Senior Security Researcher, you will have a direct impact on the direction of the company by researching threats, understanding how they appear on the network, reversing malware and helping technically shape the product direction.
	- Perform leading edge security research – malware analysis, fuzzing, web-based threats, network/protocol analysis, etc. – and generate intelligence which will be incorporated into the product
	- Create and enhance the company’s security content framework, including malware intelligence and the process workflow
	- Research new threat detection technologies and investigate approaches
	- Apply your expert insights and experience in classifying new threats and mitigation techniques
	- Collaborate across Vectra to identify new detection models – working hand-in-hand with members of the data science team
	- Pursue security research topics that contribute to the knowledge and enumeration of new threats
	- Provide an attackers-eye-view to the evidence presented by Vectra products and educate customers to the technical nature of the threat
	- 5+ years direct experience in areas of security research, malware analysis, networking/system administration or software development
	- 5+ years of attack and penetration testing experience
	- Advanced technical degree
	- Knowledgeable in exploitation technology such as shellcode, heap spray, ROP, etc.
	- Knowledgeable in network and application protocols, and traffic analysis (network forensics)
	- Proficiency with reverse engineering tools like standard debuggers, IDA pro, etc.
	- Proficiency with network traffic analysis and network forensics tools such as Wireshark and tcpdump
	- Proficiency with host forensics and memory analysis tools related to studying active exploitation
	- Knowledge of corporate security investigation and incident response processes, along with malware detection and mitigation technologies
	- Solid programming skills with scripting languages such as Python
	- Deep working knowledge of networking and network application concepts: TCP/IP, HTTP, TLS, FTP, IRC, RPC, DNS, SMB, Kerberos, etc.
	- Strong problem solving, troubleshooting and analysis skills
	- Excellent written and verbal communication skills
	- Excellent inter-personal and teamwork skills
	- Proactive, hard-working team player with a good sense of humor
	- Self-driven, able to efficiently work remotely without close supervision
	- Professional or academic research in advanced security threats
	- Operational experience in infosec as an incident handler, administrator, or internal consultant
	- Experience with big data technologies such as Hadoop and Spark
	- Participation in the broader infosec community with requisite contacts and access to external intelligence sources
	- Understanding the lifecycle and economics of modern malware and advanced threats
+ skill set:
	- Knowledge of web language abstractions (Babel, Sass, etc.)
	- Experience developing bespoke data visualizations with D3.js
	- Understanding of resource-driven API structure
	- Experience developing complex single-page applications with a UI framework (e.g. Ember, Angular, React, Vue)
+ skill set:
	- Work with product management to help define requirements for new high value features for customers
	- Help break down and plan work for your team
	- Help continue to grow the team through interviewing and hiring
	- Mentor and grow team members both technically and non-technically
	- Participate in architecture discussions and guide the technical direction of the team
	- Ensure smooth delivery of software with automated tests in a modern CI toolchain
+ skill set:
	- Design and implement high-performance libraries/APIs for machine learning and statistical techniques.
	- Experience with streaming and event-based programming
	- Experience with continuous integration and deployment workflows
+ skill set:
	- Node.js or Python, JavaScript, HTML, CSS. Experience with at least one major web framework (django / symfony / express or others)
	- Decent knowledge of modern frontend framework or libraries, preferably React
	- Voracious appetite for learning
	- Experience with Elastic Search or alternative search data technology, managing significant amount of data
	- Advanced experience with AWS (Lambda, DynamoDB, EC2, etc.) with particular focus on scalability.
	- Previous experience with CI/CD, preferably with Jenkins, deploying multiple times a day to production.
	- Experience with CapnProto or BRO logging format is a plus.
+ skill set:
	- Database hands-on experience (MySQL, couchdb, ElasticSearch, etc.)
	- Understand modern web app architecture and be able to develop features in JavaScript as well as implement the Python-based web server APIs that field requests and responses.
	- Excellent collaboration skills are required due to the cross functional communication necessary to help build the product.
	- Experience with Python, JavaScript, and a modern web framework (e.g. Ember, React, etc.)
	- Database hands-on experience (MySQL, couchdb, ElasticSearch, etc.)
	- Experience with Python web framework (e.g. Django)
	- Familiarity with web-based data visualization libraries (e.g. D3.js)
	- Understanding of semantic markup (HTML) and web styling
	- Knowledge of web language abstractions, such as ES6/ES2018, CSS preprocessors, etc.
+ Comfortable with AWS/Azure/On Premiss  deployments, networking, and security best practices.
+ skill set:
	- Database hands-on experience (Elasticsearch and Mongo preferred)
	- Administration of systems including Jenkins, Elasticsearch, Kibana, Mongo, Grafana, etc.
+ skill set:
	- Tensorflow
	- Object oriented programming, C++ or Java
	- Spark or Map Reduce
	- SQL and noSQL database experience
	- Linux
	- Source control experience, preferably GIT
+ skill set:
	- Familiarity with Hadoop, Map/Reduce, Spark, and distributed computing
	- Understanding of data pipeline architectures (e.g. Lambda, Kappa)
	- Database hands-on experience (MySQL, MongoDB, couchdb, ElasticSearch, etc.)
	- Knowledge of real-time data pipelines (e.g. Kafka and Spark Streaming)
	- Experience with continuous integration and deployment workflows
	- Experience with Docker, AWS/Azure/On-Prem deployments, and networking
+ skill set:
	- The candidate must have a sufficient understanding of and practical experience with classic statistical modeling techniques (e.g. logistic regression, CART, K-means clustering) and machine learning algorithms (e.g. gradient boosting, neural networks, random forests)
	- Comfort with ambiguous and large streams of data across different formats and entry points; Hands-on experience working with large data processing (processing large datasets); hands on experience with cloud environments (e.g. AWS, Azure) and Big Data technologies (e.g. Hadoop, Spark)
	- Expert-level Python, R and SQL coding; Experience with TensorFlow or Microsoft Cognitive Tool Kit required
	- Experience developing high value features; Hands-on experience deploying models in real-time environments
+ skill set:
	- NBA's Team Marketing and Business Operations ("TMBO") group is a unique in-house consulting arm within the NBA league office that drives best practice sharing and innovation across all NBA, WNBA, NBA G-League and NBA2K teams.
	- The Data Scientist role will be a technical expert within TMBO in all matters surrounding statistical analysis, data manipulation and interpretation, and process automation. You will be a thought leader, tasked with the responsibility to leverage the NBA’s various internal data sources to create new and innovative analytical products and outputs to inform league executives about the state of team businesses. You will uncover insights through predictive analyses and data visualization, drive better decision making by using various statistical modeling techniques, and promote efficiencies in reporting to various stakeholder groups within the league office. The demand for advanced analytical solutions to business operations issues in sports continues to grow exponentially, and this is your opportunity to grow with us in a fast-paced, collaborative environment.
	- Help lead TMBO’s efforts to develop and maintain analytical products for communicating the state of team businesses to key stakeholders at the league office
	- Work with TMBO executives to further proprietary analytical research for presentation to teams at various league workshops
	- Enhance the current report generation processes within TMBO through the lens of potential automation and conversion to different, more useful development environments (e.g., R, Python, Tableau, etc.)
	- Develop an understanding of current needs in data collection at the league office and create solutions to gather the required information
	- Perform statistical analysis and create and maintain descriptive and predictive models as needed on a project basis
	- Demonstrated skills, knowledge and experience in converting data into insights
	- Passion for developing methods to streamline processes and data flow through automation
	- Experience in understanding existing data structures, including collection and standardization processes, and ability to help shape future processes going forward with an eye toward business needs
	- Ability to consult with business analysts and operators to understand their data needs, develop systems to access the data and convert the outputs into various formats for analysis
	- Comfort with ambiguity in data and experience in working with partners to optimize third-party data streams
	- Detail-oriented, extremely organized with ability to manage projects from inception through execution
	- Strong communication skills, both verbal and written, particularly for presentations
	- Expertise in leveraging R and Python to perform statistical analysis, build models and automate processes
	- Expertise in using SQL to tap expansive databases
	- Expertise in creating informative data visualizations through Tableau
	- Familiarity with other coding languages, statistical analysis tools and business intelligence platforms preferred
	- Familiarity with Microsoft Office software required, strong skills in Excel and VBA preferred
	- Expertise in developing, executing and implementing: various classification and regression models and familiarity with more advanced machine learning techniques
	- Expertise in parsing existing code and developing ways to increase its efficiency
	- Familiarity with accessing and manipulating data via APIs
	- Experience in an advanced analytical role, preferably in an industry focused on leveraging data to develop loyal fans or customers
+ skill set:
	- Throughout the year, instructors will work on lecturing the course, curriculum development for new and existing courses, hosting office hours, managing a Teaching Assistant, assist with marketing, and others on an hourly basis.
	- Leading course discussions
	- Providing personalized support to students
	- Managing a Teaching Assistant
	- Hosting office hours
	- Improving the quality of our curriculum
	- Participating in course marketing activities
	- Is a data scientist with outstanding knowledge of and experience in Python, Calculus, Linear Algebra, Probability, Statistics and various Data Science concepts and algorithms which include supervised and unsupervised learning, dimensionality reduction, exploratory data analysis, and programming best practices
	- Is equally passionate about data science and teaching
	- Strong communication skills
	- Preferably has teaching experience alongside data science
	- Approaches problems with a design perspective
+ skill set:
	- Implement and maintain the League’s data analytics platform(s) to manage the ongoing monitoring of internal and external information related to Sports Betting/gaming;
	- Coordinate with other departments and third-parties to identify internal and external sources of meaningful information and assist in the process of data collection;
	- Work with and coordinate the integration of multiple data sets into the selected data analytics platform(s) and/or visualization tool(s);
	- Work with technical partners to automate data flows and reduce reliance on manual data development;
	- Develop queries and manage the technical aspects of data interrogation;
	- Maintain a reporting mechanism and protocols to ensure appropriate communication of alerts and/or findings, including through visualization and dashboards;
	- Oversee information requests and project activities to ensure accurate, timely, and efficient reporting deliverables; and
	- Support the investigation and review of any alerts
	- Five+ years of experience in data analytics or a comparable area of expertise
	- Integrity monitoring experience and/or understanding of the Sports Betting/Gaming industry preferred
	- Ability to exercise discretion and use independent judgment in making decisions and work with minimal functional guidance; demonstrated project management skills
	- Excellent oral and written communication skills, ability to share findings with non-technical audience, and deal effectively with the senior management, staff members, and vendors
	- Exceptional problem solving and issue-spotting skills
	- Excellent interpersonal and time management capabilities
	- Experience with data analytics platforms, such as Symantic Pro, Symantic Cortex, IBM i2
	- Understanding of statistical modeling techniques
	- Proficiency in at least one of the following languages: R, Python, SQL query writing, working with JSON/XML data
	- Data transfer and encryption knowledge, such as sftp, pgp, and others; and
	- Knowledge of Cloud platforms, such as AWS, Azure, and others
+ skill set:
	- Openstack
	- Linux kernel namespaces internals
	- Linux kernel KVM internals
	- K8s networking internals
	- OpenVSwitch
	- Workload optimization in HPC / HTC (High Throughput Computing)
	- In depth knowledge of Linux operating systems
	- Networking protocol development experience
	- Storage experience and/ or virtual machine experience
	- Understanding of the operational, maintenance, monitoring and support aspects of a business - critical system.
	- Understanding of underlying hardware in large - scale systems and how to best use it.
	- Kernel virtualization mechanisms
	- Leading software defined storage projects (Gluster, Ceph, Hadoop, etc...)
	- Experienced in working in large bare-metal data centers
+ skill set:
	- Design, deploy and operate cyber technologies in order to solve the security challenges in a technically diverse and complex environment
	- Implement and develop in-house security tools and integrate open-source solutions
	- Perform POCs and remain up-to-date and knowledgeable in regard to the latest security trends and emerging technologies
	- In-depth knowledge of security concepts, architecture and methodologies from end to end perspective
	- Strong background in most of the following topics: SIEM, Firewalls, IPS, NAC, EDR, DLP, OS Hardening, Vulnerability Management
	- Knowledge with security aspects of networking, operating systems (Windows, Linux) and virtualization
	- Coding/scripting capabilities – shell scripts (Python preferred)
	- Self-motivated and an autodidact
	- Team player
	- Familiarity with working in DevOps oriented environment (Jenkins, OpenStack, Salt and Dockers) - Advantage
	- Hold a professional certification in good standing (CISSP, GSEC) - Advantage
+ skill set:
	- Final Israel Ltd. is one of the world’s leading high-frequency trading (HFT) companies.
	- We use proprietary prediction and trading algorithms as well as highly innovative schemes for handling large amounts of data. As a major participant in the HFT industry, the challenge Final faces is two-fold: to analyze large and complex data sets off-line, as well as to process massive flows of real-time data.
	- Final’s most important asset, its employees, comprises a hand-picked group of extremely talented, highly motivated, and diverse individuals.
	- The position entails research, development and implementation of algorithms in the fields of machine learning, signal processing, data mining and statistics.
+ Strong understanding of the Hadoop ecosystems (especially Kafka, Flume, Avro, Parquet, HBase, Hive, and Hue) and related technologies with +2 years of experience
+ skill set:
	- Mongodb
	- Elasticsearch
+ skill set:
	- Good working knowledge in a wide range of machine learning methods and algorithms for classification, regression, clustering, and others – a must.
	- Good Statistical analysis, e.g. hypothesis testing, estimation theory and mathematical skills.
	- Experience dealing with end to end machine learning projects: data exploration, feature engineering/definition, model building, performance evaluation and help in implementation.
	- The ideal candidate should be able to use his/her experience in implementing advanced analytical methods (machine learning, statistical/mathematical modeling) on large amounts of raw data – dealing with all parts of modeling workflow (from data extraction, feature engineering to model building and implementation) and should be able to clearly present and communicate the findings.
	- As a Data Scientist at Playtika you will take part in projects in which analytical solutions are used for solving and/or optimizing business/product problems like user experience modeling, churn prediction, advanced segmentation and others.
	- Experience working with big data tools e.g. pySpark.
	- DL toolbox (e.g. pyTorch, Tensorflow etc. it a plus).
	- Knowledge in Deep learning models (CNN, RNN, etc …)
	- Reinforcement Learning
+ skill set:
	- Reviewing the ETL Design and Data Roadmap, and continuously improving the processes.
	- Ensuring the ETL Codes are running and constantly improving/following the game features and Business Requirements.
	- Analyzing, developing and defining data integration solutions related to big data storage;
	- Applying data modeling, data design and implementation to support business requirements;
	- Participating in all agile development lifecycle activities: estimating, planning, designing, developing, documenting and testing;
	- A minimum of 3 years of professional experience in Data Transformation (ETL) and systems integration processes.
	- A deep understanding of Data flow, Database and data transformation principles (Big data and object storage concepts knowledge considered a major asset).
	- Knowledge of big data environment is an asset (Kafka, Databricks, Vertica, Others)
+ skill set:
	- Undertake analysis to monitor and report key performance indicators (KPI’s)
	- Building out our KPI Dashboards for monitoring game performance
	- Design, run, analyse and report on A/B tests
	- Experience with business intelligence software like Tableau, Looker etc
+ Experience with Hadoop, Vertica, HBase, Spark.
+ Skilled in Spark , JavaScript, Hadoop, Vertica, HBase, Angular, React, Docker, Kubernetes – advantage
+ Experience with relational (MySQL), NoSQL databases (Couchbase/Aerospike), search engines (ElasticSearch), caching solutions;
+ skill set:
	- Experience with ETL process, real time or batch pipelines
	- Experience with Kafka
	- Experience with Vertica
	- Knowledge in containerized environments (Docker, Kubernetes)
+ 3+ years of experience working with agile at scale vs scaling agile solutions such as SAFe, LeSS, Nexus (or others).
+ Experience with Docker, AWS, and CircleCI is a bonus.
+ skill set:
	- Research and design scalable Machine Learning infrastructures with real time applications utilizing TB of data.
	- You will be working with talented and passionate teams and drive the evolution of Big Data and Data Science. You’ll work with many exciting technologies such as Spark, Airflow, Hadoop, HP-Vertica, Aerospike, Redis, TensorFlow and Kafka.
	- Direct management of architects team. Cross-functional interaction with a wide range of people and teams, work closely with data engineers and data scientists to ensure high level of professionalism and standards, as well as satisfying project requirements and timelines.
	- Design scalable and reliable data pipelines to move huge amounts of data.
	- Design complex marketing platforms enabling effective player acquisition and retention at scale of millions of daily users.
	- Experienced with current SW development practices – SCRUM, test automation, TDD, CI, CD.
+ skill set:
	- Deep understanding of micro-services architecture, Kubernetes & Docker
	- Deep understanding of designing No-SQL databases (such as: MongoDb, Redis, etc.)
+ skill set:
	- Experience with Docker, Kubernetes, ceph, AWS a plus
	- Comfortable around data stores, MySQL, PostgreSQL and Redis a plus
	- You appreciate a well-designed REST API
	- You calmly triage production issues across microservices and approach the creation of software with a DevOps mindset
+ skill set:
	- You are humble, and play well with other people. You thrive in team settings, and exhibit excellent communication and collaboration skills. For example, offering constructive feedback in code reviews, writing user-centered documentation, and chatting with masterful use of text and emojis in Slack.
	- You thrive on solving problems for your customers through product-mindedness. You are comfortable with a little bit of ambiguity, and enjoy the opportunity to shape the future of the product.
	- You are an experienced "T-shaped" software engineer exhibiting broad knowledge of the entire discipline with deeper specializations in a few areas such as architecture, design, test automation, front-end technologies, etc.
	- You are internally driven by curiosity and continuous learning. You have proven that you can be entrusted with big decisions, and strive to bring understanding and empathy to the entire team.
	- You can function effectively in a distributed team. This means you are reliable, you know when to ask for help, you invest in strong relationships with your colleagues, and you know yourself well enough to be accountable for your own self management.
+ skill set:
	- Many of us feel that we’ve found our home for the next decade. This motivates us to take a long-term approach to our work: ***writing clean, maintainable code, investing in automated testing, and building in budgets for technical debt***. This also motivates us to value mentorship; we want our most junior engineers to be in a position to mentor others and to independently take on more responsibilities in the coming years. We love it when our engineers take full ownership of a feature and drive it from design through validation.
	- Familiarity with some enterprise user authentication technology (such as LDAP, SAML, Kerberos)
	- Experience building, shipping, and maintaining the back-end of a web-accessible product
	- Experience working with compiled programing languages (e.g. Go, Rust, C++, C# or Java) and SQL databases (any)
	- Experience building and shipping on-premise enterprise software
	- Experience with information security -- either via certifications (CISSP, CEH, etc.) or professional experience (security audit, pentesting, etc.)
+ skill set:
	- Have previous experience applying machine learning, and (big) data analytics frameworks such as TensorFlow, Scikit-learn, and the Apache Hadoop ecosystem, to real world problems.
	- Are well versed in the Linux operating system. Experience with cloud technology, and containers like Docker or Kubernetes, are highly desirable.
+ skill set:
	- software engineering
	- big data analytics framework
	- product innovation
	- services and applications
	- communicating with industrial and academic experts
	- machine learning
	- statistics
	- interested in personalized medicine
	- strong interest in industry projects
	- data mining
+ Knowledge of Apache Spark
+ skill set:
	- We are looking for multidisciplinary machine learners with sharp skills in one or more of the following fields:
		* Deep Learning,
		* Bayesian Modeling,
		* Natural Language Processing.
+ skill set:
	- Ursa Labs's work is focused on the Apache Arrow open source project, a cross-language development platform for in-memory analytics. Within the Arrow project, most of our time is spent on the C++, Python, and R libraries. It is our goal to make everyday tools for data access, cleaning, wrangling, analytics, and visualization a great deal more powerful and interoperable than they are now.

















#	Interesting Companies

+ [Great Place to Work® Institute](https://www.greatplacetowork.com/about)
	- Management Consulting
	- Oakland, California


##	Unlimited Holiday

+ Confluent



##	European Companies

+ Imagimob has taken great steps since the start in 2013. We have for example been rewarded as the Startup of the Year of 2017 at Mobilgalan, and has three years in a row been on the Ny Teknik's 33 list of Sweden's most promising technology companies.

##	List of companies/jobs

+ [Women in Machine Learning and Data Science (WiMLDS): New York, NY](http://wimlds.org/jobs/#)










#	Bio Design Automation & Bio Manufacturing Automation Companies

bio manufacturing automation

+ [addgene](https://www.addgene.org/careers/)
+ [Amyris](https://amyris.com/careers/)
+ [Analytikjena](https://www.analytik-jena.com/company/careers/)
+ [Asimov, Inc.](https://www.asimov.io/)
+ BioFab:
	- http://www.uwbiofab.org/
	- http://erc-assoc.org/achievements/biofab-world%E2%80%99s-first-biological-design-build-facility-synthetic-biology
	- Not http://biofabproducts.com/about/
+ [Biolegio B.V.](https://www.biolegio.com/)
+ [BioSero](https://www.biosero.com/about-us/)
+ Boston University:
	- CIDAR Lab
	- Biological Design Center, BDC
+ BP
+ [De Novo DNA](http://www.denovodna.com/)
+ [DSM](https://www.dsm.com/corporate/home.html)
+ GE Healthcare
+ [genomatica](https://www.genomatica.com/careers/)
+ Genome Foundry
	- https://www.genomefoundry.org/aboutus
	- https://www.concordia.ca/research/genome-foundry.html
+ [Ginkgo Bioworks](https://www.ginkgobioworks.com/about/)
+ [Hamilton Robotics](https://www.hamiltoncompany.com/)
+ [HighRes biosolutions](https://highresbio.com/careers/)
+ [Integrated DNA Technologies](https://www.idtdna.com/pages)
+ [Labcyte](https://www.labcyte.com/)
+ Leselagen
+ [lifefoundry](http://www.life-foundry.com/careers.html)
+ Lincoln DNA Foundry
+ [Molecular Devices](https://www.moleculardevices.com/about-us#gref)
+ [New England BioLabs](https://www.neb.com/about-neb/careers)
+ [Novozymes](https://www.novozymes.com/en)
+ [opentrons](https://opentrons.com/jobs)
+ [OriCiro Genomics Inc.](https://www.oriciro.com/)
+ Ozen
+[Ranomics Incorporated](https://www.ranomics.com/careers)
+ [SGIDNA, a Synthetic Genomics, Inc. company](https://www.sgidna.com/careers/)
+ [Synbicite](http://www.synbicite.com/about-us/)
+ [Synbiobeta](https://synbiobeta.com/job-board/)
+ [Synlogic](https://www.synlogictx.com/our-people/join-us/)
+ [TeselaGen Biotechnology Inc.](https://teselagen.com/)
+ [ThermoFisher Scientific](http://jobs.thermofisher.com/)
+ [Zymergen](https://www.zymergen.com/careers/)
















##	Events to look for job opportunities in BDA

+ Annual Biomedical Research Conference for Minority Students, ABRCMS
+ Fungal Genetics Conference
+ Programming Biology Night (Seattle)
+ SACNAS -  The National Diversity in STEM Conference
+ Scientista Symposium
+ WomenHack Boston
+ WomenHack Seattle


##	Other Resources

+ [BioLEGO](http://www.cs.technion.ac.il/~edwardv/BioLego/html/BioLego.html)

















###	Skill Sets for Research Internships with Alibaba Group

+ Quantum Algorithm for Near-term Quantum Devices
	- A general-purpose fault-tolerant quantum computer will require millions of physical qubits and millions of quantum gate operations. With quantum computers of significant size now on the horizon, we should understand how to best exploit the initially limited abilities and how to develop and run useful quantum algorithms within the limited circuit depth of intermediate size quantum devices with limited error correction.
+ Research in Practical Applications of Quantum-Safe Communication
	- Quantum communication may refer to quantum cryptography, quantum teleportation, and quantum entanglement. Among those, quantum key distribution (QKD) is one of the most practical applications in recent years. Quantum cryptography takes the advantage of the laws of quantum physics to protect data,
	- Currently, the most significant problems in practical quantum cryptography systems include: high-speed quantum random number generation, long-distance fiber quantum key distribution with high key generation speed, co-fiber transmission of classical and quantum optical signals, as well as practical commercialization and stabilization.
	- Our project aims to study these critical issues in quantum cryptography system for practical applications. Due to the transmission loss and dark count, the bottleneck for its practical application lies in the trade-off between high speed key generation rate and long transmission distance. In order to solve these problems, one potential solution is to design more efficient telecommunication protocol to exceed the theoretical up bound of the generation rate. Meanwhile, the project will also focus on the study and practical solutions for quantum random number generation, post-quantum cryptography algorithm, the migration of classical and quantum networks, etc
+ Building an innovative and systematic AI benchmarks platform
	- Currently in Alibaba Group, deep learning and related applications have been employed in various business departments. Tmall, Alitrip, Taobao, Ant Financial and other departments are making extensive use of emerging deep learning technologies to continuously improve application and algorithms and enhance the consumer experience. On the one hand, Alibaba's engineering teams design, experiment and deploy different deep learning algorithms and applications every day. On the other hand, deep learning requires a lot of computational power, which also puts higher requirements on the computational power of the hardware and their adaptability to the application. How to balance the demand and supply relationship between these two and integrate the solution into a systematic platform product? How to automatically and systematically evaluate the computional power of an AI hardware? How to evaluate the advantages and disadvantages of a hardware for usage in an application and give customer recommendations through a systematic platform? These are the challenges we are currently dealing with and we need to solve. Recently we have launched the AI ​​Matrix product (through aimatrix.ai website), but it is still in the early stage of the product. In the future, we need more people who have the same understanding as us and are willing to involve in solving these problems. Let's contribute our own strength and make the AI ​​Matrix as an effective systematic platform and an impactful technical brand.
	- [AI Matrix](https://aimatrix.ai/en-us/)
	- https://github.com/alibaba/ai-matrix
+ Using HW/SW Mechanisms to Improve Performance of remote Heterogeneous Systems
	- Alibaba is an e-commerce and AI company. We generate enormous data and consume huge amount of computation and storage resources every day. It is critical for Alibaba to keep on improving data center design given the emerging of powerful accelerator computation clusters.
	- We would focus on:
		* 1. Analyze different AI workloads in distributed GPU clusters, study their computation and network requirement
		* 2. Based on current remote accelerator technique, improve its efficiency via hardware/software solutions
		* 3. Apply the technique to real workload
	- Requirement:
		* 1. PHD candidate, experienced with distributed heterogeneous systems
		* 2. It's a plus if candidate worked with deep learning algorithms
		* 3. it's a plus if candidate has top conference publications
+ Last mile of datacenter as a computer -- local protocol and semantics based ASIC/FPGA cloud
	- Developers and customers prefer to use heterogeneous compute resources with a set of local server access protocol and semantics. We need to find talents to do research and prototyping with a specific local API on an ASIC or FPGA chip.
+ Emerging Accelerator Architecture, Programming Model, and Optimizations
	- The emerging hardware accelerator architectures, such as process-in-memory (PIM) and neuromorphic computing,  have shown great potential to speed up AI/ML and data-heavy applications. This research aims to investigate these non-traditional architecture designs and their performance implications for domain-specific applications in Alibaba datacenters and ecosystem. It will study the emerging architecture’s programming model for usability and explore the software-hardware co-design strategies (e.g. reinforcement learning based architecture space exploration, architecture-aware compression and sparsity exploitation) and optimization trade-offs to maximize the performance.
+ Execution engine optimization based on GPUs and other modern hardware
	- Targeting Maxcompute SQL engine, we'd like to import modern hardware technology (such as GPU, FPGA etc) to model and improve the core operators of the distributed execution engine, optimize the system performance on specific scenes at last.
+ Performance/Power/Area (PPA) Modeling & Analysis
	- The ***Computing Technology Lab of Alibaba Damo Academy*** focuses on advanced research topics in computing, memory/storage, and interconnect technologies that can revolutionize today's computing systems with holistic innovations ranging from system architectures to VLSI designs, to enable new computing capabilities for improving energy efficiency and performance across multiple application domains, including both high-performance and embedded computing.
+ Research on Domain Specific Architecture
	- As the end of Dennard's scaling and Moore's Law running out of steam, the traditional architecture for general-purpose processors can no longer meet the requirements of high performance and low energy consumption for various emerging applications. To allow the computing to have higher performance/energy efficiency, Domain Specific Architecture (DSA) has become a popular solution. However, there are many challenges in the DSA design. For example, the definition of the scope of Domain, trade-off between specialization and general-purpose, instruction set design, compiler design and optimization, memory wall, ultra-low-power design, micro-structure design and optimization, etc. This internship Project is a thorough and detailed study of the DSA to address these challenges.
	- The Computing Technology Lab focuses on advanced research topics in computing, memory/storage, and interconnect technologies that can revolutionize today's computing systems with holistic innovations ranging from system architectures to VLSI designs, to enable new computing capabilities for improving energy efficiency and performance across multiple application domains, including both high-performance and embedded computing.
+ Research on Cloud Server Architecture
	- Perform profiling/modeling and evaluation of workloads for our cloud server, design and optimize server architecture including but not limited to: CPU, cache/memory, storage and accelerators.
+ Research on algorithms/architectures of the next-generation AliNPU for training
	- AliNPU targeting for neural network training is a key component of Alibaba’s AI Chip strategy. To design an architecture surpassing the best of the AI training chips, such as GPU and TPU, we must look into all aspects from algorithms to HW architecture, for the potential computational efficiency improvements.
	- The works may focus on one or a few of the following directions：
		* 1. Algorithm innovations that may improve the system efficiency, and the experiments.
		* 2. The analysis of the theoretic bounds and/or the proof with regards to the algorithm innovations.
		* 3. Experimental HW architecture designs, simulations and their PPA analysis.
+ Hyper-scale cloud datacenter's compute resource pool and management platform prototype
	- Compute pools will be widely deployed in hyper-scale cloud datacenter. Alibaba Infrastructure AI Ops Platform (TIANJI) team is now actively seeking talents to work on research in this area.
+ Research on optimization of AI accelerator
	- Nowadays high performance computing has become one of the hot topics of AI research. The research aims on optimizing power dissipation and energy efficiency of AI accelerators targeting various of AI applications, providing high quality computation support for AI applications.
	- The research topics include:
		* 1. Research on computation pattern of various AI applications to look for bottleneck
		* 2. Research on AI accelerator architectures, implementations to improve performance and energy efficiency
		* 3. Codesign AI accelerator (SW/HW) and application to maximize the performance of accelerator)
+ Accelerating Machine Learning Applications on Heterogeneous Computing Architectures
	- This research aims to optimize ML applications on heterogeneous accelerators such as GPUs, FPGAs, and/or ASICs. S/he will conduct analysis and exploration on various performance bottlenecks in the full software/hardware stack, including ML algorithm improvement, model level transformation (e.g. compression, sparsity, data parallelization), and domain-specific architecture innovations, in order to dramatically boost the ML application's performance.



###	Other Information From Job Descriptions

Funny lines in job descriptions:
+ ["Frequently caught reading and engaging too much in AI/ML banter"](https://jobs.lever.co/sigopt/b172f247-6185-43e3-bc18-4a79f243122a), last viewed May 7, 2019. Jobs with SigOpt






#	References

Citations/References that use the *LaTeX/BibTeX* notation are taken
	from my *BibTeX* database (set of *BibTeX* entries).



#	Author Information

The MIT License (MIT)

Copyright (c) <2016> Zhiyang Ong

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

Email address: echo "cukj -wb- 23wU4X5M589 TROJANS cqkH wiuz2y 0f Mw Stanford" | awk '{ sub("23wU4X5M589","F.d_c_b. ") sub("Stanford","d0mA1n"); print $5, $2, $8; for (i=1; i<=1; i++) print "6\b"; print $9, $7, $6 }' | sed y/kqcbuHwM62z/gnotrzadqmC/ | tr 'q' ' ' | tr -d [:cntrl:] | tr -d 'ir' | tr y "\n"		Don't compromise my computing accounts. You have been warned.
